{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, accuracy_score, f1_score, precision_score\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import ensemble, preprocessing\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from my_functions import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import LinearSVC as LSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581007</td>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581008</td>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581009</td>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581010</td>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581011</td>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "0       2596   51   3  258    0   510  221  232  148  6279  ...   0   0   0   \n",
       "1       2590   56   2  212   -6   390  220  235  151  6225  ...   0   0   0   \n",
       "2       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "3       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "4       2595   45   2  153   -1   391  220  234  150  6172  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "581007  2396  153  20   85   17   108  240  237  118   837  ...   0   0   0   \n",
       "581008  2391  152  19   67   12    95  240  237  119   845  ...   0   0   0   \n",
       "581009  2386  159  17   60    7    90  236  241  130   854  ...   0   0   0   \n",
       "581010  2384  170  15   60    5    90  230  245  143   864  ...   0   0   0   \n",
       "581011  2383  165  13   60    4    67  231  244  141   875  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "0        0   0   0   0   0   0   5  \n",
       "1        0   0   0   0   0   0   5  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "4        0   0   0   0   0   0   5  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "581007   0   0   0   0   0   0   3  \n",
       "581008   0   0   0   0   0   0   3  \n",
       "581009   0   0   0   0   0   0   3  \n",
       "581010   0   0   0   0   0   0   3  \n",
       "581011   0   0   0   0   0   0   3  \n",
       "\n",
       "[581012 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/covtype.data', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "Name: 54, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[54].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2886</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>26</td>\n",
       "      <td>5253</td>\n",
       "      <td>234</td>\n",
       "      <td>240</td>\n",
       "      <td>136</td>\n",
       "      <td>4051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2742</td>\n",
       "      <td>134</td>\n",
       "      <td>22</td>\n",
       "      <td>150</td>\n",
       "      <td>69</td>\n",
       "      <td>3215</td>\n",
       "      <td>248</td>\n",
       "      <td>224</td>\n",
       "      <td>92</td>\n",
       "      <td>6091</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576882</td>\n",
       "      <td>2617</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>390</td>\n",
       "      <td>128</td>\n",
       "      <td>2081</td>\n",
       "      <td>215</td>\n",
       "      <td>211</td>\n",
       "      <td>130</td>\n",
       "      <td>592</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576883</td>\n",
       "      <td>2614</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>125</td>\n",
       "      <td>2051</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>618</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576884</td>\n",
       "      <td>2612</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>371</td>\n",
       "      <td>123</td>\n",
       "      <td>2021</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576885</td>\n",
       "      <td>2610</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>365</td>\n",
       "      <td>110</td>\n",
       "      <td>1991</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576886</td>\n",
       "      <td>2608</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>361</td>\n",
       "      <td>108</td>\n",
       "      <td>1961</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>131</td>\n",
       "      <td>698</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495141 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "2       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "3       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "5       2579  132   6  300  -15    67  230  237  140  6031  ...   0   0   0   \n",
       "11      2886  151  11  371   26  5253  234  240  136  4051  ...   0   0   0   \n",
       "12      2742  134  22  150   69  3215  248  224   92  6091  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "576882  2617   29  13  390  128  2081  215  211  130   592  ...   0   0   0   \n",
       "576883  2614   21  13  379  125  2051  211  212  135   618  ...   0   0   0   \n",
       "576884  2612   17  13  371  123  2021  208  211  138   644  ...   0   0   0   \n",
       "576885  2610   16  14  365  110  1991  208  211  138   671  ...   0   0   0   \n",
       "576886  2608   23  14  361  108  1961  211  209  131   698  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "5        0   0   0   0   0   0   2  \n",
       "11       0   0   0   0   0   0   2  \n",
       "12       0   0   0   0   0   0   2  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "576882   0   0   0   0   0   0   2  \n",
       "576883   0   0   0   0   0   0   2  \n",
       "576884   0   0   0   0   0   0   2  \n",
       "576885   0   0   0   0   0   0   2  \n",
       "576886   0   0   0   0   0   0   2  \n",
       "\n",
       "[495141 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[54] <= 2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2886</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>26</td>\n",
       "      <td>5253</td>\n",
       "      <td>234</td>\n",
       "      <td>240</td>\n",
       "      <td>136</td>\n",
       "      <td>4051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2742</td>\n",
       "      <td>134</td>\n",
       "      <td>22</td>\n",
       "      <td>150</td>\n",
       "      <td>69</td>\n",
       "      <td>3215</td>\n",
       "      <td>248</td>\n",
       "      <td>224</td>\n",
       "      <td>92</td>\n",
       "      <td>6091</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495136</td>\n",
       "      <td>2617</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>390</td>\n",
       "      <td>128</td>\n",
       "      <td>2081</td>\n",
       "      <td>215</td>\n",
       "      <td>211</td>\n",
       "      <td>130</td>\n",
       "      <td>592</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495137</td>\n",
       "      <td>2614</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>125</td>\n",
       "      <td>2051</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>618</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495138</td>\n",
       "      <td>2612</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>371</td>\n",
       "      <td>123</td>\n",
       "      <td>2021</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495139</td>\n",
       "      <td>2610</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>365</td>\n",
       "      <td>110</td>\n",
       "      <td>1991</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495140</td>\n",
       "      <td>2608</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>361</td>\n",
       "      <td>108</td>\n",
       "      <td>1961</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>131</td>\n",
       "      <td>698</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495141 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "0       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "1       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "2       2579  132   6  300  -15    67  230  237  140  6031  ...   0   0   0   \n",
       "3       2886  151  11  371   26  5253  234  240  136  4051  ...   0   0   0   \n",
       "4       2742  134  22  150   69  3215  248  224   92  6091  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "495136  2617   29  13  390  128  2081  215  211  130   592  ...   0   0   0   \n",
       "495137  2614   21  13  379  125  2051  211  212  135   618  ...   0   0   0   \n",
       "495138  2612   17  13  371  123  2021  208  211  138   644  ...   0   0   0   \n",
       "495139  2610   16  14  365  110  1991  208  211  138   671  ...   0   0   0   \n",
       "495140  2608   23  14  361  108  1961  211  209  131   698  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "0        0   0   0   0   0   0   2  \n",
       "1        0   0   0   0   0   0   2  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "4        0   0   0   0   0   0   2  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "495136   0   0   0   0   0   0   2  \n",
       "495137   0   0   0   0   0   0   2  \n",
       "495138   0   0   0   0   0   0   2  \n",
       "495139   0   0   0   0   0   0   2  \n",
       "495140   0   0   0   0   0   0   2  \n",
       "\n",
       "[495141 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().drop('index', axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "Name: 54, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[54].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(54, axis = 1)\n",
    "Y = pd.factorize(df[54])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 54)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape[0]/5)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([28792, 21208], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0\n",
      "f1_score:\n",
      "0.9011912191442134\n",
      "precision_score:\n",
      "0.9000288819639735\n",
      "accuracy_score:\n",
      "0.9028\n",
      "Confusion Matrix:\n",
      "[[5152  549]\n",
      " [ 423 3876]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      5701\n",
      "           1       0.88      0.90      0.89      4299\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8762978464124818\n",
      "precision_score:\n",
      "0.8748448308444031\n",
      "accuracy_score:\n",
      "0.8781\n",
      "Confusion Matrix:\n",
      "[[4994  707]\n",
      " [ 512 3787]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      5701\n",
      "           1       0.84      0.88      0.86      4299\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7658901460024897\n",
      "precision_score:\n",
      "0.7665646136492621\n",
      "accuracy_score:\n",
      "0.7672\n",
      "Confusion Matrix:\n",
      "[[4210 1491]\n",
      " [ 837 3462]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.74      0.78      5701\n",
      "           1       0.70      0.81      0.75      4299\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "0.2 1\n",
      "f1_score:\n",
      "0.9013010515731921\n",
      "precision_score:\n",
      "0.9000342715812495\n",
      "accuracy_score:\n",
      "0.903\n",
      "Confusion Matrix:\n",
      "[[5171  552]\n",
      " [ 418 3859]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91      5723\n",
      "           1       0.87      0.90      0.89      4277\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8753643458016835\n",
      "precision_score:\n",
      "0.8739825388613102\n",
      "accuracy_score:\n",
      "0.8774\n",
      "Confusion Matrix:\n",
      "[[5026  697]\n",
      " [ 529 3748]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5723\n",
      "           1       0.84      0.88      0.86      4277\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7717322271755237\n",
      "precision_score:\n",
      "0.7721124094906744\n",
      "accuracy_score:\n",
      "0.7768\n",
      "Confusion Matrix:\n",
      "[[4629 1094]\n",
      " [1138 3139]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81      5723\n",
      "           1       0.74      0.73      0.74      4277\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "0.2 2\n",
      "f1_score:\n",
      "0.8982335040924778\n",
      "precision_score:\n",
      "0.8968781506302878\n",
      "accuracy_score:\n",
      "0.9003\n",
      "Confusion Matrix:\n",
      "[[5214  564]\n",
      " [ 433 3789]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      5778\n",
      "           1       0.87      0.90      0.88      4222\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8724828345300577\n",
      "precision_score:\n",
      "0.8735596631110416\n",
      "accuracy_score:\n",
      "0.8759\n",
      "Confusion Matrix:\n",
      "[[5198  580]\n",
      " [ 661 3561]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      5778\n",
      "           1       0.86      0.84      0.85      4222\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7692585609627991\n",
      "precision_score:\n",
      "0.7681895457255643\n",
      "accuracy_score:\n",
      "0.772\n",
      "Confusion Matrix:\n",
      "[[4405 1373]\n",
      " [ 907 3315]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      5778\n",
      "           1       0.71      0.79      0.74      4222\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "0.5 0\n",
      "f1_score:\n",
      "0.8870453404723103\n",
      "precision_score:\n",
      "0.8875020107723913\n",
      "accuracy_score:\n",
      "0.88972\n",
      "Confusion Matrix:\n",
      "[[13045  1333]\n",
      " [ 1424  9198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90     14378\n",
      "           1       0.87      0.87      0.87     10622\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "f1_score:\n",
      "0.8580122685506582\n",
      "precision_score:\n",
      "0.8564952115936223\n",
      "accuracy_score:\n",
      "0.86044\n",
      "Confusion Matrix:\n",
      "[[12390  1988]\n",
      " [ 1501  9121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88     14378\n",
      "           1       0.82      0.86      0.84     10622\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7741932762412875\n",
      "precision_score:\n",
      "0.7730690747255385\n",
      "accuracy_score:\n",
      "0.77804\n",
      "Confusion Matrix:\n",
      "[[11357  3021]\n",
      " [ 2528  8094]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     14378\n",
      "           1       0.73      0.76      0.74     10622\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.77      0.78      0.77     25000\n",
      "weighted avg       0.78      0.78      0.78     25000\n",
      "\n",
      "0.5 1\n",
      "f1_score:\n",
      "0.8829193282590573\n",
      "precision_score:\n",
      "0.8825860752905663\n",
      "accuracy_score:\n",
      "0.8854\n",
      "Confusion Matrix:\n",
      "[[12887  1470]\n",
      " [ 1395  9248]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     14357\n",
      "           1       0.86      0.87      0.87     10643\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "f1_score:\n",
      "0.8565562195850692\n",
      "precision_score:\n",
      "0.8554404934598026\n",
      "accuracy_score:\n",
      "0.8592\n",
      "Confusion Matrix:\n",
      "[[12437  1920]\n",
      " [ 1600  9043]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88     14357\n",
      "           1       0.82      0.85      0.84     10643\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.771149317816419\n",
      "precision_score:\n",
      "0.7699257808847871\n",
      "accuracy_score:\n",
      "0.77456\n",
      "Confusion Matrix:\n",
      "[[11208  3149]\n",
      " [ 2487  8156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80     14357\n",
      "           1       0.72      0.77      0.74     10643\n",
      "\n",
      "    accuracy                           0.77     25000\n",
      "   macro avg       0.77      0.77      0.77     25000\n",
      "weighted avg       0.78      0.77      0.78     25000\n",
      "\n",
      "0.5 2\n",
      "f1_score:\n",
      "0.8860450048704022\n",
      "precision_score:\n",
      "0.8862736765287726\n",
      "accuracy_score:\n",
      "0.88856\n",
      "Confusion Matrix:\n",
      "[[12964  1369]\n",
      " [ 1417  9250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     14333\n",
      "           1       0.87      0.87      0.87     10667\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "f1_score:\n",
      "0.8509229235563065\n",
      "precision_score:\n",
      "0.8507152100262619\n",
      "accuracy_score:\n",
      "0.85404\n",
      "Confusion Matrix:\n",
      "[[12483  1850]\n",
      " [ 1799  8868]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     14333\n",
      "           1       0.83      0.83      0.83     10667\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7672061123556289\n",
      "precision_score:\n",
      "0.7660260477390795\n",
      "accuracy_score:\n",
      "0.77024\n",
      "Confusion Matrix:\n",
      "[[11055  3278]\n",
      " [ 2466  8201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.79     14333\n",
      "           1       0.71      0.77      0.74     10667\n",
      "\n",
      "    accuracy                           0.77     25000\n",
      "   macro avg       0.77      0.77      0.77     25000\n",
      "weighted avg       0.77      0.77      0.77     25000\n",
      "\n",
      "0.8 0\n",
      "f1_score:\n",
      "0.8460830524657951\n",
      "precision_score:\n",
      "0.8452148172772189\n",
      "accuracy_score:\n",
      "0.849225\n",
      "Confusion Matrix:\n",
      "[[19842  3203]\n",
      " [ 2828 14127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87     23045\n",
      "           1       0.82      0.83      0.82     16955\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.85      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n",
      "f1_score:\n",
      "0.8312805155811258\n",
      "precision_score:\n",
      "0.8309593548549447\n",
      "accuracy_score:\n",
      "0.835025\n",
      "Confusion Matrix:\n",
      "[[19680  3365]\n",
      " [ 3234 13721]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86     23045\n",
      "           1       0.80      0.81      0.81     16955\n",
      "\n",
      "    accuracy                           0.84     40000\n",
      "   macro avg       0.83      0.83      0.83     40000\n",
      "weighted avg       0.84      0.84      0.84     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7676834967154456\n",
      "precision_score:\n",
      "0.7671490008374773\n",
      "accuracy_score:\n",
      "0.769875\n",
      "Confusion Matrix:\n",
      "[[17340  5705]\n",
      " [ 3500 13455]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79     23045\n",
      "           1       0.70      0.79      0.75     16955\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.77      0.77     40000\n",
      "\n",
      "0.8 1\n",
      "f1_score:\n",
      "0.8453235629884854\n",
      "precision_score:\n",
      "0.8440770961542217\n",
      "accuracy_score:\n",
      "0.848175\n",
      "Confusion Matrix:\n",
      "[[19679  3340]\n",
      " [ 2733 14248]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87     23019\n",
      "           1       0.81      0.84      0.82     16981\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.84      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n",
      "f1_score:\n",
      "0.8305563012521898\n",
      "precision_score:\n",
      "0.8300151073504303\n",
      "accuracy_score:\n",
      "0.834125\n",
      "Confusion Matrix:\n",
      "[[19585  3434]\n",
      " [ 3201 13780]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86     23019\n",
      "           1       0.80      0.81      0.81     16981\n",
      "\n",
      "    accuracy                           0.83     40000\n",
      "   macro avg       0.83      0.83      0.83     40000\n",
      "weighted avg       0.83      0.83      0.83     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7695059488064917\n",
      "precision_score:\n",
      "0.7683602948163101\n",
      "accuracy_score:\n",
      "0.7723\n",
      "Confusion Matrix:\n",
      "[[17648  5371]\n",
      " [ 3737 13244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.79     23019\n",
      "           1       0.71      0.78      0.74     16981\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.77      0.77     40000\n",
      "\n",
      "0.8 2\n",
      "f1_score:\n",
      "0.847199225686218\n",
      "precision_score:\n",
      "0.8460925140016815\n",
      "accuracy_score:\n",
      "0.850125\n",
      "Confusion Matrix:\n",
      "[[19770  3253]\n",
      " [ 2742 14235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87     23023\n",
      "           1       0.81      0.84      0.83     16977\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.85      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n",
      "f1_score:\n",
      "0.8262136169817684\n",
      "precision_score:\n",
      "0.825468823107455\n",
      "accuracy_score:\n",
      "0.82975\n",
      "Confusion Matrix:\n",
      "[[19448  3575]\n",
      " [ 3235 13742]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85     23023\n",
      "           1       0.79      0.81      0.80     16977\n",
      "\n",
      "    accuracy                           0.83     40000\n",
      "   macro avg       0.83      0.83      0.83     40000\n",
      "weighted avg       0.83      0.83      0.83     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.769135441892917\n",
      "precision_score:\n",
      "0.7678659683158744\n",
      "accuracy_score:\n",
      "0.77235\n",
      "Confusion Matrix:\n",
      "[[17807  5216]\n",
      " [ 3890 13087]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80     23023\n",
      "           1       0.72      0.77      0.74     16977\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.77      0.77     40000\n",
      "\n",
      "Elapsed Time: 876 seconds\n"
     ]
    }
   ],
   "source": [
    "rf = np.empty([9, 7])\n",
    "mlp = np.empty([9, 7])\n",
    "svm = np.empty([9, 7])\n",
    "test_sizes = [0.2,0.5,0.8]\n",
    "j = 0\n",
    "start1 = time.time()\n",
    "for size in test_sizes:\n",
    "    for i in np.arange(3):\n",
    "        print(size, i)\n",
    "        x_train1, x_test1, y_train1, y_test1 = train_test_split(X_train, Y_train, test_size=size)\n",
    "        rf_time = time.time()\n",
    "        rf_f1, rf_apr, rf_acc, rf_auc = RFfunc(x_train1, x_test1, y_train1, y_test1)\n",
    "        rf_time = time.time() - rf_time\n",
    "        \n",
    "        mlp_time = time.time()\n",
    "        mlp_f1, mlp_apr, mlp_acc, mlp_auc = MLPfunc(x_train1, x_test1, y_train1, y_test1)\n",
    "        mlp_time = time.time() - mlp_time\n",
    "        \n",
    "        svm_time = time.time()\n",
    "        svm_f1, svm_apr, svm_acc, svm_auc = LSVMfunc(x_train1, x_test1, y_train1, y_test1)\n",
    "        svm_time = time.time() - svm_time\n",
    "        rf[j] = [rf_f1, rf_apr, rf_acc, rf_auc, i, size, rf_time]\n",
    "        mlp[j] = [mlp_f1, mlp_apr, mlp_acc, mlp_auc, i, size, mlp_time]\n",
    "        svm[j] = [svm_f1, svm_apr, svm_acc, svm_auc, i, size, svm_time]\n",
    "        j = j + 1\n",
    "now1 = time.time()\n",
    "print('Elapsed Time: ' + str(int(now1-start1)) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.DataFrame(rf, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "mlp_df = pd.DataFrame(mlp, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "svm_df = pd.DataFrame(svm, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "rf_df['avg'] = round(rf_df.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "mlp_df['avg'] = round(mlp_df.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "svm_df['avg'] = round(svm_df.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "mlp_mean = round(mlp_df.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "rf_mean = round(rf_df.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "svm_mean = round(svm_df.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "rf_mean['avg'] = round(rf_mean.drop('time',axis=1).mean(axis=1),3).values\n",
    "mlp_mean['avg'] = round(mlp_mean.drop('time',axis=1).mean(axis=1),3).values\n",
    "svm_mean['avg'] = round(svm_mean.drop('time',axis=1).mean(axis=1),3).values\n",
    "\n",
    "rf_mean['avg_std'] = round(rf_df.groupby('test_size').std(),3)['avg'].values\n",
    "mlp_mean['avg_std'] = round(mlp_df.groupby('test_size').std(),3)['avg'].values\n",
    "svm_mean['avg_std'] = round(svm_df.groupby('test_size').std(),3)['avg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.900985191203893\n",
      "precision_score:\n",
      "0.9008482422497388\n",
      "accuracy_score:\n",
      "0.9033\n",
      "Confusion Matrix:\n",
      "[[5281  489]\n",
      " [ 478 3752]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      5770\n",
      "           1       0.88      0.89      0.89      4230\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8765092039553918\n",
      "precision_score:\n",
      "0.8763921645702306\n",
      "accuracy_score:\n",
      "0.8794\n",
      "Confusion Matrix:\n",
      "[[5162  608]\n",
      " [ 598 3632]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      5770\n",
      "           1       0.86      0.86      0.86      4230\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Elapsed Time: 120 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7705814748717709\n",
      "precision_score:\n",
      "0.7703179781846514\n",
      "accuracy_score:\n",
      "0.7726\n",
      "Confusion Matrix:\n",
      "[[4332 1438]\n",
      " [ 836 3394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.75      0.79      5770\n",
      "           1       0.70      0.80      0.75      4230\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.78      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 88 seconds\n",
      "Elapsed Time: 215 seconds\n",
      "f1_score:\n",
      "0.9013952611420826\n",
      "precision_score:\n",
      "0.9001626108074873\n",
      "accuracy_score:\n",
      "0.9035\n",
      "Confusion Matrix:\n",
      "[[5248  539]\n",
      " [ 426 3787]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      5787\n",
      "           1       0.88      0.90      0.89      4213\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8744711664501218\n",
      "precision_score:\n",
      "0.8728336674895547\n",
      "accuracy_score:\n",
      "0.8769\n",
      "Confusion Matrix:\n",
      "[[5080  707]\n",
      " [ 524 3689]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      5787\n",
      "           1       0.84      0.88      0.86      4213\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.88      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Elapsed Time: 123 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7639070673478658\n",
      "precision_score:\n",
      "0.7630077542112108\n",
      "accuracy_score:\n",
      "0.7666\n",
      "Confusion Matrix:\n",
      "[[4367 1420]\n",
      " [ 914 3299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      5787\n",
      "           1       0.70      0.78      0.74      4213\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.76      0.77      0.76     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 73 seconds\n",
      "Elapsed Time: 204 seconds\n",
      "f1_score:\n",
      "0.8992892401899686\n",
      "precision_score:\n",
      "0.8992547843680205\n",
      "accuracy_score:\n",
      "0.9013\n",
      "Confusion Matrix:\n",
      "[[5213  495]\n",
      " [ 492 3800]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      5708\n",
      "           1       0.88      0.89      0.89      4292\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8723968083979263\n",
      "precision_score:\n",
      "0.8714758116466346\n",
      "accuracy_score:\n",
      "0.8746\n",
      "Confusion Matrix:\n",
      "[[5030  678]\n",
      " [ 576 3716]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5708\n",
      "           1       0.85      0.87      0.86      4292\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7670760942295995\n",
      "precision_score:\n",
      "0.7661088966639431\n",
      "accuracy_score:\n",
      "0.7706\n",
      "Confusion Matrix:\n",
      "[[4468 1240]\n",
      " [1054 3238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80      5708\n",
      "           1       0.72      0.75      0.74      4292\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 91 seconds\n",
      "Elapsed Time: 182 seconds\n",
      "f1_score:\n",
      "0.8968003372874984\n",
      "precision_score:\n",
      "0.8955126904345654\n",
      "accuracy_score:\n",
      "0.8988\n",
      "Confusion Matrix:\n",
      "[[5190  570]\n",
      " [ 442 3798]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      5760\n",
      "           1       0.87      0.90      0.88      4240\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8689571809277405\n",
      "precision_score:\n",
      "0.8677729022573031\n",
      "accuracy_score:\n",
      "0.8715\n",
      "Confusion Matrix:\n",
      "[[5054  706]\n",
      " [ 579 3661]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5760\n",
      "           1       0.84      0.86      0.85      4240\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 107 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7696087407092999\n",
      "precision_score:\n",
      "0.7683217909772877\n",
      "accuracy_score:\n",
      "0.7731\n",
      "Confusion Matrix:\n",
      "[[4481 1279]\n",
      " [ 990 3250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80      5760\n",
      "           1       0.72      0.77      0.74      4240\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 76 seconds\n",
      "Elapsed Time: 190 seconds\n",
      "f1_score:\n",
      "0.9011401916599715\n",
      "precision_score:\n",
      "0.9012810397136823\n",
      "accuracy_score:\n",
      "0.9035\n",
      "Confusion Matrix:\n",
      "[[5290  477]\n",
      " [ 488 3745]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      5767\n",
      "           1       0.89      0.88      0.89      4233\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8696664708144859\n",
      "precision_score:\n",
      "0.8676730627454192\n",
      "accuracy_score:\n",
      "0.8716\n",
      "Confusion Matrix:\n",
      "[[4967  800]\n",
      " [ 484 3749]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.89      5767\n",
      "           1       0.82      0.89      0.85      4233\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7708135161540624\n",
      "precision_score:\n",
      "0.7714487748429484\n",
      "accuracy_score:\n",
      "0.7724\n",
      "Confusion Matrix:\n",
      "[[4278 1489]\n",
      " [ 787 3446]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79      5767\n",
      "           1       0.70      0.81      0.75      4233\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.78      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 99 seconds\n",
      "Elapsed Time: 202 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "rf = np.empty([5, 4])\n",
    "mlp = np.empty([5, 4])\n",
    "svm = np.empty([5, 4])\n",
    "j = 0\n",
    "for train_index, test_index in kf.split(X_train, Y_train):\n",
    "    x_train = X_train[train_index]\n",
    "    y_train = Y_train[train_index]\n",
    "    \n",
    "    x_test = X_train[test_index]\n",
    "    y_test = Y_train[test_index]\n",
    "        \n",
    "    start = time.time()\n",
    "    rf_f1, rf_apr, rf_acc, rf_auc = RFfunc(x_train, x_test, y_train, y_test)\n",
    "    mlp_f1, mlp_apr, mlp_acc, mlp_auc = MLPfunc(x_train, x_test, y_train, y_test)\n",
    "    svm_f1, svm_apr, svm_acc, svm_auc = LSVMfunc(x_train, x_test, y_train, y_test)\n",
    " \n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    rf[j] = [rf_f1, rf_apr, rf_acc, rf_auc]\n",
    "    mlp[j] = [mlp_f1, mlp_apr, mlp_acc, mlp_auc]\n",
    "    svm[j] = [svm_f1, svm_apr, svm_acc, svm_auc]\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_stats = pd.DataFrame(rf, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "mlp_stats = pd.DataFrame(mlp, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "svm_stats = pd.DataFrame(svm, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "rf_stats['avg'] = rf_stats.mean(axis=1).values\n",
    "mlp_stats['avg'] = mlp_stats.mean(axis=1).values\n",
    "svm_stats['avg'] = svm_stats.mean(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc    avg\n",
       "0  0.901  0.901  0.903  0.965  0.918\n",
       "1  0.901  0.900  0.904  0.966  0.918\n",
       "2  0.899  0.899  0.901  0.964  0.916\n",
       "3  0.897  0.896  0.899  0.962  0.914\n",
       "4  0.901  0.901  0.904  0.966  0.918"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.900\n",
       "apr    0.899\n",
       "acc    0.902\n",
       "auc    0.965\n",
       "avg    0.917\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rf_stats.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.89475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.89325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.89025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.88775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.88975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.877  0.876  0.879  0.947  0.89475\n",
       "1  0.874  0.873  0.877  0.949  0.89325\n",
       "2  0.872  0.871  0.875  0.943  0.89025\n",
       "3  0.869  0.868  0.872  0.942  0.88775\n",
       "4  0.870  0.868  0.872  0.949  0.88975"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.872\n",
       "apr    0.871\n",
       "acc    0.875\n",
       "auc    0.946\n",
       "avg    0.891\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mlp_stats.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.78975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.78350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.78525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.78850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.79050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.771  0.770  0.773  0.845  0.78975\n",
       "1  0.764  0.763  0.767  0.840  0.78350\n",
       "2  0.767  0.766  0.771  0.837  0.78525\n",
       "3  0.770  0.768  0.773  0.843  0.78850\n",
       "4  0.771  0.771  0.772  0.848  0.79050"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.769\n",
       "apr    0.768\n",
       "acc    0.771\n",
       "auc    0.843\n",
       "avg    0.787\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(svm_stats.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.2</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.845</td>\n",
       "      <td>180.136</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.845</td>\n",
       "      <td>93.520</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.8</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.844</td>\n",
       "      <td>13.636</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc     time    avg  avg_std\n",
       "test_size                                                     \n",
       "0.2        0.769  0.769  0.772  0.845  180.136  0.789    0.003\n",
       "0.5        0.771  0.770  0.774  0.845   93.520  0.790    0.003\n",
       "0.8        0.769  0.768  0.771  0.844   13.636  0.788    0.001"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1         0.768667\n",
       "apr        0.768000\n",
       "acc        0.771333\n",
       "auc        0.843667\n",
       "avg        0.787667\n",
       "avg_std    0.003333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.2</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.949</td>\n",
       "      <td>168.691</td>\n",
       "      <td>28.860</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.934</td>\n",
       "      <td>70.042</td>\n",
       "      <td>12.403</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.8</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.910</td>\n",
       "      <td>31.469</td>\n",
       "      <td>5.953</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc     time     avg  avg_std\n",
       "test_size                                                      \n",
       "0.2        0.874  0.874  0.877  0.949  168.691  28.860    0.001\n",
       "0.5        0.855  0.854  0.858  0.934   70.042  12.403    0.003\n",
       "0.8        0.829  0.829  0.833  0.910   31.469   5.953    0.003"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.2</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.966</td>\n",
       "      <td>6.475</td>\n",
       "      <td>1.843</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.5</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.955</td>\n",
       "      <td>4.435</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.8</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.394</td>\n",
       "      <td>1.121</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc   time    avg  avg_std\n",
       "test_size                                                   \n",
       "0.2        0.900  0.899  0.902  0.966  6.475  1.843    0.001\n",
       "0.5        0.885  0.886  0.888  0.955  4.435  1.492    0.002\n",
       "0.8        0.846  0.845  0.849  0.927  2.394  1.121    0.001"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([180.136,  93.52 ,  13.636])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    clf = ensemble.RandomForestClassifier(n_jobs = -1, n_estimators=200, \n",
    "                                          random_state=42)\n",
    "    clf.fit(x_train1, y_train1)\n",
    "    probs = clf.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.6651,y_test1, probs)\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    start = time.time()\n",
    "    mlp = MLPClassifier(solver='adam', activation='relu', alpha=0.001, \n",
    "                        hidden_layer_sizes = (32, 32), max_iter = 10000)\n",
    "    mlp.fit(x_train1, y_train1)\n",
    "    probs = mlp.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.77, y_test1, probs)\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    start = time.time()\n",
    "    svc = SVC(kernel = 'rbf', C = 5, degree = 10, gamma = 0.04, \n",
    "              max_iter =  100000, probability = True, n_jobs = -1)\n",
    "    svc.fit(x_train1, y_train1)\n",
    "    probs = svc.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.7521,y_test1, probs)\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSVMfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    start=time.time()\n",
    "    svc = LSVC(C = 3, loss = 'hinge', max_iter=10000)\n",
    "    svc = CalibratedClassifierCV(svc)\n",
    "    svc.fit(x_train1,y_train1)\n",
    "    probs = svc.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.7521,y_test1, probs)\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "now = time.time()\n",
    "print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "\n",
    "probs_svm = svc.predict_proba(x_test1)[:,1]\n",
    "#probs_svm = svc.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = shuffle(X, Y)\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_train = X_train[0:50000]\n",
    "Y_train = Y_train[0:50000]\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X_train, Y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 4 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf = ensemble.RandomForestClassifier(n_jobs = -1, n_estimators=200, random_state=42)\n",
    "clf.fit(x_train1, y_train1)\n",
    "\n",
    "now = time.time()\n",
    "print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "\n",
    "probs = clf.predict_proba(x_test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(tpr - fpr) w/ th =  0.45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_test1, probs, 'RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.849855245405214\n",
      "precision_score:\n",
      "0.8487540683517956\n",
      "accuracy_score:\n",
      "0.852425\n",
      "Confusion Matrix:\n",
      "[[19665  3223]\n",
      " [ 2680 14432]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87     22888\n",
      "           1       0.82      0.84      0.83     17112\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.85      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.849855245405214, 0.8487540683517956, 0.852425, 0.9304391912856051)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_info(0.455,y_test1, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 17 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "mlp = MLPClassifier(solver='adam', activation='relu', alpha=0.001, hidden_layer_sizes = (32, 32), max_iter = 10000)\n",
    "mlp.fit(x_train1, y_train1)\n",
    "\n",
    "now = time.time()\n",
    "print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "probs_nn = mlp.predict_proba(x_test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(tpr - fpr) w/ th =  0.4461228714959328\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeVxU5f7A8c/DpiJuuAOiILjgrriWue+GaWWYZV2zfbnV1SxNu5X35q/NMksr61pmkLum5pIpamnuKy64oCwuCG6IbMP398cZRlDAURkGhuf9es1r5pw558x3yM53zvM85/soEUHTNE3TbsXJ3gFomqZpJYNOGJqmaZpVdMLQNE3TrKIThqZpmmYVnTA0TdM0q+iEoWmapllFJwxN0zTNKjphaA5FKRWtlLqmlEpWSp1RSs1SSnncsE0npdQfSqkrSqlLSqlflVJBN2xTUSn1mVLqlPlYR83L1fL5XKWUekUptV8pdVUpFauUmqeUambL76tpRUknDM0R3S8iHkBLoBXwVvYbSqmOwGpgCeAF+AF7gD+VUv7mbdyAtUAToC9QEegEJALt8vnMz4F/Aq8AnkADYDEw4HaDV0q53O4+mlYUlL7TW3MkSqloYJSI/G5e/hBoIiIDzMsbgX0i8sIN+/0GJIjICKXUKOA/QH0RSbbiMwOBQ0BHEdmazzbrgZ9EZKZ5+UlznPealwV4CXgVcAFWAckiMjrHMZYAESLyqVLKC/gCuA9IBqaIyFQr/kSadsf0FYbmsJRSPkA/4Kh52R3jSmFeHpvPBXqZX/cEVlqTLMx6ALH5JYvb8ADQHggCfgYeUUopAKVUFaA3EK6UcgJ+xbgy8jZ//qtKqT53+fmaViCdMDRHtFgpdQWIAc4B75jXe2L8mz+dxz6ngez+iar5bJOf290+Px+ISJKIXAM2AgJ0Nr/3ELBZROKBtkB1EXlPRNJF5DjwLRBaCDFoWr50wtAc0QMiUgHoCjTieiK4AGQBtfPYpzZw3vw6MZ9t8nO72+cnJvuFGG3F4cAw86pHgTnm13UBL6XUxewHMA6oWQgxaFq+dMLQHJaIRACzgI/Ny1eBzcDDeWw+FKOjG+B3oI9SqryVH7UW8FFKBRewzVXAPcdyrbxCvmE5DHhIKVUXo6lqgXl9DHBCRCrneFQQkf5Wxqtpd0QnDM3RfQb0Ukq1NC+/CTxhHgJbQSlVRSk1CegIvGveZjbGSXmBUqqRUspJKVVVKTVOKXXTSVlEooCvgDClVFellJtSqqxSKlQp9aZ5s93AEKWUu1IqAHjqVoGLyC4gAZgJrBKRi+a3tgKXlVJjlVLllFLOSqmmSqm2d/IH0jRr6YShOTQRSQB+BCaYlzcBfYAhGP0OJzGG3t5rPvEjImkYHd+HgDXAZYyTdDXg73w+6hVgGvAlcBE4BgzG6JwGmAKkA2eBH7jevHQrYeZYfs7xnUzA/RjDhk9gNKXNBCpZeUxNuyN6WK2maZpmFX2FoWmapllFJwxN0zTNKjphaJqmaVbRCUPTNE2zSokrclatWjWpV6+evcPQNE0rUXbs2HFeRKrfzTFKXMKoV68e27dvt3cYmqZpJYpS6uTdHkM3SWmapmlW0QlD0zRNs4pOGJqmaZpVSlwfRl4yMjKIjY0lNTXV3qFoDqxs2bL4+Pjg6upq71A0zS4cImHExsZSoUIF6tWrh3m+GU0rVCJCYmIisbGx+Pn52TscTbMLmzVJKaW+V0qdU0rtz+d9pZSaqpQ6qpTaq5RqfaeflZqaStWqVXWy0GxGKUXVqlX1VaxWqtmyD2MW0LeA9/sBgebHM8D0u/kwnSw0W9P/xrTSzmZNUiKyQSlVr4BNBgE/mmcW26KUqqyUqi0ihTHVpaZpWtEwZUD6VcgyQVam8chMhdRLgEBW1vX1pjS4mghZGSBZ5ofk89r8uBBNRpkqpGSYqNTifvBuY7evas8+DG9yTEkJxJrX3ZQwlFLPYFyF4OvrWyTB3S6lFI899hizZ88GIDMzk9q1a9O+fXuWLVvGrFmz2L59O9OmTcu1X7169ahQoQJOTk7UrFmTH3/8kVq1apGcnMy//vUvfv/9d8qWLUvVqlX56KOPaN++PR4eHiQnJxdK3DNmzMDd3Z0RI0Zw6NAhQkNDUUoxf/58Hn/8cf7666+7On5CQgJeXl5MmzaNZ5991rL+xu9w49/nxx9/5MMPP0REEBFGjhzJ6NGjC/ysDz74gO+++w5nZ2emTp1Knz59btpm7dq1jBkzhqysLDw8PJg1axYBAQHMmjWLMWPG4O3tDcBLL73EqFGj7uq7a3YgYpyYM67B5Tg4H2WcvC/FgHIyTsBZpusn98xUSEk0Tvpiuv6+iLGcZYLzR6B8dfNJP8d+F06ASznIvGbzr+UKVECRVa0OTqU0YeR1fZ/n5Bwi8g3wDUBwcHCxnMCjfPny7N+/n2vXrlGuXDnWrFljOfncyrp166hWrRrjxo3jv//9L1OnTmXUqFH4+fkRFRWFk5MTx48f5+DBg4Ue93PPPWd5vXjxYgYNGsS77xoTz91Ossg+sTs55W7lnDdvHh06dCAsLCxXwijIb7/9xmeffcbq1avx8vIiNTXVkojzExkZSXh4OAcOHCA+Pp6ePXty5MgRnJ2dc233/PPPs2TJEho3bsxXX33FpEmTmDVrFgCPPPLITQlds7Msk3GyT71snKTjdsDZ/eDkYiSFswcg7Ypxok+9BGmXrT+2cjaOU6YClK1oXnY2EotyBqWM5fLVIPkc1GhsbO9k3q92C3B2g4q1jX0qeF1/z8nFSCQVfXLv4+QCbuWhXGVAmT/rxoex/lJaFp+sPsLcnfFkHvwDObiWsvM+5emnr/Dqq69a/TV37NjBk08+CdBUKTUV+KeIiFLq38DTGLM6AowTkRUFHcueCSMWqJNj2QeIt1MshaJfv34sX76chx56iLCwMIYNG8bGjRut3v++++5j6tSpHDt2jL///ps5c+ZYTsD+/v74+/vn2j45OZlBgwZx4cIFMjIymDRpEoMGDeLq1asMHTqU2NhYTCYTEyZM4JFHHuHNN99k6dKluLi40Lt3bz7++GP+/e9/4+HhQVBQEJ999hnOzs5s2LCBdevW5boK+Oijj5g7dy5paWkMHjyYd999l+joaPr160e3bt3YvHkzixcvpm7durliDAsL45NPPuHRRx8lLi7OqiT6wQcf8PHHH+Pl5QUYw1mffvrpAvdZsmQJoaGhlClTBj8/PwICAti6dSsdO3bMtZ1SisuXjZPKpUuXLJ+hFZGMVOMEn3kNjq+HC9GQcBgunIT0ZLh2wTipZjfhpBdwJV3J1zi5upQBV3eo381o/ilf1bgiEIE67cC9mrFNGQ/jiiD7BF6M+6RMWcKD0zdwPCGZ++u68Puv69m2ewdubm707duXAQMGEBgYaNWxnn/+eb755hs6deq0H6PPuC/wm/ntKSLysbVx2TNhLAVeUkqFY0xwf6lQ+i9+exPO7Lvrw+RSqxn0m3zLzUJDQ3nvvfcYOHAge/fuZeTIkbeVMJYtW0azZs04cOAALVu2vOnX8Y3Kli3LokWLqFixIufPn6dDhw6EhISwcuVKvLy8WL58OWCcGJOSkli0aBGHDh1CKcXFixdzHat///4899xzeHh43NT0s3r1aqKioti6dSsiQkhICBs2bMDX15fDhw/zv//9j6+++uqm+GJiYjhz5gzt2rVj6NCh/PLLL7z++uu3/Dvs37+fNm3yvuyeMWMGkPvKCCAuLo4OHTpYln18fIiLi7tp/5kzZ9K/f3/KlStHxYoV2bJli+W9BQsWsGHDBho0aMCUKVOoU6fOTftrN0g+B9cuGm3yWZlG086V05CWDOcijdeX4+FijPE6KyPv47i6QyUfqBEEVeoZJ3dnN+Pkjhjr3auBixvUbArunkX5LYvMhavpVHZ3xdlJMbp3Q7wql+Xw5jVc7dgBd3d3ALp06cKiRYt48MEHefHFF0lISMDd3Z1vv/2WRo0a5Tre6dOnuXz5cs4fTj8CD3A9YdwWmyUMpVQY0BWoppSKBd7BaIpDRGYAK4D+wFEgBfiHrWIpKs2bNyc6OpqwsDD69+9v9X7dunXD2dmZ5s2bM2nSJDZs2GDVfiLCuHHj2LBhA05OTsTFxXH27FmaNWvG6NGjGTt2LAMHDqRz585kZmZStmxZRo0axYABAxg4cKDV8a1evZrVq1fTqlUrwLiyiYqKwtfXl7p16+Y6UecUHh7O0KFDASOZPvXUUwUmDGtGId2YKLLlNdVwXsebMmUKK1asoH379nz00Ue8/vrrzJw5k/vvv59hw4ZRpkwZZsyYwRNPPMEff/xxy3gcWkqScdJPOGwkg4TDRpNP9vorVvy+q1AbPGoYP7oa9jV+6ZfzNK4AnJyh3r1QuXj2SxYlEWHx7jje/TWSsX0bMaydL32b1gLAtWlTxo8fT2JiIuXKlWPFihUEBwfzzDPPMGPGDAIDA/n777954YUXbvo3GxcXh4+PT85V2X3F2V5SSo0AtgP/EpELBcVpy1FSw27xvgAvFvoHW3ElYEshISGMHj2a9evXk5iYaNU+2X0Y2Zo0acKePXvIysq6qU8gpzlz5pCQkMCOHTtwdXWlXr16pKam0qBBA3bs2MGKFSt466236N27NxMnTmTr1q2sXbuW8PBwpk2bZvUJUUR46623buqDiI6Opnz58vnuFxYWxtmzZ5kzZw4A8fHxREVFERgYSLly5UhPT8fNzQ2ApKQky9+gSZMm7Nixg+7du1sVHxhXFDEx18dQxMbG3tTclJCQwJ49e2jfvj1g9Fn07WuM/K5ataplu6effpqxY8da/dklmojRLHTyL6Nz9/wRo8kouoArY5dyRnt+ZV+o3dJo9slun3d2NfoTKvtCRW9zW71WkPiL1xi/aB/rDifQyrcywXWr5Hq/cePGjB07ll69euHh4UGLFi1wcXHhr7/+4uGHH7Zsl5aWdtOx8/ohxfW+4unA++bl94FPgJEFxeoQd3oXJyNHjqRSpUo0a9aM9evX39Ex6tevT3BwMO+88w7vvfceSimioqKIjIxk0KBBlu0uXbpEjRo1cHV1Zd26dZw8aVQvjo+Px9PTk8cee8wyEig5OZmUlBT69+9Phw4dCAgIsDqePn36MGHCBIYPH46HhwdxcXG3LI9x+PBhrl69mqtZ6J133iE8PJwJEybQpUsXfvrpJ0aOHMm1a9eYO3cuH374IQBvvfUWb7zxBsuWLaNWrVqkpaXx9ddf88orr+T7eSEhITz66KO8/vrrlsTUrl27XNtUqVKFS5cuceTIERo0aMCaNWto3LgxYFy6165dG4ClS5da1juEi6dg/0IwpUPiUUg+C+kpEL/TuHK4Ubkq0GyocTVQu4XRHFS1PpStBC5li3Xbf0mzZHcc4xftx5QlTBwYxBOd6uHsdPPf96mnnuKpp54CYNy4cdSqVYvKlSuze/fuXNuZTCZLc25ISAjPP/88sbGxOTex9BWLyNnslUqpb4Flt4pXJ4xC5uPjwz//+c8835s1axaLFy+2LOdsP7/RzJkz+de//kVAQADu7u6WYbU5DR8+nPvvv5/g4GBatmxpab/ct28fY8aMwcnJCVdXV6ZPn86VK1cYNGgQqampiAhTpkyx+jv17t2bgwcPWtpBPTw8+OmnnwrsYwkLC2Pw4MG51j344IOEhoYyYcIEPv/8c5599lmmTp2KiDBixAjuu+8+wOhPOXv2LD179kREUEoxcqTxwye/PowmTZowdOhQgoKCcHFx4csvv7TE179/f2bOnImXlxfffvstDz74IE5OTlSpUoXvv/8egKlTp1oGBHh6elpGTpUol+IgdpvRZ3BiAyBwZBW5Bh9mDy31qAn+3YxmoRpB4NMWvFtD+RpQwFWtVrgqlXOlZZ3KfDCkGXU83fPd7ty5c9SoUYNTp06xcOFCNm/eTHh4OPPmzePhhx9GRNi7dy8tWrS4KYlUqFAh57lmBPAFwA33vQ0G8qzKkZPK55Kl2AoODpYbJ1A6ePCgY/0i1IqtYvNv7dpFo9nozD7YNQcux96wgTI6kqvWN5qK2j0DzR4GZ/0b0Z4yTVl8t+kEGaYsXupujHLK/lFUkM6dO5OYmIirqyuffvopPXr04MSJEzz//POcPn2ajIwMQkNDmThx4k37bt++nSeffJIDBw6kATOBl83DamcDLTF+UUQDz95q4JH+16NpJYEp09zX8Cds/AQu3jB5mlsFaDII/LoYfQqV6+qmo2ImMv4yYxfsZV/cJQY0r21JFNYM9shrtKWfnx8rV6685b7BwcHs378fpdR+EXkpe72IPH6bX0EnDE0rdlKSjKalswcgfpdxx3Lcjuvvu7obVwv+3cC3gzEM1angIdia/aRlmpj2x1Gmrz9GZXdXvhremn5Na5XI2mQOkzCsuazTtLtR6M23aVcg+k8jOSQdN4atnjtw83bu1SBoEHj6Q/3uUPcenSBKkOjzKcyIOEZISy8mDAiiSnk3e4d0xxwiYZQtW5bExERd4lyzmez5MMqWLXv7O2emGaOTLsbAkd/gzH6I237zdi5ljWGqNZuAVyuo097okNb9DiXO1bRM1kSe5YFW3jSsVYG1r3fFt2r+ndolhUP8S/Tx8SE2NpaEhIRbb6xpdyh7xr18ZZmMG9rORsLZfXDwV+OO55TzubdzcgGfdsbdyi0fNQ9bDdB9Dg5iY1QCby3cR9zFazT1rkhAjQoOkSzAQRKGq6urngVNK1qmTEg+A/sXwL55kHTi5rpHFb2N5ND6CeN+hopeUKs5VLKuKKVWslxKyeA/KyKZuz0W/2rl+eWZjgTUqGDvsAqVQyQMTSsSx9bB318bQ1lvHMZayRcCehhXDj7BRpOSSxn7xKkVOVOW8OCMvzhx/iovdK3PKz0CKevqeP1MOmFoWl6ysowO6CMr4XgEXE2AhEPGe06uENjH6ID29IfAXro5qZRKuppO5XJGscAxfRriXbkcTb0r2Tssm9G3dGoaGE1MZ/bDX9Ng2evwXhWYcS/8Mcm4QS4jBe57A8YcZ2W7n2j43h4CHvuYyQt33pQsNmzYQOvWrXFxcWH+/Pm53vvhhx8IDAwkMDCQH374AYArV67QsmVLy6NatWqW+Q5mzJhBs2bNaNmyJffeey+RkZEArFmzhjZt2tCsWTPatGmjCyUWMRFhwY5Yun28nvBtRg2zPk1qOXSyAK5PfFNSHm3atBFNu2smk8ix9SJr3xf5ZYTIOxVzP6a1F/nfAJGYbSIZqZbdMjMzxd/fX44dOyZpaWnSvHlzOXDgQK5DnzhxQvbs2SOPP/64zJs3z7I+MTFR/Pz8JDExUZKSksTPz0+SkpJuCq1169YSEREhIiKXLl2yrF+yZIn06dNHRER27twpcXFxIiKyb98+8fLyKry/jVagmKSr8vh3f0vdsctkyFd/StTZK/YOySrAdrnL869uktJKl6TjsGaiMYIpp7r3GndI1+8G3sHglveolq1btxIQEGCZzCo0NJQlS5YQFBRk2aZevXoAN1UaXrVqFb169cLT05jLoVevXqxcuZJhw64Xdo6KiuLcuXN07twZgIoVK1reu3r1qmXYeHapeTDqaKWmppKWlkaZMrrfxJYW7Yrl7UX7EeDdkCY83qEuTnkUC3RUOmFojsuUYdwUt2uOMa3nuYNgylECus0/oMsbxpwNVvZBxMXF5ZpYycfHh7///vuO971xkqewsDAeeeSRXPcTffnll3z66aekp6fn2fS0YMECWrVqpZNFEfAsX4Y29Tz57+Cm+FRxjKGyt0MnDM3xRK2BzdOMKUCzubpD5TrG6KU2Txp3S99BR7VYOVHTne4bHh5+0/zlL774Ii+++CI///wzkyZNsvR9ABw4cICxY8eyevVqq2LQbk+GKYtvNx4n0yS80iOQLg2qc19gtVJ7g7BOGFrJl5wAUauN4a7nDphLewON74cKXtDppUKb1c2aiZoK2jfnHCmxsbF07drVsrxnzx4yMzPznZ42NDSU559/Ptf+gwcP5scff6R+/fq390W0W9ofd4mxC/ZyIP4y97fwuq1igY5KJwytZMpMh5ObjFFMOQvzATTsD/eNBu+8T7x3o23btkRFRXHixAm8vb0JDw/n559/tmrfPn36MG7cOC5cMGbBXL16NR988IHl/bCwsFz9GYBlhkKA5cuXW15fvHiRAQMG8MEHH3DPPfcUxlfTzFIzTExdG8XXG45Txd2NGY+1pm/T2vYOq3i4217zon7oUVKl3MnNIr++lntEU9ijIie3iKRcKJIQli9fLoGBgeLv7y+TJk0SEZEJEybIkiVLRERk69at4u3tLe7u7uLp6SlBQUGWfb/77jupX7++1K9fX77//vtcx/Xz85ODBw/mWvfKK69IUFCQtGjRQrp27Sr79+8XEZH3339f3N3dpUWLFpbH2bNnbfm1S41Dpy9LwLjlMnrubrl4Nd3e4RQaCmGUlENMoKQ5uKTj8Os/jXLfKeZ50t2rQdMHoeMLRnlvTbsLV9MyWXXgDENaG7XCYpJSCpwBryRSSu0QkeC7OYZuktKKJ1MGzB8JB5fmXt/tbWg0AGoG5b2fpt2miCMJjFu4j/hL12juU4mAGhUcLlkUFp0wtOLDlAmHl8PuMDgRYdxd7VwG2j8L/l0goKe9I9QcyIWr6by/PJKFO+OoX7088551vGKBhU0nDM3+rpyBte/D7p+ur3OrAA/MgJbD8t9P0+5QdrHAk4kpvNQtgJe6BzhkscDCphOGZj+Jx+DPz2Dnj9fXdRsPLUILbRispuWUmJxGFXc3nJ0Ub/ZthHeVcjTxcvD6T4VIJwytaGWmQeQS2DX7+v0SlepA70nGNKSleIy7ZjsiwrwdsUxaFsnYfo0Y3r4uvZvUsndYJY5OGFrROB8F0++5XpqjnCe0egxaDoe6newbm+bQYpJSGLdoHxujztOunicd/avaO6QSSycMzbauJsKSF425rLMN/MxIFC5u9otLKxUW7ozl7cX7UcD7DzRleDvfUlUssLDphKHZxtG18NOQ3Oue+h3qtLVPPFqpVM2jDO38PPnP4GZ4Vy5n73BKPJ0wtMKVkQq/jcndkf3oXGjQx34xaaVGhimLryOOYcqCf/YM5L4G1bmvQXV7h+UwdMLQCs+aicaMdWIy6jgN+wU89P+sWtHYH3eJMfP3cvD0ZQa1vF4sUCs8OmFody/LBD8/AkfXGMvDfoGGfe0bk1ZqpGaY+Oz3KL7deBzP8m58/Xgb+ugRUDZh04ShlOoLfA44AzNFZPIN7/sCPwCVzdu8KSIrbBmTVsgSj8FXHcCUbiy/vBOq6lLbWtE5lZTCd5uO81BrH8b1b0wld1d7h+SwnG69yZ1RSjkDXwL9gCBgmFLqxgJAbwNzRaQVEAp8Zat4tEKWfhUWjIIvWhvJoskQePscK7dF0bBhQwICApg8efJNu3366acEBQXRvHlzevTowcmTJwFYt24dLVu2tDzKli3L4sWLAWMM/fjx42nQoAGNGzdm6tSpAMyZM4fmzZvTvHlzOnXqxJ49e4ru+2t2dSU1g3nbjXlJGtSswLrRXfm/h5rrZGFjtrzCaAccFZHjAEqpcGAQEJljGwGyJy2uBMTbMB6tMFy7CDN7QmKUsVw1AO7/HOrdi8lk4sUXX2TNmjX4+PjQtm1bQkJCcs133apVK7Zv3467uzvTp0/njTfe4JdffqFbt27s3r0bgKSkJAICAujduzcAs2bNIiYmhkOHDuHk5MS5c+cA8PPzIyIigipVqvDbb7/xzDPPWD1dqlZyrTt0jvGL9nHmciqtfCsTUKNCqZwu1R5smTC8gZgcy7FA+xu2+TewWin1MlAeyLO6nFLqGeAZAF9fXTLCLuJ2wtwn4NKp6+s6jzZKeTgZF6pbt24lICAAf39/wJghbsmSJbkSRrdu3SyvO3TowE8/5agfZTZ//nz69euHu7txEpg+fTo///wzTubPqVGjBgCdOnXKdazY2NhC+rJacZR0NZ33l0WyaFccgTU8mP98J10ssIjZrEkKyGt4wo2TbwwDZomID9AfmK2UuikmEflGRIJFJLh6dT3qpkht/x6+7wffdjOSRfkaxjDZf1+CHhMsyQIgLi6OOnXqWJZ9fHyIi4vL99Dfffcd/fr1u2l9eHh4rpnnjh07xi+//EJwcDD9+vUjKirK6mNpjsGUJTw0/S9+3RPPKz0CWfbKvbT2rWLvsEodW15hxAJ1ciz7cHOT01NAXwAR2ayUKgtUA87ZMC7NGqmXYfYD16c/bTIY2j0LdTvmu0tek3HlN6zxp59+Yvv27URERORaf/r0afbt20efPtfv20hLS6Ns2bJs376dhQsXMnLkSDZu3Gh5f926dXz33Xds2rTpdr6hVgIkXEmjanmjWOC4/o3xrlKOxrUr3npHzSZseYWxDQhUSvkppdwwOrVvmA2HU0APAKVUY6AskGDDmLRbyTJB2DCYXMdIFlUDYfxZeHhWgckCjCuKmJjrrZCxsbF4eXndtN3vv//Of/7zH5YuXUqZMmVyvTd37lwGDx6Mq+v1zksfHx8efPBBAAYPHszevXst7+3du5dRo0axZMkSqlbVNYIchYjwy7ZTdP9kPT9vNZpBewbV1MnCzmyWMEQkE3gJWAUcxBgNdUAp9Z5SKsS82b+Ap5VSe4Aw4EkpaXPGOooL0RDxIUyqAYdXAAoGfQUvbgXXslYdom3btkRFRXHixAnS09MJDw8nJCQk1za7du3i2WefZenSpZa+iJzCwsJyNUcBPPDAA/zxxx8ARERE0KBBAwBOnTrFkCFDmD17tmWdVvKdSkxh+My/GbtgH0G1K3JvQDV7h6SZ6Tm9S7vzR2HeE3B2v7FcpR60HgH3vApOtz+hzIoVK3j11VcxmUyMHDmS8ePHM3HiRIKDgwkJCaFnz57s27eP2rVrA8YghqVLjQvP6Oho7rnnHmJiYiwd3AAXL15k+PDhnDp1Cg8PD2bMmEGLFi0YNWoUCxYsoG7dugC4uLig/22UbPN3xDJh8X6cnRRv9W/EsLa6WGBhKYw5vXXCKK0y0+CvqfDHJGPZvyt0nwA+d/XvSdPuysaoBGb9Gc2kwU2pXUkXC8WRyD8AACAASURBVCxMhZEwdGmQ0iYrCyImw4aPQLKMdcPnQ2Av+8allUrpmVlMX3+MLBFe69WAzoHV6RyoR0IWVzphlCaJx2Dpy3DyT2O508vQ7W2r+yg0rTDtibnIG/P3cvjsFYa08tbFAksAnTBKgytnYdmr5s5swLcTPPQ9VKxt37i0UulauolP1xzmu00nqFGhLDNHBNMzqKa9w9KsoBOGozu+3rhDO/Ui1GgCfSaBfzc9d7ZmNzEXUvjhr5OEtvPlzX6NqFhW138qKXTCcFSHV8K6SXBmHzi5wCM/QeP77R2VVkpdTs1g5f4zDA2uQ4OaFVg/piteega8EkcnDEeTdBzmjjASBRhzZ/d6H8rrm9o0+/jj0FnGLdzPuSuptPatQkAND50sSiidMByFCOyeA0teNJbL14CnVoOnn33j0kqtxOQ03lsWyZLd8TSsWYEZj7choIaHvcPS7oJOGI7iz8/h93eM1w99D00ftG88WqlmyhIenrGZmAspvNazAc93rY+biy0rEWlFwaqEYa4F5SsiR20cj3Y7RGDfPFj0rHFPhVsFeHUvuHvaOzKtlDp3JZVq5cvg7KQYP6AxPlXcaVhLlyB3FLdM+UqpAcA+YI15uaVSapGtA9NuIeMavFsZFj4N5atD3XvhyWU6WWh2kZUlzPn7JN0/jmCOuVhgj8Y1dbJwMNZcYbyHMfHROgAR2a2UCrBpVFrBUi8b1WQBfDvCiKXg4mbfmLRSK/r8Vd5cuJctx5PoVL8qXfSd2g7LmkbFDBG5eMO6klWAypHsXwBTWxqvqwbAyJXUb9gYpRRKKdzc8k8cNWvWRClF165dc61/8803UUrlKjU+b948yzGVUrRv3z7XMW48ToMGDXJtn3OWPc1xzd0eQ5/PNnAg7jKThzRjzqj2+FbV06U6KmsSxkGl1FDAyTy3xWfAFhvHpeVlxRiYPxJSEuGB6fDyDpKSkjh+/DgvvPACUVFRZGRk8PDDD9+066pVqyxzYd/o//7v/25aN3ToUDw8PBARoqKiGD16NGBMsfr444/ftL2fnx+TJk1CRHjggQc4ePDgXX5ZrSTwrlyO+xpUZ83rXQht56tLezg6ESnwgTHX9v8Bu8yPyUC5W+1nq0ebNm2kVNo5W+SdiiLv1xC5fMayul27dmL8ZzS4uLiIi4vLTbsrpaROnToCSJcuXSzrvby8RCklzs7O4ubmJiIiv/76qwCSlpaWbzg3HientLQ0AeTAgQO3+SW14i41I1M+XX1YPll1yN6haLcJ2C53ef615gqjj4iMFZFW5sebgJ48uSidPXD9/op/7oUK1+vuREdH59rUw8ODzMzMXOv69OmDiHDq1Klc6zds2EB8fDy///57rvVffvklAGXKlLE0MW3YsMHqcLPn9dbNUo5l16kL3P/FJj5fG0XcxdQ8p+TVHJs1CePtPNaNL+xAtHwkHIHpnYzXI5bmShb5ydkskJyczOrVq3n11Vdv2q5r165Ur16d7t2751p/7do1ADp16mSpINqtWzerwn3ooYc4d+4czz77rFXba8VfSnom7y+LZMj0v7iSmsn3TwbzydAWuvmpFMo3YSil+iilpgDeSqlPczxmAllFF2IpdjYSvmxrvG79BPh3YeXKlTRs2JCAgAAmT55MvXr1cu2SnJyMs7Mz8+fPRynFjBkzAPjss88s/4NHRESglEJESEhIQCmFyWQiPT0dZ2dnS2f2V199RceOHXFzcyMrK4vLly/n+qwdO3bQpEkTmjVrRmpqKhMnTmTBggW0adPG8rlayRd34Rqzt5xkeHtfVr92H90b6cqypVZ+bVVAK+Ap4KT5OfsxFKh2t21hd/ooNX0Y2/9n9Fm8U1Fk9oMiIpKZmSn+/v5y7NgxSUtLk+bNm8sff/whgLzyyisSFRUlgISEhEjnzp2lffv2sm3btlyHBSQ4OFj8/PxERCQjI0OqV68uTk5O4ubmJmPGjJF33nlHAPH09JT169eLs7OzAPL2229b9gEk+7/F+fPn5ZdffhFAatasWXR/I81mLqakS9jfJy3L8RdT7BiNVhiwZR+GiOwSke+AhiLyXY7HXBE5b7sUprHrJ/j1n8ad289EwGPzAdi6dSsBAQH4+/vj5uZGaGgoW7ZsoW7dukydOpXAwEBcXFzw8/Nj06ZN7Nq1K8/Dnz17lmHDhgHXfzBku3z5Ml5eXvTt25ekpCS6du2KyWRi6tSpLFiwgNq1a+PqapSj3rFjB0opqlatyvDhwy3Hzu73+OKLL2z5V9JsZPWBM/T6NILxi/dz9FwygJ4uVQOs68PwVkqFK6X2KqWOZD9sHllplHENwodf7+AeFgZeLS1vx8XFWTqUAXx8fIiLiyM6Otpy4t+6dSsxMTFkZWXRsWPHmz5CRHB1dbUkDFdXV6ZPn0758uXx9PQkMjKSp556it9++42OHTuyePFiRISMjAxiYmI4ffo0U6ZM4bHHHqN37960atWKDz/8kIyMjJt+jbz88su2/Xtphep8chov/byTZ2bvwLO8G4te6KSLBWq5WJMwZgH/AxTG6Ki5QLgNYyqdsrJg1gA4tAzKVobnN4Nf51yb5LwSyJaz4zErK4vXXnuNTz75JN+P+fvvv3F3d6dp06YAZGRkMH36dHbt2kV8fDzNmzfngw8+AOD777/nyy+/pE2bNly5csVyU2BmZiabNm1izpw5bNq0iUWLFrF27dq7/hNo9mPKEh6a/herD5xldO8G/PryvTT3qWzvsLRixprSIO4iskop9bGIHAPeVkpttHVgpUrqZZh+D1w6ZVSZffC7PGfE8/HxISYmxrIcGxuLl5eXZfnKlSvs37/f0ml95swZQkJCWLp0KcHBwQCEh4dbri4Adu/eDUD9+vUB44a9yZMnA9CoUSNWr14NwJEjR1i+fLklji5dulCtWjUA+vfvz86dO+nRo0eh/Dm0onP2cirVPYxige/c3wSfKuUIrKnrP2l5s+YKI00ZP2OPKaWeU0rdD9SwcVylhwhMbWUkC+/gfJMFQNu2bYmKiuLEiROkp6cTHh5OSEiI5f1KlSpx/vx5oqOjiY6OpkOHDrmSRVZWFvPmzSM0NNSyj7e3N5GRkSQkJACwZs0aGjduDGC5MzwrK4tJkybx3HPPAcZ9HXv37iUlJYXMzEwiIiL0PRclTFaWMHvLSXp8EsGcv08C0K1RDZ0stAJZc4XxGuABvAL8B6gEjLRlUKXKnjBIOQ+1msHTBTfruLi4MG3aNPr06YPJZGLkyJE0adKEiRMnEhwcnCt55GXDhg34+Pjg7+9vWefl5cU777zDfffdh6urK3Xr1mXWrFkAhIWFWW7iGzJkCP/4xz8AqFKlCq+//jpt27ZFKUX//v0ZMGDAXfwRtKJ0PCGZNxfuY+uJJO4NqEbXhvr3n2YdlVe7+C13UspHRGJtEM8tBQcHy/bt2+3x0YUvfjd80wXKVYHRR8FZz2el2dYv204xcckByrg48fbAIB5u46NvwCsllFI7RCT4bo5R4BlKKdUW8AY2ich5pVQTYCzQHfC5mw8u9Q6tgHBzX0LvSTpZaEXCp4o7XRtW5/1BTalRsay9w9FKmHzPUkqpD4AHgT0YHd2LgH9iFCJ8rmjCc1BxOyH8UeP1AzOg5bCCt9e0O5SWaeKLtcZEmaP7NOSegGrcE1DNzlFpJVVBP2sHAS1E5JpSyhOINy8fLprQHNSfn8OaieBaHp7bCFXr2zsizUHtOJnEG/P3cizhKkODfSx1wTTtThWUMFJF5BqAiCQppQ7pZHGXLp82kgUY06nqZKHZwNW0TD5adZgfNkfjVakcP4xsR5cGehY87e4VlDD8lVILza8VUC/HMiIy5FYHV0r1BT4HnIGZIjI5j22GAv/GmMVvj4g8an34Jcjh3yDMPJy193/Au7V949EcVvzFa/y89RQjOtRlTN9GeJTR/WNa4SjoX9KDNyxPu50DK6WcgS+BXkAssE0ptVREInNsEwi8BdwjIheUUo45vi/LdD1Z9J0M7XUXkFa4LqVksHzfaR5t70tgzQpsfKMbNXWntlbI8k0YInK3tR7aAUdF5DiAUioco18kMsc2TwNfisgF82fmPYdoSZaeAv+tbbxu9Rh0eN6+8WgOZ+X+M0xYsp+kq+m09/ekfnUPnSw0m7DmTu875Q3E5FiONa/LqQHQQCn1p1Jqi7kJ6yZKqWeUUtuVUtuz70guMZa9Zjx7+sP9unqrVnjOXUnlhTk7eO6nHVT3KMOSF++hfnVdLFCzHVs2buY1HOPGuwRdgECgK8Z9HRuVUk1F5GKunUS+Ab4B48a9wg/VRk5tgb3h4FETXt6Zb8kPTbtdpixh6IzNxF9KZUyfhjxznz+uzrb8/adpt5EwlFJlRCTtNo4dC9TJseyDMTT3xm22iEgGcEIpdRgjgWy7jc8pnjJS4eehxuvHF+lkoRWK05euUbNCWaNYYEgT6lRx1yXItSJzy58kSql2Sql9QJR5uYVSypq2lW1AoFLKTynlBoQCS2/YZjHQzXzcahhNVMdvI/7iSQR+fhhSL0Hnf0HNJvaOSCvhsrKEWX+eoMcnEfyUXSywYQ2dLLQiZc0VxlRgIMbJHRHZo5TqdqudRCRTKfUSsApjWO33InJAKfUexlSBS83v9VZKRQImYIyIJN7hdyk+lr4EJzZAo4HQfYK9o9FKuKPnknlzwV62n7zAfQ2q072RYw4m1Io/axKGk4icvOEOUZM1BxeRFcCKG9ZNzPFagNfND8dw6m9jilXnMjD0R90Upd2V8K2nmLj0AOVcnfnk4RYMae2t79bW7MaahBGjlGoHiPneipcBPUVrXtJT4PvexuvHF4KTs33j0Uo836ru9Gxcg3dDmlK9Qhl7h6OVctYkjOcxmqV8gbPA7+Z12o2yCwp2Hg317rVvLFqJlJphYuraKADe6NuITvWr0am+LhaoFQ/WJIxMEQm99Wal3IkNcHwd1GoO3d+2dzRaCbQ9Ook3FuzleMJVQtvW0cUCtWLHmoSxzTzc9RdgoYhcsXFMJdOKN4znId/ofgvttiSnZfLRykP8uOUk3pXL8ePIdtyniwVqxdAtE4aI1FdKdcIYFvuuUmo3EC4i4TaPrqTYMQsSDkK9zlCjsb2j0UqYM5euEb4thic61mNMn4aU18UCtWLqtqZoNc+L8RkwXETs0qNb7KZojdkK3/UyXv9zL1Spa994tBLhwtV0lu07zeMdjH8v5y6n6hnwNJuy+RSt5g/xwCgaGAo0BpYAne7mQx3K1m+M5xFLdLLQbklE+G3/GSYu2c/FlAw61a9K/eoeOlloJYI11777gV+BD0Vko43jKVkuxcK+eeDVGvy72jsarZg7dzmVCUv2s+rAWZp5V+LHke11sUCtRLEmYfiLSJbNIymJ1pvng+r1nn3j0Io9U5bw8NebOXMplbf6NeKpe/1w0cUCtRIm34ShlPpERP4FLFBK3dTRYc2Mew7NlAm7ZkONJuDX2d7RaMVU/MVr1KpoFAt8b1BT6lQph7++qtBKqIKuMH4xP9/WTHulxl7zILGWw+wbh1YsmbKEHzdH8+HKw7zVvxEjOtbT82prJV5BM+5tNb9sLCK5koa5qODdzshXconAxk+N1y2H2zcWrdg5eu4Kb8zfy85TF+nasDo9Gte0d0iaViisaUQdmce6pwo7kBIlJRGSjkGLYeDuycqVK2nYsCEBAQFMnjz5ps1fe+01WrZsScuWLWnQoAGVK1e2vNe3b18qV67MwIED8/yol19+GQ+P600YJ0+epEePHjRv3pyuXbsSGxtree/UqVP07t2bxo0bExQURHR0dOF9Z80qP/99iv6fb+LE+atMeaQF/3uyLd6Vy9k7LE0rFAX1YTyCMZTWTym1MMdbFYCLee9VSmz91niu1xmTycSLL77ImjVr8PHxoW3btoSEhBAUFGTZfMqUKZbXX3zxBbt27bIsjxkzhpSUFL7++uubPmb79u1cvJj7Tz169GhGjBjBE088wR9//MFbb73F7NmzARgxYgTjx4+nV69eJCcn4+SkO1WLWr1q7vRuUpN/hzShmocuFqg5loLOKFuBL4Gj5ufsx3igt+1DK8YOLDKemzzA1q1bCQgIwN/fHzc3N0JDQ1myZEm+u4aFhTFs2PV+jx49elChQoWbtjOZTIwZM4YPP/ww1/rIyEh69OgBQLdu3SyfFRkZSWZmJr16GTcRenh44O7ufldfU7u11AwTH/x2kMm/HQKgU/1qTHu0tU4WmkPKN2GIyAkR+V1E2orI2hyPreYpVUunpBNw/rAxOZJbeeLi4qhT5/pMtD4+PsTFxeW568mTJzlx4gTdu3e/5cdMmzaNkJAQateunWt9ixYtWLBgAQCLFi3iypUrJCYmcuTIESpXrsyQIUNo1aoVY8aMwWSyatoS7Q79fTyRfp9v5OuI41xJzeB2qiZoWkmUb8JQSkWYny8opZJyPC4opZKKLsRiJsL8i7/jSwB5niTyqzAaHh7OQw89hLNzwVVV4uPjmTdvHi+//PJN73388cdERETQqlUrIiIi8Pb2xsXFhczMTDZu3MjHH3/Mtm3bOH78OLNmzbq976ZZ5UpqBm8v3scj32zBlCX8PKo9/xncTFeW1RxeQcNqs6dh1cX4s52NhD0/g2t5qNsRMK4oYmJiLJvExsbi5eWV5+7h4eF8+eWXt/yYXbt2cfToUQICAgBISUkhICCAo0eP4uXlxcKFRpdScnIyCxYsoFKlSvj4+NCqVSv8/f0BeOCBB9iyZQtPPVW6xyfYwtnLaczfEcuoe/14vXcD3N10sUCtdCioSSr77u46gLOImICOwLNA+SKIrfhZY55dduiPllVt27YlKiqKEydOkJ6eTnh4OCEhITftevjwYS5cuEDHjh1v+TEDBgzgzJkzREdHEx0djbu7O0ePHgXg/PnzZGUZ/2k++OADRo4caYnjwoULJCQkAPDHH3/k6njX7k7S1XRmb44GIKCGBxvf6M7bA4N0stBKFWuG0SzGmJ61PvAjRgHCn20aVXGUkgRH10D56hDY07LaxcWFadOm0adPHxo3bszQoUNp0qQJEydOZOnSpZbtwsLCCA0NvanZonPnzjz88MOsXbsWHx8fVq1aVWAY69evp2HDhjRo0ICzZ88yfvx4AJydnfn444/p0aMHzZo1Q0R4+umnC/EPUDqJCL/uiafXpxG8tyyS4wnJAHq6VK1UumV5c6XUThFprZQaA6SJyFSl1C4RaVU0IeZmt/Lmi1+A3XOg30fQ/pmi/3ytyJ29nMr4Rfv5/eBZmvtU4sOHmtOoVkV7h6Vpd6RIypsDmUqph4HHgQfM61zv5kNLpN1zjGedLEoFU5Yw1FwscHz/xvzjnnq6WKBW6lmTMEYCL2CUNz+ulPIDwmwbVjFjMo8irnuPfePQbC72Qgq1K5XD2Unx/qCm+Hq6U69a6eyy07Qb3fInk4jsB14BtiulGgExIvIfm0dWnPz+b+M56IECN9NKLlOWMHPjcXp+GsFPW04CcF+D6jpZaFoO1sy41xmYDcQBCqillHpcRP60dXDFxpbpxnPbUfaNQ7OJw2eu8MaCveyJuUiPRjXo3UQXC9S0vFjTJDUF6C8ikQBKqcYYCeSuOk9KjMMrQUzGrHq6NpPD+WnLSd799QAVyrryeWhLQlp46RvwNC0f1iQMt+xkASAiB5VSbjaMqfgQgUXPGq9Dpto3Fq1QiQhKKQJqeNC/WW0mDgyiqq7/pGkFsiZh7FRKfY1xVQEwHNhVwPaO4+SfkHoR2j0LtZrZOxqtEFxLN/HpmsM4OSne6teYDv5V6eBf1d5haVqJYE0by3PAMeANYCxwHONub8d39HfjucPz9o1DKxSbjyXS9/MNfLvxBClpJl0sUNNuU4FXGEqpZkB9YJGIfFjQtg4p3nwh5eln3zi0u3I5NYMPVhwibOsp6lZ15+en29Opvi6Rpmm3q6AJlMZhzKy3E2irlHpPRL4vssjs7fReOL4e2jxp70i0u3TuchqLd8XxzH3+vNazAeXcCq4WrGla3gpqkhoONBeRh4G2wG23yyil+iqlDiuljiql3ixgu4eUUqKUKj4jr/76wngOzmuGWq24S0xOY9afJwCjWOCmsd0Y17+xThaadhcKapJKE5GrACKSoJS6rTGlSilnjBn6egGxwDal1NKcI67M21XAuDHw79uK3JauXYB9c6GcJ9RuYe9otNsgIizdE8+/lx4gOS2T+xpUx7+6hx4BpWmFoKCE4Z9jLm8F1M85t7eIDLnFsdsBR0XkOIBSKhwYBETesN37wIfA6NsJ3Kb2/GI83/uafePQbkv8xWu8vXg/fxw6R8s6lfnwoeb4V/ewd1ia5jAKShgP3rA87TaP7Q3E5FiOBdrn3EAp1QqoIyLLlFL5Jgyl1DPAMwC+vr63GcYdOBEByhnuecX2n6UVikxTFqHfbCHhShoTBgbxZKd6ODvpG/A0rTDlmzBEZO1dHjuv/1st4xjNTVxTgCdvdSAR+Qb4Bozy5ncZV8GysuDwCqjoY9OP0QpHTFIKXpXL4eLsxH8HN8PX0x3fqu72DkvTHJIta13EYszWl80HiM+xXAFoCqxXSkUDHYCldu/43mOeG6pFqF3D0AqWacrimw3H6PlphGUmvHsDq+lkoWk2ZMv5JbcBgeZy6HFAKPBo9psicokc84UrpdYDo0XEDrMjmWVlwZp3jNe6OarYOnj6MmMX7GVv7CV6BdWkX7Pa9g5J00oFqxOGUqqMiKRZu72IZCqlXgJWAc7A9yJyQCn1HrBdRJYWfAQ7OLgUUs5Dq8ehbCV7R6PlYfbmaN79NZJK5VyZ9mgrBjSrrYsFaloRsaa8eTvgO6AS4KuUagGMEpGXb7WviKwAVtywbmI+23a1JmCbOvmX8dz9bfvGod0ku1hgg5oVuL+FFxMGBuFZvnTUwNS04sKaK4ypwEBgMYCI7FFKdbNpVPYStRqqBkKFWvaORDNLSc/k41VHcHFWjOvfmPb+VWmviwVqml1Y0+ntJCInb1hnskUwdmXKhAsnoLyuMVRc/Hn0PH0+28D3f54gPTNLFwvUNDuz5gojxtwsJea7t18Gjtg2LDs4vNx49u1o3zg0Ll3L4L/LD/LL9hj8qpVn7rMdaefnae+wNK3UsyZhPI/RLOULnAV+5w7qShV7f5onSOrwgn3j0DifnMave+N5rkt9Xu0ZSFlXXf9J04qDWyYMETmHMSTWcaUkQdx2KFMJPKrbO5pSKeFKGr/uiWfkvX7Ur+7BprHddae2phUz1oyS+pYcd2hnE5FnbBKRPRxZZTw/ONO+cZRCIsLi3XG8+2skKWkmujWqgV+18jpZaFoxZE2T1O85XpcFBpO7RlTJF7fDePYpPtXVS4O4i9cYv2gf6w8n0NrXKBboV628vcPSNC0f1jRJ/ZJzWSk1G1hjs4js4doF49ldd6wWFaNY4GYSk9P59/1BPN5RFwvUtOLuTkqD+AF1CzsQuzp/GCoXQRVcjVOJKXhXMYoFTh7SHF9Pd+p46vpPmlYS3PI+DKXUBaVUkvlxEePqYpztQytCV89Ded3ZbUuZpiymrz9GzykR/Lg5GoB7AqrpZKFpJUiBVxjKKNLTAqN4IECWOOLdU6mXoGqAvaNwWAfiLzF2wV72x12mT5OaDNDFAjWtRCowYYiIKKUWiUibogqoyCUchowU3eFtIz/8Fc37yyKp7O7G9OGtdWVZTSvBrOnD2KqUai0iO20ejT3sCTOeA3raNw4Hk10ssFGtCgxq6c2EgY2p7K6HympaSZZvwlBKuYhIJnAv8LRS6hhwFWMmPRGR1kUUo23tmw+1mkO9e+0diUO4mpbJR6sO4+qsGD8gSBcL1DQHUtAVxlagNfBAEcVS9ETgUgxUqWfvSBzChiMJvLVwH/GXrvFEx3qWqwxN0xxDQQlDAYjIsSKKpehl339Rs6l94yjhLqVk8P7ySObviMW/ulEssG09fU+LpjmaghJGdaXU6/m9KSKf2iCeonXhhPFcLdC+cZRw56+m8du+07zQtT6v9NDFAjXNURWUMJwBD8xXGg5p2/fGc63m9o2jBDp3JZWlu+MZ1dnfUiywiq7/pGkOraCEcVpE3iuySOwhdpvx7O0Y/fdFQURYsDOO95dFci3DRI/GNfGrVl4nC00rBW7Zh+HQLsVCvc7gpJtQrBGTlMK4RfvYGHWe4LpVmPygLhaoaaVJQQmjR5FFYQ+Z6ZBxFereY+9ISoRMUxbDvt3ChavpvD+oCcPb18VJFwvUtFIl34QhIklFGUiRSzhoPLuWtW8cxVz0+avU8XTHxdmJDx8yigX6VNH1nzStNLpl8UGHlWQeIaWH1OYpw5TFl+uO0nvKBkuxwE71q+lkoWml2J2UN3cMJ/80nqs3sm8cxdD+uEu8MX8vkacvM6BZbQY297J3SJqmFQOlN2GYMoznSj72jaOY+d+fJ5i0/CCe5d2Y8Vgb+jatZe+QNE0rJkpvwji2Fjz9QZeuAK4XC2ziVYkhrbx5e0AQldxd7R2WpmnFSOlNGOkpuoYUkJyWyYcrD+Hm7MTbA4No5+dJOz9d1kPTtJuVzk5vEUg5D7VKd4f3+sPn6DNlA7O3nEQwrjI0TdPyUzqvMLL7L1xL54ifC1fTeX95JAt3xhFQw4P5z3WiTd0q9g5L07RirnQmjIwU47lSHfvGYScXUtJZfeAsr3QP4MXuAZRx0Xe6a5p2azZtklJK9VVKHVZKHVVKvZnH+68rpSKVUnuVUmuVUnVtGY9F8lnjOSuzSD6uODh3OZVvNhxDRPCv7sGfY7vzeu+GOllommY1myUMpZQz8CXQDwgChimlgm7YbBcQLCLNgfnAh7aKJ5fsm/ZKwT0YIsLcbTH0+DSCT1YfITrRuLrSI6A0TbtdtmySagccFZHjAEqpcGAQEJm9gYisy7H9FuAxG8Zz3Zm9xnOFmkXycfYSk5TCwXZp/gAAEsJJREFUWwv3senoedr5eTJ5SDNdLFDTtDtmy4ThDcTkWI4F2hew/VPAb3m9oZR6BngGwNfX9+4jS7tiPFdvfPfHKqayiwVeTMlg0gNNebSdry4WqGnaXbFlwsjr7JTnuE2l1GNAMNAlr/dF5BvgG4Dg4OC7H/t5eg+4eYCL483hcOL8VXzNxQI/eqgFdau641W5nL3D0jTNAdiy0zsWyDkMyQeIv3EjpVRPYDwQIiJpNoznukux4OpYJ9EMUxZfrI2iz5QN/PBXNAAd/7+9O4+uqr4WOP7dyWWGIKNFpoQCMmRCI0VcD0VAaV0BQSpQJyg8H7Y+aetE7Vu0UEt58qrYV1BBcFrVUhAwFSxSCqI+Rq1JIAHBMKoQiIBIiGTY749zwGsI5CTk3pubsz9rZZ17z3DP/t0kd9/fGfbvu60sWRhjakwoexhbgG4ikgB8CowBfhS8goj0AZ4Dhqpqfghj+bYvPoH2aWHbXahlHTzOI0uy2HHoJOkpVzAs1YoFGmNqXsgShqqWiMj9wCqc8cEXqup2EZkObFXVDGAWzrjhi8Wp6bRfVYeFKiYATh93pm2uDOluwmXhe3t4fEUObZo1YP7daQzpVbdP5BtjIiekN+6p6kpgZbl5U4MeDw7l/it09h6MK/qEfdc16WyxwOQOzRl9TUemfL8nzRvZpbLGmNDx353eBZ8407joPGxzsqiYmW/toEEglqnpvUiLb0lavBULNMaEnv+KDx7e5kwv7x3ZOKph7Y58bnpqPa9t3k8gVqxYoDEmrPzXwygpcqZRVNr8i1NnmP637Sz/6DO6X96UuXf0p08nKxZojAkv/yWMY/ugXnTd7XzidDFrcvOZPKgbPx3YlfoB/3UMjTGR57+EUXwaik9FOopKHTpRxPKPPuU/BnQhoXUT3ptyo53UNsZElP8Sxhd58J2kSEdxQarKX7YcYMaKXIrLyhja+zvEt25iycIYE3H+SxhfHYJW3SIdRYX2FZxiyuvZbMgroF+XlswcmUy8FQs0xtQS/ksYZwqhU79IR3GektIyfjR/EydOFzNjRBJjruloxQKNMbWKvxJG0Qko/Rpia0/RwU+OfEVnt1jgH253igW2a271n4wxtY+/Lrc5ddSZNmwe2TiAMyVlzP7HxwydvZ6XN+wDoF+XVpYsjDG1lr96GGWlzjTCd3l/dOA4jy7JYufhkwxPvYJb+7SPaDzGGOOFvxJGodvDkMh1rBa8t4ffrcihbbOGLLgnjUE9rVigMSY6+CthHN/vTBtdFvZdny0WmNqxOWP6dmLK93sQ19AulTXGRA9/JYxD2c60dfhKm39ZVMzvV+6gYb0Yfp3em6s7t+TqzlYs0BgTffx10ru02JmG6RzGP3IOM+TJd1i0ZT/1AzFWLNAYE9X81cMoLoRm7UBCe39DwVdfM+1vOWRkfkaP7zRj3l1ppHQM/2EwY4ypSf5KGIeyw3IPxsmiEtbuzOfng7tz3w3ftWKBxpg6wV8Jo1GLb05817DPjp9m2b8+5Sc3fJf41k14f8qNdlLbGFOn+CthlBZD2541+pJlZcqrm/cz860dlJYptyS1I751E0sWxpg6x2cJ4wzUb1xjL7fn6CmmvJ7Fpj1fcF3XVvx+RDKdWtXc6xtjTG3ir4RRUlRj92CUlJZx5/Ob+LKomCduS+aHaR2QEJ9MN8aYSPJXwsjPgaaXdmf17vyTxLdqQiA2hqdGp9K5VWMuj2tYQwEaY0zt5a/Ldxo2h5jq5civS0p5cvXHDJ39Li+5xQL7JrS0ZGGM8Q1/9TAKC6Bllypv9uH+Yzy6JItd+V8xsk97RlqxQGOMD/knYZwpdKZVHM97/vo8ZryVS7u4hrww/hoGXtk2BMEZY0zt55+EUVLkTNt4u6y2rEyJiRGu6nwZd3yvE48O7UEzu1TWGONj/ksYgQYXXe3E6WJ+tyKHRvVimTY80YoFGmOMyz8nvQsLnOlFxsJYtf0QQ558h9c//JQmDQJWLNAYY4L4p4dR7PYwGsadt+joV1/z6ze2syL7c3q1i2PhuGtIbB/5YVyNMaY28U/COHtIqnGr8xZ9VVTCu7uO8PDNV3LvgC7Ui/VPx8sYY7zyT8I4eciZBpz7Jj49fpplHx7kpwO7Et+6Cf/3y0E0beCft8MYY6oqpF+lRWSoiOwUkd0iMqWC5Q1EZJG7fJOIxIcsmDJn8KSy2Ia8smEvNz35DnPWfsK+AudyW0sWxhhzcSH7lBSRWGAOMAQ4CGwRkQxVzQlabQJwTFW7isgY4L+B0SEJyD0kNWnZft4+IPxbt9bMGJFEx5ZWLNAYY7wI5dfqvsBuVc0DEJG/AMOB4IQxHPiN+3gJ8CcREQ3B5UmlXxcSC2w7UsysUWmMutqKBRpjTFWEMmG0Bw4EPT8IfO9C66hqiYicAFoBR4NXEpF7gXsBOnXqVK1gYlsl8EXnoSy/dTBtWzSr1msYY4yfhfIcRkVf38v3HLysg6rOU9U0VU1r06ZN9aLpcQstxy9iyoOTadu2LYmJiRWuduzYMUaMGEFycjJ9+/Zl27Zt55Y9/fTTJCYm0rt3b2bPnn1ufmZmJtdeey1JSUmkp6fz5ZdfAnDmzBnGjx9PUlISKSkprFu37tw2ixYtIjk5md69e/PII49Ur03GGBNGoUwYB4GOQc87AJ9daB0RCQDNgS9CGBPjxo3j73//+wWXz5gxg9TUVLKysnj55ZeZPHkyANu2bWP+/Pls3ryZzMxM3nzzTXbt2gXAxIkTmTlzJtnZ2YwYMYJZs2YBMH/+fACys7NZvXo1Dz74IGVlZRQUFPDwww+zZs0atm/fzuHDh1mzZk0om22MMZcslAljC9BNRBJEpD4wBsgot04GcI/7eBTwz1Ccvwg2YMAAWra8cKmPnJwcBg0aBECPHj3Yu3cvhw8fJjc3l379+tG4cWMCgQDXX389y5YtA2Dnzp0MGDAAgCFDhvD666+f91pt27blsssuY+vWreTl5dG9e3fO9pYGDx58bhtjjKmtQpYwVLUEuB9YBeQCf1XV7SIyXUSGuastAFqJyG7gF8B5l96GW0pKCkuXLgVg8+bN7Nu3j4MHD5KYmMj69espKCigsLCQlStXcuCAc4omMTGRjAwnFy5evPjc/JSUFN544w1KSkrYs2cPH3zwAQcOHKBr167s2LGDvXv3UlJSwvLly89tY4wxtVVIbz5Q1ZXAynLzpgY9LgJ+GMoYqmrKlClMnjyZ1NRUkpKS6NOnD4FAgJ49e/Loo48yZMgQmjZtSkpKCoGA8/YtXLiQBx54gOnTpzNs2DDq168PwI9//GNyc3NJS0ujc+fO9O/fn0AgQIsWLXjmmWcYPXo0MTEx9O/fn7y8vEg22xhjKmV3q5UTFxfHCy+8AICqkpCQQEJCAgATJkxgwoQJADz22GN06NABcA5dvf322wB8/PHHrFixAoBAIMBTTz117rX79+9Pt27dAEhPTyc9PR2AefPmERsbG4bWGWNM9VnRpHKOHz/OmTNnAHj++ecZMGAAcXFOwcL8/HwA9u/fz9KlSxk7duy35peVlfH4448zadIkAAoLCzl1yhmwafXq1QQCAXr16vWtbY4dO8bcuXOZOHFimFpojDHV47sextixY1m3bh1Hjx6lQ4cOTJs2jeJip2zIpEmTyM3N5e677yY2NpZevXqxYMGCc9vedtttFBQUUK9ePebMmUOLFi0AeO2115gzZw4AI0eOZPz48YCTFG6++WZiYmJo3749r7zyyrnXmjx5MpmZmQBMnTqV7t27h6X9xhhTXRJtYz6kpaXp1q1bIx2GMcZEFRH5QFXTLuk1oi1hiMgRYF81N29NubvIfcDa7A/WZn+4lDZ3VtVq3vnsiLqEcSlEZOulZthoY232B2uzP0S6zXbS2xhjjCeWMIwxxnjit4QxL9IBRIC12R+szf4Q0Tb76hyGMcaY6vNbD8MYY0w1WcIwxhjjSZ1MGCIyVER2ishuETmvAq6INBCRRe7yTSISH/4oa5aHNv9CRHJEJEtE1ohI50jEWZMqa3PQeqNEREUk6i/B9NJmEbnd/V1vF5FXwx1jTfPwt91JRNaKyL/cv+8fRCLOmiIiC0UkX0S2XWC5iMgf3fcjS0SuCltwqlqnfoBY4BOgC1AfyAR6lVvnJ8Cz7uMxwKJIxx2GNg8EGruP7/NDm931mgHrgY1AWqTjDsPvuRvwL6CF+7xtpOMOQ5vnAfe5j3sBeyMd9yW2eQBwFbDtAst/ALyFM2JpP2BTuGKriz2MvsBuVc1T1TPAX4Dh5dYZDrzkPl4CDBKRioaLjRaVtllV16pqoft0I84IiNHMy+8Z4LfAE0BROIMLES9t/ndgjqoeA1DV/DDHWNO8tFmBOPdxc84f2TOqqOp6Lj7y6HDgZXVsBC4TkXbhiK0uJoz2QPBoRAfdeRWuo85ATyeAVmGJLjS8tDnYBJxvKNGs0jaLSB+go6q+Gc7AQsjL77k70F1E3heRjSIyNGzRhYaXNv8GuFNEDuKMv/Of4QktYqr6/15j6mK12op6CuWvHfayTjTx3B4RuRNIA64PaUShd9E2i0gM8BQwLlwBhYGX33MA57DUDTi9yHdFJFFVj4c4tlDx0uaxwIuq+gcRuRZ4xW1zWejDi4iIfX7VxR7GQaBj0PMOnN9FPbeOiARwurEX6wLWdl7ajIgMBn4FDFPVr8MUW6hU1uZmQCKwTkT24hzrzYjyE99e/7bfUNViVd0D7MRJINHKS5snAH8FUNUNQEOcIn11laf/91CoiwljC9BNRBJEpD7OSe2McutkAPe4j0cB/1T3bFKUqrTN7uGZ53CSRbQf14ZK2qyqJ1S1tarGq2o8znmbYaoazbXxvfxtL8e5wAERaY1ziCqax//10ub9wCAAEemJkzCOhDXK8MoA7navluoHnFDVz8Ox4zp3SEpVS0TkfmAVzhUWC1V1u4hMB7aqagawAKfbuhunZzEmchFfOo9tngU0BRa75/f3q+qwiAV9iTy2uU7x2OZVwE0ikgOUAg+rakHkor40Htv8IDBfRH6Oc2hmXDR/ARSR13AOKbZ2z8v8GqgHoKrP4pyn+QGwGygExocttih+X40xxoRRXTwkZYwxJgQsYRhjjPHEEoYxxhhPLGEYY4zxxBKGMcYYTyxhmFpHREpF5KOgn/iLrBt/oaqeVdznOrciaqZbVuPKarzGJBG52308TkSuCFr2vIj0quE4t4hIqodtfiYijS9138ZYwjC10WlVTQ362Rum/d6hqik4hSlnVXVjVX1WVV92n44DrghaNlFVc2okym/inIu3OH8GWMIwl8wShokKbk/iXRH50P3pX8E6vUVks9sryRKRbu78O4PmPycisZXsbj3Q1d12kDvOQrY7TkEDd/5M+WZ8kf9x5/1GRB4SkVE49br+7O6zkdszSBOR+0TkiaCYx4nI/1Yzzg0EFZ0TkWdEZKs442BMc+c9gJO41orIWnfeTSKywX0fF4tI00r2YwxgCcPUTo2CDkctc+flA0NU9SpgNPDHCrabBDytqqk4H9gH3VIRo4Hr3PmlwB2V7D8dyBaRhsCLwGhVTcKpjHCfiLQERgC9VTUZeDx4Y1VdAmzF6QmkqurpoMVLgJFBz0cDi6oZ51CcUiBn/UpV04Bk4HoRSVbVP+LUGRqoqgPdciH/BQx238utwC8q2Y8xQB0sDWLqhNPuh2awesCf3GP2pTg1ksrbAPxKRDoAS1V1l4gMAq4GtrglURrhJJ+K/FlETgN7cUpkXwnsUdWP3eUvAT8F/oQzvsbzIrIC8Fw+XVWPiEieWwNol7uP993XrUqcTXBKZQSPtna7iNyL83/dDmcwoaxy2/Zz57/v7qc+zvtmTKUsYZho8XPgMJCC0zM+b0AkVX1VRDYBtwCrRGQiTinol1T1lx72cUdwcUIRqXCMFLe+UV+cgndjgPuBG6vQlkXA7cAOYJmqqjif3p7jxBl5biYwBxgpIgnAQ8A1qnpMRF7EKcJXngCrVXVsFeI1BrBDUiZ6NAc+d8c4uAvn2/W3iEgXIM89DJOBc2hmDTBKRNq667QU7+OZ7wDiRaSr+/wu4B33mH9zVV2Jc0K5oiuVTuKUWK/IUuBWnHEcFrnzqhSnqhbjHFrq5x7OigNOASdE5HLg+xeIZSNw3dk2iUhjEamot2bMeSxhmGgxF7hHRDbiHI46VcE6o4FtIvIR0ANnGMscnA/Wt0UkC1iNc7imUqpahFMJdLGIZANlwLM4H75vuq/3Dk7vp7wXgWfPnvQu97rHgBygs6pududVOU733MgfgIdUNRNnLO/twEKcw1xnzQPeEpG1qnoE5wqu19z9bMR5r4yplFWrNcYY44n1MIwxxnhiCcMYY4wnljCMMcZ4YgnDGGOMJ5YwjDHGeGIJwxhjjCeWMIwxxnjy/3rdQFWH4CGvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7724523399472379\n",
      "precision_score:\n",
      "0.7752285593730208\n",
      "accuracy_score:\n",
      "0.7787\n",
      "Confusion Matrix:\n",
      "[[4722  984]\n",
      " [1229 3065]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81      5706\n",
      "           1       0.76      0.71      0.73      4294\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.77      0.77     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7724523399472379, 0.7752285593730208, 0.7787, 0.8533112008686465)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_roc(y_test1, probs_nn, 'MLPClassifier')\n",
    "\n",
    "matrix_info(0.53, y_test1, probs_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svc = LSVC(C = 3, loss = 'hinge', max_iter=10000)\n",
    "svc = CalibratedClassifierCV(svc)\n",
    "svc.fit(x_train1,y_train1)\n",
    "probs = svc.predict_proba(x_test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(tpr - fpr) w/ th =  0.4392937121505492\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_test1, probs, 'LinearSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1336s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (3.5927s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   42.9s\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:   45.2s\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   50.5s\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:   52.2s\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:   52.4s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:   58.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:  1.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=200,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_sta...\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['relu'],\n",
       "                         'alpha': array([0.1   , 0.01  , 0.001 , 0.0001]),\n",
       "                         'hidden_layer_sizes': [(10, 1), (16, 16), (10, 10),\n",
       "                                                (64, 1), (64, 64), (32, 32),\n",
       "                                                (32, 16)],\n",
       "                         'max_iter': [10000], 'solver': ['adam', 'lbfgs']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'solver': ['adam','lbfgs'], \n",
    "              'activation': ['relu'],\n",
    "              'max_iter': [10000], \n",
    "              'alpha': 10.0 ** -np.arange(1, 5), \n",
    "              'hidden_layer_sizes':[(10,1),(16,16),(10,10),(64,1),(64,64),(32,32),(32,16)]}\n",
    "cv = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1, cv = 5, verbose = 20)\n",
    "cv.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (32, 32), 'max_iter': 10000, 'solver': 'adam'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>7.990436</td>\n",
       "      <td>1.569940</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>0.772617</td>\n",
       "      <td>0.752323</td>\n",
       "      <td>0.750707</td>\n",
       "      <td>0.763232</td>\n",
       "      <td>0.761674</td>\n",
       "      <td>0.008841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>7.058115</td>\n",
       "      <td>2.703894</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.751716</td>\n",
       "      <td>0.760097</td>\n",
       "      <td>0.754747</td>\n",
       "      <td>0.747879</td>\n",
       "      <td>0.785051</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.013194</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.674869</td>\n",
       "      <td>1.525631</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.714574</td>\n",
       "      <td>0.758481</td>\n",
       "      <td>0.753939</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.763232</td>\n",
       "      <td>0.751899</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.320126</td>\n",
       "      <td>1.681171</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.731530</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>0.728889</td>\n",
       "      <td>0.785859</td>\n",
       "      <td>0.748101</td>\n",
       "      <td>0.020433</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.913055</td>\n",
       "      <td>2.750792</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.735567</td>\n",
       "      <td>0.765751</td>\n",
       "      <td>0.705455</td>\n",
       "      <td>0.754747</td>\n",
       "      <td>0.766061</td>\n",
       "      <td>0.745516</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.506609</td>\n",
       "      <td>1.948132</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.713363</td>\n",
       "      <td>0.760501</td>\n",
       "      <td>0.709899</td>\n",
       "      <td>0.766061</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.741477</td>\n",
       "      <td>0.024548</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9.704839</td>\n",
       "      <td>2.271285</td>\n",
       "      <td>0.016954</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.746871</td>\n",
       "      <td>0.682553</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.720404</td>\n",
       "      <td>0.734206</td>\n",
       "      <td>0.030471</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.900554</td>\n",
       "      <td>3.510965</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.766653</td>\n",
       "      <td>0.598950</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.728712</td>\n",
       "      <td>0.065155</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.403415</td>\n",
       "      <td>1.289711</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.768268</td>\n",
       "      <td>0.723748</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.771717</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.058188</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.644494</td>\n",
       "      <td>2.149511</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.716996</td>\n",
       "      <td>0.721325</td>\n",
       "      <td>0.715960</td>\n",
       "      <td>0.756768</td>\n",
       "      <td>0.701010</td>\n",
       "      <td>0.722411</td>\n",
       "      <td>0.018494</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.418736</td>\n",
       "      <td>1.055409</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.747679</td>\n",
       "      <td>0.711632</td>\n",
       "      <td>0.671515</td>\n",
       "      <td>0.730101</td>\n",
       "      <td>0.750303</td>\n",
       "      <td>0.722249</td>\n",
       "      <td>0.028919</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>4.339085</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.752523</td>\n",
       "      <td>0.766155</td>\n",
       "      <td>0.754747</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.756768</td>\n",
       "      <td>0.719502</td>\n",
       "      <td>0.076245</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.898115</td>\n",
       "      <td>1.589946</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.714978</td>\n",
       "      <td>0.711228</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.723636</td>\n",
       "      <td>0.748687</td>\n",
       "      <td>0.719098</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>4.873254</td>\n",
       "      <td>0.801719</td>\n",
       "      <td>0.009573</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.754542</td>\n",
       "      <td>0.752423</td>\n",
       "      <td>0.725253</td>\n",
       "      <td>0.736566</td>\n",
       "      <td>0.612525</td>\n",
       "      <td>0.716271</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>9.038930</td>\n",
       "      <td>3.347687</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.765442</td>\n",
       "      <td>0.651050</td>\n",
       "      <td>0.703434</td>\n",
       "      <td>0.753131</td>\n",
       "      <td>0.697374</td>\n",
       "      <td>0.714090</td>\n",
       "      <td>0.041304</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>5.658972</td>\n",
       "      <td>1.124508</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.664110</td>\n",
       "      <td>0.657512</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>0.763232</td>\n",
       "      <td>0.712231</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.210390</td>\n",
       "      <td>2.714899</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.763827</td>\n",
       "      <td>0.699515</td>\n",
       "      <td>0.571717</td>\n",
       "      <td>0.690505</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.698982</td>\n",
       "      <td>0.071294</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.970049</td>\n",
       "      <td>1.898774</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.707711</td>\n",
       "      <td>0.683764</td>\n",
       "      <td>0.758384</td>\n",
       "      <td>0.583838</td>\n",
       "      <td>0.753131</td>\n",
       "      <td>0.697366</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>8.413029</td>\n",
       "      <td>0.984643</td>\n",
       "      <td>0.016784</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.662495</td>\n",
       "      <td>0.551696</td>\n",
       "      <td>0.749495</td>\n",
       "      <td>0.768485</td>\n",
       "      <td>0.747071</td>\n",
       "      <td>0.695831</td>\n",
       "      <td>0.080835</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.558672</td>\n",
       "      <td>0.900883</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.704078</td>\n",
       "      <td>0.723748</td>\n",
       "      <td>0.620606</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.674343</td>\n",
       "      <td>0.687995</td>\n",
       "      <td>0.037722</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.408656</td>\n",
       "      <td>5.986479</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.570505</td>\n",
       "      <td>0.778182</td>\n",
       "      <td>0.610115</td>\n",
       "      <td>0.084030</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.749013</td>\n",
       "      <td>2.109914</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>0.608337</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.457776</td>\n",
       "      <td>0.415416</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.617124</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.608081</td>\n",
       "      <td>0.585393</td>\n",
       "      <td>0.022399</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.763739</td>\n",
       "      <td>0.954291</td>\n",
       "      <td>0.014362</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.599596</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.573760</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.017235</td>\n",
       "      <td>0.486691</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.570044</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.568485</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.731167</td>\n",
       "      <td>1.624097</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.195055</td>\n",
       "      <td>2.215836</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.436230</td>\n",
       "      <td>1.343536</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.199000</td>\n",
       "      <td>1.925228</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.485845</td>\n",
       "      <td>0.261638</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.217780</td>\n",
       "      <td>0.874409</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.316649</td>\n",
       "      <td>0.493024</td>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.566007</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567054</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.129839</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.565253</td>\n",
       "      <td>0.566893</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.737171</td>\n",
       "      <td>1.441851</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.562778</td>\n",
       "      <td>0.567851</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.566061</td>\n",
       "      <td>0.564848</td>\n",
       "      <td>0.565762</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.882086</td>\n",
       "      <td>0.014160</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.554343</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.564712</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.171541</td>\n",
       "      <td>0.114868</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.548648</td>\n",
       "      <td>0.562601</td>\n",
       "      <td>0.572929</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.539394</td>\n",
       "      <td>0.558168</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.361633</td>\n",
       "      <td>0.091521</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.492128</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.571717</td>\n",
       "      <td>0.553159</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.969273</td>\n",
       "      <td>0.534459</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.500606</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.548283</td>\n",
       "      <td>0.550170</td>\n",
       "      <td>0.025865</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.227036</td>\n",
       "      <td>1.006160</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.541195</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.538182</td>\n",
       "      <td>0.517980</td>\n",
       "      <td>0.546373</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.396893</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.580541</td>\n",
       "      <td>0.530291</td>\n",
       "      <td>0.566869</td>\n",
       "      <td>0.520404</td>\n",
       "      <td>0.533737</td>\n",
       "      <td>0.546373</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.702365</td>\n",
       "      <td>0.553420</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.544272</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.122207</td>\n",
       "      <td>0.653832</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.509487</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.534141</td>\n",
       "      <td>0.532929</td>\n",
       "      <td>0.542252</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.279504</td>\n",
       "      <td>0.812623</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.568026</td>\n",
       "      <td>0.567044</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.433131</td>\n",
       "      <td>0.540556</td>\n",
       "      <td>0.053705</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.419162</td>\n",
       "      <td>0.233678</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.540394</td>\n",
       "      <td>0.053825</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.112695</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.540394</td>\n",
       "      <td>0.053825</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.184682</td>\n",
       "      <td>0.009574</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.432553</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.540313</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.334714</td>\n",
       "      <td>0.266565</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.564392</td>\n",
       "      <td>0.566640</td>\n",
       "      <td>0.465859</td>\n",
       "      <td>0.519192</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.536678</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.138262</td>\n",
       "      <td>0.935874</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.515751</td>\n",
       "      <td>0.482020</td>\n",
       "      <td>0.544646</td>\n",
       "      <td>0.572525</td>\n",
       "      <td>0.536436</td>\n",
       "      <td>0.033781</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.103918</td>\n",
       "      <td>0.033503</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.530480</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.533042</td>\n",
       "      <td>0.052142</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.450079</td>\n",
       "      <td>0.279044</td>\n",
       "      <td>0.006583</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.491922</td>\n",
       "      <td>0.525657</td>\n",
       "      <td>0.508283</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.532073</td>\n",
       "      <td>0.030640</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.267883</td>\n",
       "      <td>0.214943</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.475172</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.482020</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.531831</td>\n",
       "      <td>0.043527</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.521421</td>\n",
       "      <td>0.155729</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.497780</td>\n",
       "      <td>0.557351</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.452929</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.528518</td>\n",
       "      <td>0.045809</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.181530</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.432782</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092550</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.432782</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.130053</td>\n",
       "      <td>0.055953</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.432553</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.513411</td>\n",
       "      <td>0.065947</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.273413</td>\n",
       "      <td>0.175963</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.448123</td>\n",
       "      <td>0.439822</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.491032</td>\n",
       "      <td>0.062428</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "38       7.990436      1.569940         0.015160        0.010392   \n",
       "46       7.058115      2.703894         0.005784        0.000399   \n",
       "4        4.674869      1.525631         0.008380        0.003867   \n",
       "40       7.320126      1.681171         0.007979        0.001410   \n",
       "50       8.913055      2.750792         0.013364        0.001492   \n",
       "24       6.506609      1.948132         0.011171        0.003051   \n",
       "8        9.704839      2.271285         0.016954        0.003154   \n",
       "18       6.900554      3.510965         0.006383        0.001018   \n",
       "2        4.403415      1.289711         0.005984        0.000630   \n",
       "30       5.644494      2.149511         0.006384        0.001492   \n",
       "16       4.418736      1.055409         0.007181        0.000751   \n",
       "54       4.339085      0.795141         0.004946        0.001681   \n",
       "10       4.898115      1.589946         0.007982        0.000636   \n",
       "52       4.873254      0.801719         0.009573        0.001016   \n",
       "36       9.038930      3.347687         0.014960        0.002602   \n",
       "44       5.658972      1.124508         0.006189        0.000744   \n",
       "12       7.210390      2.714899         0.011569        0.007792   \n",
       "32       3.970049      1.898774         0.007981        0.005990   \n",
       "22       8.413029      0.984643         0.016784        0.003225   \n",
       "26       6.558672      0.900883         0.008378        0.001492   \n",
       "14       5.408656      5.986479         0.004788        0.000399   \n",
       "28       4.749013      2.109914         0.005386        0.001353   \n",
       "45       0.457776      0.415416         0.008379        0.006296   \n",
       "9        1.763739      0.954291         0.014362        0.001353   \n",
       "51       1.017235      0.486691         0.015758        0.003645   \n",
       "42       2.731167      1.624097         0.005385        0.000491   \n",
       "20       5.195055      2.215836         0.009970        0.002094   \n",
       "48       4.436230      1.343536         0.010173        0.003420   \n",
       "6        5.199000      1.925228         0.009375        0.001353   \n",
       "49       0.485845      0.261638         0.010171        0.001935   \n",
       "34       3.217780      0.874409         0.008577        0.000489   \n",
       "0        2.316649      0.493024         0.004987        0.001093   \n",
       "33       0.207445      0.129839         0.008377        0.005802   \n",
       "23       1.737171      1.441851         0.022140        0.007553   \n",
       "39       0.885720      0.882086         0.014160        0.007608   \n",
       "5        0.171541      0.114868         0.005386        0.000489   \n",
       "11       0.361633      0.091521         0.008380        0.001020   \n",
       "25       0.969273      0.534459         0.012269        0.003648   \n",
       "27       1.227036      1.006160         0.011368        0.004661   \n",
       "17       0.396893      0.252252         0.006982        0.002524   \n",
       "13       0.702365      0.553420         0.007779        0.001715   \n",
       "53       1.122207      0.653832         0.009576        0.002648   \n",
       "37       1.279504      0.812623         0.017951        0.006370   \n",
       "35       0.419162      0.233678         0.008378        0.000489   \n",
       "15       0.112695      0.023263         0.006184        0.001164   \n",
       "7        0.282843      0.184682         0.009574        0.002054   \n",
       "31       0.334714      0.266565         0.007381        0.004306   \n",
       "55       1.138262      0.935874         0.007181        0.000977   \n",
       "29       0.103918      0.033503         0.005386        0.001850   \n",
       "3        0.450079      0.279044         0.006583        0.001850   \n",
       "19       0.267883      0.214943         0.007979        0.004040   \n",
       "41       0.521421      0.155729         0.007980        0.001093   \n",
       "21       0.181530      0.037242         0.010172        0.000747   \n",
       "1        0.092550      0.031893         0.005985        0.001093   \n",
       "43       0.130053      0.055953         0.007580        0.002411   \n",
       "47       0.273413      0.175963         0.008374        0.005339   \n",
       "\n",
       "   param_activation param_alpha param_hidden_layer_sizes param_max_iter  \\\n",
       "38             relu       0.001                 (32, 32)          10000   \n",
       "46             relu      0.0001                 (10, 10)          10000   \n",
       "4              relu         0.1                 (10, 10)          10000   \n",
       "40             relu       0.001                 (32, 16)          10000   \n",
       "50             relu      0.0001                 (64, 64)          10000   \n",
       "24             relu        0.01                 (32, 32)          10000   \n",
       "8              relu         0.1                 (64, 64)          10000   \n",
       "18             relu        0.01                 (10, 10)          10000   \n",
       "2              relu         0.1                 (16, 16)          10000   \n",
       "30             relu       0.001                 (16, 16)          10000   \n",
       "16             relu        0.01                 (16, 16)          10000   \n",
       "54             relu      0.0001                 (32, 16)          10000   \n",
       "10             relu         0.1                 (32, 32)          10000   \n",
       "52             relu      0.0001                 (32, 32)          10000   \n",
       "36             relu       0.001                 (64, 64)          10000   \n",
       "44             relu      0.0001                 (16, 16)          10000   \n",
       "12             relu         0.1                 (32, 16)          10000   \n",
       "32             relu       0.001                 (10, 10)          10000   \n",
       "22             relu        0.01                 (64, 64)          10000   \n",
       "26             relu        0.01                 (32, 16)          10000   \n",
       "14             relu        0.01                  (10, 1)          10000   \n",
       "28             relu       0.001                  (10, 1)          10000   \n",
       "45             relu      0.0001                 (16, 16)          10000   \n",
       "9              relu         0.1                 (64, 64)          10000   \n",
       "51             relu      0.0001                 (64, 64)          10000   \n",
       "42             relu      0.0001                  (10, 1)          10000   \n",
       "20             relu        0.01                  (64, 1)          10000   \n",
       "48             relu      0.0001                  (64, 1)          10000   \n",
       "6              relu         0.1                  (64, 1)          10000   \n",
       "49             relu      0.0001                  (64, 1)          10000   \n",
       "34             relu       0.001                  (64, 1)          10000   \n",
       "0              relu         0.1                  (10, 1)          10000   \n",
       "33             relu       0.001                 (10, 10)          10000   \n",
       "23             relu        0.01                 (64, 64)          10000   \n",
       "39             relu       0.001                 (32, 32)          10000   \n",
       "5              relu         0.1                 (10, 10)          10000   \n",
       "11             relu         0.1                 (32, 32)          10000   \n",
       "25             relu        0.01                 (32, 32)          10000   \n",
       "27             relu        0.01                 (32, 16)          10000   \n",
       "17             relu        0.01                 (16, 16)          10000   \n",
       "13             relu         0.1                 (32, 16)          10000   \n",
       "53             relu      0.0001                 (32, 32)          10000   \n",
       "37             relu       0.001                 (64, 64)          10000   \n",
       "35             relu       0.001                  (64, 1)          10000   \n",
       "15             relu        0.01                  (10, 1)          10000   \n",
       "7              relu         0.1                  (64, 1)          10000   \n",
       "31             relu       0.001                 (16, 16)          10000   \n",
       "55             relu      0.0001                 (32, 16)          10000   \n",
       "29             relu       0.001                  (10, 1)          10000   \n",
       "3              relu         0.1                 (16, 16)          10000   \n",
       "19             relu        0.01                 (10, 10)          10000   \n",
       "41             relu       0.001                 (32, 16)          10000   \n",
       "21             relu        0.01                  (64, 1)          10000   \n",
       "1              relu         0.1                  (10, 1)          10000   \n",
       "43             relu      0.0001                  (10, 1)          10000   \n",
       "47             relu      0.0001                 (10, 10)          10000   \n",
       "\n",
       "   param_solver                                             params  \\\n",
       "38         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "46         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "4          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "40         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "50         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "24         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "8          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "18         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "2          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "30         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "16         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "54         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "10         adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "52         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "36         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "44         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "12         adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "32         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "22         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "26         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "14         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "28         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "45        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "9         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "51        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "42         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "20         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "48         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "6          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "49        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "34         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "0          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "33        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "23        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "39        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "5         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "11        lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "25        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "27        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "17        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "13        lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "53        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "37        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "35        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "15        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "7         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "31        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "55        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "29        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "3         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "19        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "41        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "21        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "1         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "43        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "47        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "38           0.769479           0.772617           0.752323   \n",
       "46           0.751716           0.760097           0.754747   \n",
       "4            0.714574           0.758481           0.753939   \n",
       "40           0.731530           0.750000           0.744242   \n",
       "50           0.735567           0.765751           0.705455   \n",
       "24           0.713363           0.760501           0.709899   \n",
       "8            0.746871           0.682553           0.749899   \n",
       "18           0.766653           0.598950           0.749899   \n",
       "2            0.768268           0.723748           0.616162   \n",
       "30           0.716996           0.721325           0.715960   \n",
       "16           0.747679           0.711632           0.671515   \n",
       "54           0.752523           0.766155           0.754747   \n",
       "10           0.714978           0.711228           0.696970   \n",
       "52           0.754542           0.752423           0.725253   \n",
       "36           0.765442           0.651050           0.703434   \n",
       "44           0.664110           0.657512           0.769293   \n",
       "12           0.763827           0.699515           0.571717   \n",
       "32           0.707711           0.683764           0.758384   \n",
       "22           0.662495           0.551696           0.749495   \n",
       "26           0.704078           0.723748           0.620606   \n",
       "14           0.567218           0.567447           0.567273   \n",
       "28           0.567218           0.567447           0.567273   \n",
       "45           0.567218           0.617124           0.567273   \n",
       "9            0.567218           0.567447           0.599596   \n",
       "51           0.570044           0.567447           0.567273   \n",
       "42           0.567218           0.567447           0.567273   \n",
       "20           0.567218           0.567447           0.567273   \n",
       "48           0.567218           0.567447           0.567273   \n",
       "6            0.567218           0.567447           0.567273   \n",
       "49           0.567218           0.567447           0.567273   \n",
       "34           0.567218           0.567447           0.567273   \n",
       "0            0.566007           0.567447           0.567273   \n",
       "33           0.567218           0.567447           0.567273   \n",
       "23           0.562778           0.567851           0.567273   \n",
       "39           0.567218           0.567447           0.567273   \n",
       "5            0.548648           0.562601           0.572929   \n",
       "11           0.492128           0.567447           0.567273   \n",
       "25           0.500606           0.567447           0.567273   \n",
       "27           0.567218           0.541195           0.567273   \n",
       "17           0.580541           0.530291           0.566869   \n",
       "13           0.553492           0.544426           0.567273   \n",
       "53           0.509487           0.567447           0.567273   \n",
       "37           0.568026           0.567044           0.567273   \n",
       "35           0.567218           0.567447           0.432727   \n",
       "15           0.567218           0.567447           0.567273   \n",
       "7            0.567218           0.432553           0.567273   \n",
       "31           0.564392           0.566640           0.465859   \n",
       "55           0.567218           0.515751           0.482020   \n",
       "29           0.530480           0.567447           0.432727   \n",
       "3            0.567218           0.491922           0.525657   \n",
       "19           0.475172           0.567447           0.482020   \n",
       "41           0.497780           0.557351           0.567273   \n",
       "21           0.432782           0.567447           0.567273   \n",
       "1            0.432782           0.567447           0.567273   \n",
       "43           0.567218           0.432553           0.432727   \n",
       "47           0.448123           0.439822           0.432727   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "38           0.750707           0.763232         0.761674        0.008841   \n",
       "46           0.747879           0.785051         0.759897        0.013194   \n",
       "4            0.769293           0.763232         0.751899        0.019351   \n",
       "40           0.728889           0.785859         0.748101        0.020433   \n",
       "50           0.754747           0.766061         0.745516        0.022892   \n",
       "24           0.766061           0.757576         0.741477        0.024548   \n",
       "8            0.771313           0.720404         0.734206        0.030471   \n",
       "18           0.763636           0.764444         0.728712        0.065155   \n",
       "2            0.771717           0.757576         0.727500        0.058188   \n",
       "30           0.756768           0.701010         0.722411        0.018494   \n",
       "16           0.730101           0.750303         0.722249        0.028919   \n",
       "54           0.567273           0.756768         0.719502        0.076245   \n",
       "10           0.723636           0.748687         0.719098        0.017114   \n",
       "52           0.736566           0.612525         0.716271        0.052964   \n",
       "36           0.753131           0.697374         0.714090        0.041304   \n",
       "44           0.707071           0.763232         0.712231        0.047316   \n",
       "12           0.690505           0.769293         0.698982        0.071294   \n",
       "32           0.583838           0.753131         0.697366        0.063259   \n",
       "22           0.768485           0.747071         0.695831        0.080835   \n",
       "26           0.717172           0.674343         0.687995        0.037722   \n",
       "14           0.570505           0.778182         0.610115        0.084030   \n",
       "28           0.567273           0.772525         0.608337        0.082082   \n",
       "45           0.567273           0.608081         0.585393        0.022399   \n",
       "9            0.567273           0.567273         0.573760        0.012916   \n",
       "51           0.568485           0.567273         0.568105        0.001070   \n",
       "42           0.567273           0.567273         0.567297        0.000078   \n",
       "20           0.567273           0.567273         0.567297        0.000078   \n",
       "48           0.567273           0.567273         0.567297        0.000078   \n",
       "6            0.567273           0.567273         0.567297        0.000078   \n",
       "49           0.567273           0.567273         0.567297        0.000078   \n",
       "34           0.567273           0.567273         0.567297        0.000078   \n",
       "0            0.567273           0.567273         0.567054        0.000528   \n",
       "33           0.567273           0.565253         0.566893        0.000824   \n",
       "23           0.566061           0.564848         0.565762        0.001816   \n",
       "39           0.554343           0.567273         0.564712        0.005184   \n",
       "5            0.567273           0.539394         0.558168        0.012357   \n",
       "11           0.567273           0.571717         0.553159        0.030574   \n",
       "25           0.567273           0.548283         0.550170        0.025865   \n",
       "27           0.538182           0.517980         0.546373        0.018822   \n",
       "17           0.520404           0.533737         0.546373        0.023154   \n",
       "13           0.488889           0.567273         0.544272        0.029015   \n",
       "53           0.534141           0.532929         0.542252        0.022305   \n",
       "37           0.567273           0.433131         0.540556        0.053705   \n",
       "35           0.567273           0.567273         0.540394        0.053825   \n",
       "15           0.567273           0.432727         0.540394        0.053825   \n",
       "7            0.567273           0.567273         0.540313        0.053886   \n",
       "31           0.519192           0.567273         0.536678        0.039805   \n",
       "55           0.544646           0.572525         0.536436        0.033781   \n",
       "29           0.567273           0.567273         0.533042        0.052142   \n",
       "3            0.508283           0.567273         0.532073        0.030640   \n",
       "19           0.567273           0.567273         0.531831        0.043527   \n",
       "41           0.452929           0.567273         0.528518        0.045809   \n",
       "21           0.432727           0.567273         0.513492        0.065931   \n",
       "1            0.432727           0.567273         0.513492        0.065931   \n",
       "43           0.567273           0.567273         0.513411        0.065947   \n",
       "47           0.567273           0.567273         0.491032        0.062428   \n",
       "\n",
       "    rank_test_score  \n",
       "38                1  \n",
       "46                2  \n",
       "4                 3  \n",
       "40                4  \n",
       "50                5  \n",
       "24                6  \n",
       "8                 7  \n",
       "18                8  \n",
       "2                 9  \n",
       "30               10  \n",
       "16               11  \n",
       "54               12  \n",
       "10               13  \n",
       "52               14  \n",
       "36               15  \n",
       "44               16  \n",
       "12               17  \n",
       "32               18  \n",
       "22               19  \n",
       "26               20  \n",
       "14               21  \n",
       "28               22  \n",
       "45               23  \n",
       "9                24  \n",
       "51               25  \n",
       "42               26  \n",
       "20               26  \n",
       "48               26  \n",
       "6                26  \n",
       "49               26  \n",
       "34               26  \n",
       "0                32  \n",
       "33               33  \n",
       "23               34  \n",
       "39               35  \n",
       "5                36  \n",
       "11               37  \n",
       "25               38  \n",
       "27               39  \n",
       "17               39  \n",
       "13               41  \n",
       "53               42  \n",
       "37               43  \n",
       "35               44  \n",
       "15               44  \n",
       "7                46  \n",
       "31               47  \n",
       "55               48  \n",
       "29               49  \n",
       "3                50  \n",
       "19               51  \n",
       "41               52  \n",
       "21               53  \n",
       "1                53  \n",
       "43               55  \n",
       "47               56  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  50 | elapsed:  1.4min remaining:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:  1.4min remaining:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  50 | elapsed:  1.6min remaining:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed:  1.8min remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=-1,\n",
       "                                              oob_score=False, random_state=42,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [None, 1, 10, 50, 100],\n",
       "                         'max_features': ['auto'], 'n_estimators': [100, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "             return_train_score=False, scoring='accuracy', verbose=20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_features': ['auto'],\n",
    "    'max_depth' : [None,1,10,50,100]\n",
    "}\n",
    "\n",
    "clf = ensemble.RandomForestClassifier(n_jobs = -1, random_state=42)\n",
    "cv = GridSearchCV(clf, param_grid=params, scoring='accuracy', n_jobs=-1, verbose=20, refit='accuracy', cv=5)\n",
    "cv.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>26.580557</td>\n",
       "      <td>2.034733</td>\n",
       "      <td>3.465882</td>\n",
       "      <td>1.523289</td>\n",
       "      <td>50</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'auto', 'n_e...</td>\n",
       "      <td>0.8986</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.9025</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>0.9067</td>\n",
       "      <td>0.90368</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.713555</td>\n",
       "      <td>2.050797</td>\n",
       "      <td>1.846000</td>\n",
       "      <td>0.351028</td>\n",
       "      <td>None</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'max_features': 'auto', 'n...</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.9025</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.90366</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>23.108911</td>\n",
       "      <td>0.530579</td>\n",
       "      <td>1.185286</td>\n",
       "      <td>1.016089</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 100, 'max_features': 'auto', 'n_...</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.9025</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.90366</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.672633</td>\n",
       "      <td>1.363912</td>\n",
       "      <td>3.476981</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>None</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': None, 'max_features': 'auto', 'n...</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>0.9060</td>\n",
       "      <td>0.90274</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14.453780</td>\n",
       "      <td>2.516284</td>\n",
       "      <td>3.793836</td>\n",
       "      <td>1.623405</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 100, 'max_features': 'auto', 'n_...</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>0.9060</td>\n",
       "      <td>0.90274</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>13.115615</td>\n",
       "      <td>1.537157</td>\n",
       "      <td>3.510704</td>\n",
       "      <td>1.871417</td>\n",
       "      <td>50</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'auto', 'n_e...</td>\n",
       "      <td>0.8985</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>0.90236</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.241929</td>\n",
       "      <td>1.199059</td>\n",
       "      <td>3.823229</td>\n",
       "      <td>1.331812</td>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'auto', 'n_e...</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>0.7987</td>\n",
       "      <td>0.7949</td>\n",
       "      <td>0.8042</td>\n",
       "      <td>0.80108</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.722892</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>1.957967</td>\n",
       "      <td>0.442841</td>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'max_features': 'auto', 'n_e...</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>0.80094</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.532364</td>\n",
       "      <td>1.301699</td>\n",
       "      <td>2.559750</td>\n",
       "      <td>0.752490</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 'auto', 'n_es...</td>\n",
       "      <td>0.6168</td>\n",
       "      <td>0.6034</td>\n",
       "      <td>0.5991</td>\n",
       "      <td>0.6061</td>\n",
       "      <td>0.6062</td>\n",
       "      <td>0.60632</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.565582</td>\n",
       "      <td>0.361229</td>\n",
       "      <td>1.975552</td>\n",
       "      <td>0.497620</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 'auto', 'n_es...</td>\n",
       "      <td>0.5893</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.5876</td>\n",
       "      <td>0.5916</td>\n",
       "      <td>0.5911</td>\n",
       "      <td>0.58988</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7      26.580557      2.034733         3.465882        1.523289   \n",
       "1      20.713555      2.050797         1.846000        0.351028   \n",
       "9      23.108911      0.530579         1.185286        1.016089   \n",
       "0      10.672633      1.363912         3.476981        0.364100   \n",
       "8      14.453780      2.516284         3.793836        1.623405   \n",
       "6      13.115615      1.537157         3.510704        1.871417   \n",
       "5      13.241929      1.199059         3.823229        1.331812   \n",
       "4       5.722892      0.164345         1.957967        0.442841   \n",
       "2       4.532364      1.301699         2.559750        0.752490   \n",
       "3       4.565582      0.361229         1.975552        0.497620   \n",
       "\n",
       "  param_max_depth param_max_features param_n_estimators  \\\n",
       "7              50               auto                200   \n",
       "1            None               auto                200   \n",
       "9             100               auto                200   \n",
       "0            None               auto                100   \n",
       "8             100               auto                100   \n",
       "6              50               auto                100   \n",
       "5              10               auto                200   \n",
       "4              10               auto                100   \n",
       "2               1               auto                100   \n",
       "3               1               auto                200   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "7  {'max_depth': 50, 'max_features': 'auto', 'n_e...             0.8986   \n",
       "1  {'max_depth': None, 'max_features': 'auto', 'n...             0.8991   \n",
       "9  {'max_depth': 100, 'max_features': 'auto', 'n_...             0.8991   \n",
       "0  {'max_depth': None, 'max_features': 'auto', 'n...             0.8994   \n",
       "8  {'max_depth': 100, 'max_features': 'auto', 'n_...             0.8994   \n",
       "6  {'max_depth': 50, 'max_features': 'auto', 'n_e...             0.8985   \n",
       "5  {'max_depth': 10, 'max_features': 'auto', 'n_e...             0.8003   \n",
       "4  {'max_depth': 10, 'max_features': 'auto', 'n_e...             0.8016   \n",
       "2  {'max_depth': 1, 'max_features': 'auto', 'n_es...             0.6168   \n",
       "3  {'max_depth': 1, 'max_features': 'auto', 'n_es...             0.5893   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "7             0.9059             0.9025             0.9047             0.9067   \n",
       "1             0.9057             0.9025             0.9045             0.9065   \n",
       "9             0.9057             0.9025             0.9045             0.9065   \n",
       "0             0.9052             0.9014             0.9017             0.9060   \n",
       "8             0.9052             0.9014             0.9017             0.9060   \n",
       "6             0.9050             0.9014             0.9017             0.9052   \n",
       "5             0.8073             0.7987             0.7949             0.8042   \n",
       "4             0.8087             0.7981             0.7945             0.8018   \n",
       "2             0.6034             0.5991             0.6061             0.6062   \n",
       "3             0.5898             0.5876             0.5916             0.5911   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "7          0.90368        0.002908                1  \n",
       "1          0.90366        0.002648                2  \n",
       "9          0.90366        0.002648                2  \n",
       "0          0.90274        0.002478                4  \n",
       "8          0.90274        0.002478                4  \n",
       "6          0.90236        0.002502                6  \n",
       "5          0.80108        0.004310                7  \n",
       "4          0.80094        0.004710                8  \n",
       "2          0.60632        0.005840                9  \n",
       "3          0.58988        0.001413               10  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_rf.sort_values('rank_test_score')['params'][0])\n",
    "cv_rf.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  40 | elapsed:   11.8s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  40 | elapsed:   17.5s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  40 | elapsed:   19.9s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  40 | elapsed:   21.9s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.1s finished\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=10000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.0001, verbose=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [1, 3, 5, 10], 'loss': ['hinge'],\n",
       "                         'max_iter': [10000, 20000]},\n",
       "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "             return_train_score=False, scoring='accuracy', verbose=20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [1,3,5,10],\n",
    "    'loss':['hinge'],\n",
    "    'max_iter': [10000,20000]\n",
    "}\n",
    "\n",
    "svc = LSVC(max_iter=10000)\n",
    "cv = GridSearchCV(svc, param_grid=params, scoring='accuracy', n_jobs=-1, verbose=20, refit='accuracy', cv=5)\n",
    "cv.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 3, 'loss': 'hinge', 'max_iter': 10000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.475669</td>\n",
       "      <td>0.261214</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>6.299989e-04</td>\n",
       "      <td>3</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 3, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.969010</td>\n",
       "      <td>0.078606</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>3.234067e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 1, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.788789</td>\n",
       "      <td>0.7816</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.875406</td>\n",
       "      <td>0.591390</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>1.340243e-06</td>\n",
       "      <td>3</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 3, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.787788</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.007708</td>\n",
       "      <td>0.088960</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>3.994022e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 1, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.028292</td>\n",
       "      <td>0.420864</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>1.163857e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 5, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.504991</td>\n",
       "      <td>1.917206</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.265192e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 10, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.332012</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>3.991615e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 5, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.787788</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>9.771397</td>\n",
       "      <td>0.950261</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.997331e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 10, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.792208</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.787788</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "2       2.475669      0.261214         0.000997    6.299989e-04       3   \n",
       "0       0.969010      0.078606         0.000997    3.234067e-07       1   \n",
       "3       2.875406      0.591390         0.000998    1.340243e-06       3   \n",
       "1       1.007708      0.088960         0.000799    3.994022e-04       1   \n",
       "5       5.028292      0.420864         0.001795    1.163857e-03       5   \n",
       "7       7.504991      1.917206         0.001000    1.265192e-06      10   \n",
       "4       5.332012      0.568569         0.001196    3.991615e-04       5   \n",
       "6       9.771397      0.950261         0.001796    3.997331e-04      10   \n",
       "\n",
       "  param_loss param_max_iter                                         params  \\\n",
       "2      hinge          10000   {'C': 3, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "0      hinge          10000   {'C': 1, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "3      hinge          20000   {'C': 3, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "1      hinge          20000   {'C': 1, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "5      hinge          20000   {'C': 5, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "7      hinge          20000  {'C': 10, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "4      hinge          10000   {'C': 5, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "6      hinge          10000  {'C': 10, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "2           0.793207              0.783              0.773              0.770   \n",
       "0           0.793207              0.783              0.773              0.770   \n",
       "3           0.793207              0.783              0.773              0.770   \n",
       "1           0.793207              0.783              0.773              0.770   \n",
       "5           0.793207              0.783              0.773              0.770   \n",
       "7           0.793207              0.785              0.772              0.769   \n",
       "4           0.791209              0.783              0.773              0.769   \n",
       "6           0.792208              0.780              0.773              0.771   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "2           0.789790           0.7818        0.009079                1  \n",
       "0           0.788789           0.7816        0.008910                2  \n",
       "3           0.787788           0.7814        0.008757                3  \n",
       "1           0.786787           0.7812        0.008619                4  \n",
       "5           0.786787           0.7812        0.008619                4  \n",
       "7           0.786787           0.7812        0.009202                4  \n",
       "4           0.787788           0.7808        0.008511                7  \n",
       "6           0.787788           0.7808        0.008204                7  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   55.1s\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 267 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 270 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 271 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 273 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 275 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 279 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 282 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 283 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [1, 5], 'degree': [3, 7, 10],\n",
       "                         'gamma': array([4.00000000e-02, 6.68740305e-01, 1.11803399e+01, 1.86918598e+02,\n",
       "       3.12500000e+03]),\n",
       "                         'kernel': ['linear', 'rbf'], 'max_iter': [100000]},\n",
       "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "             return_train_score=False, scoring='accuracy', verbose=20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [1, 5], \n",
    "    'kernel': ['linear', 'rbf'], \n",
    "    'degree': [3, 7, 10],\n",
    "    'gamma': np.power(5, np.linspace(-2,5, 5)),\n",
    "    'max_iter': [100000]\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "cv = GridSearchCV(svc, param_grid=params, scoring='accuracy', n_jobs=-1, verbose=20, refit='accuracy', cv=5)\n",
    "cv.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>304001</td>\n",
       "      <td>2867</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>1924</td>\n",
       "      <td>206</td>\n",
       "      <td>212</td>\n",
       "      <td>142</td>\n",
       "      <td>1812</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333606</td>\n",
       "      <td>3097</td>\n",
       "      <td>295</td>\n",
       "      <td>15</td>\n",
       "      <td>175</td>\n",
       "      <td>49</td>\n",
       "      <td>1505</td>\n",
       "      <td>178</td>\n",
       "      <td>235</td>\n",
       "      <td>197</td>\n",
       "      <td>1191</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260019</td>\n",
       "      <td>3039</td>\n",
       "      <td>162</td>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>16</td>\n",
       "      <td>3059</td>\n",
       "      <td>223</td>\n",
       "      <td>240</td>\n",
       "      <td>152</td>\n",
       "      <td>1292</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72966</td>\n",
       "      <td>3041</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>376</td>\n",
       "      <td>66</td>\n",
       "      <td>5389</td>\n",
       "      <td>216</td>\n",
       "      <td>213</td>\n",
       "      <td>131</td>\n",
       "      <td>3750</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351036</td>\n",
       "      <td>3210</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>224</td>\n",
       "      <td>228</td>\n",
       "      <td>139</td>\n",
       "      <td>1812</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238314</td>\n",
       "      <td>3068</td>\n",
       "      <td>208</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>-12</td>\n",
       "      <td>2190</td>\n",
       "      <td>217</td>\n",
       "      <td>244</td>\n",
       "      <td>164</td>\n",
       "      <td>3705</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336087</td>\n",
       "      <td>2932</td>\n",
       "      <td>222</td>\n",
       "      <td>12</td>\n",
       "      <td>663</td>\n",
       "      <td>96</td>\n",
       "      <td>2627</td>\n",
       "      <td>205</td>\n",
       "      <td>251</td>\n",
       "      <td>181</td>\n",
       "      <td>1716</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461295</td>\n",
       "      <td>3250</td>\n",
       "      <td>308</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>11</td>\n",
       "      <td>693</td>\n",
       "      <td>212</td>\n",
       "      <td>237</td>\n",
       "      <td>164</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269104</td>\n",
       "      <td>3175</td>\n",
       "      <td>108</td>\n",
       "      <td>19</td>\n",
       "      <td>277</td>\n",
       "      <td>68</td>\n",
       "      <td>752</td>\n",
       "      <td>249</td>\n",
       "      <td>214</td>\n",
       "      <td>86</td>\n",
       "      <td>666</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283134</td>\n",
       "      <td>3245</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>365</td>\n",
       "      <td>132</td>\n",
       "      <td>1136</td>\n",
       "      <td>238</td>\n",
       "      <td>200</td>\n",
       "      <td>85</td>\n",
       "      <td>2012</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  44  45  46  \\\n",
       "304001  2867   12  13  212    2  1924  206  212  142  1812  ...   1   0   0   \n",
       "333606  3097  295  15  175   49  1505  178  235  197  1191  ...   0   0   0   \n",
       "260019  3039  162   4  134   16  3059  223  240  152  1292  ...   0   1   0   \n",
       "72966   3041   29  12  376   66  5389  216  213  131  3750  ...   0   0   0   \n",
       "351036  3210   59   6    0    0  1902  224  228  139  1812  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "238314  3068  208   5   90  -12  2190  217  244  164  3705  ...   0   0   0   \n",
       "336087  2932  222  12  663   96  2627  205  251  181  1716  ...   0   0   0   \n",
       "461295  3250  308   3   67   11   693  212  237  164  1262  ...   0   0   0   \n",
       "269104  3175  108  19  277   68   752  249  214   86   666  ...   0   0   1   \n",
       "283134  3245   72  19  365  132  1136  238  200   85  2012  ...   0   0   0   \n",
       "\n",
       "        47  48  49  50  51  52  53  \n",
       "304001   0   0   0   0   0   0   0  \n",
       "333606   0   0   0   0   0   0   0  \n",
       "260019   0   0   0   0   0   0   0  \n",
       "72966    0   0   0   0   0   0   0  \n",
       "351036   0   0   0   0   0   0   0  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "238314   0   0   0   0   0   0   0  \n",
       "336087   0   0   0   0   0   0   0  \n",
       "461295   0   0   0   0   0   1   0  \n",
       "269104   0   0   0   0   0   0   0  \n",
       "283134   0   0   0   0   0   0   0  \n",
       "\n",
       "[5000 rows x 54 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel': 'rbf', 'max_iter': 100000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.353872</td>\n",
       "      <td>0.139754</td>\n",
       "      <td>0.384772</td>\n",
       "      <td>0.029790</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.568265</td>\n",
       "      <td>0.166693</td>\n",
       "      <td>0.366639</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.853188</td>\n",
       "      <td>0.191577</td>\n",
       "      <td>0.471340</td>\n",
       "      <td>0.056109</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.380591</td>\n",
       "      <td>0.248239</td>\n",
       "      <td>0.649270</td>\n",
       "      <td>0.065163</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.784785</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.110350</td>\n",
       "      <td>0.194912</td>\n",
       "      <td>0.670208</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.784785</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.134265</td>\n",
       "      <td>0.188002</td>\n",
       "      <td>0.668224</td>\n",
       "      <td>0.025363</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.784785</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.285059</td>\n",
       "      <td>0.356587</td>\n",
       "      <td>0.320534</td>\n",
       "      <td>0.085731</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.797798</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.551552</td>\n",
       "      <td>0.226214</td>\n",
       "      <td>0.402159</td>\n",
       "      <td>0.017890</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.797798</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.557165</td>\n",
       "      <td>0.271641</td>\n",
       "      <td>0.416488</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.797798</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.357085</td>\n",
       "      <td>0.254296</td>\n",
       "      <td>0.558053</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.800801</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.142968</td>\n",
       "      <td>0.370902</td>\n",
       "      <td>0.606597</td>\n",
       "      <td>0.033079</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.800801</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.855678</td>\n",
       "      <td>0.554625</td>\n",
       "      <td>0.720833</td>\n",
       "      <td>0.156519</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.800801</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.743483</td>\n",
       "      <td>0.223548</td>\n",
       "      <td>0.297605</td>\n",
       "      <td>0.028529</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.797830</td>\n",
       "      <td>0.065108</td>\n",
       "      <td>0.275204</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.147974</td>\n",
       "      <td>0.247351</td>\n",
       "      <td>0.305790</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.929534</td>\n",
       "      <td>0.182026</td>\n",
       "      <td>0.276261</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.940992</td>\n",
       "      <td>0.166402</td>\n",
       "      <td>0.291833</td>\n",
       "      <td>0.023074</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.194976</td>\n",
       "      <td>0.246384</td>\n",
       "      <td>0.283885</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.111710</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>0.287033</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.258697</td>\n",
       "      <td>0.152972</td>\n",
       "      <td>0.309771</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.168159</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>0.291235</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.211166</td>\n",
       "      <td>0.290105</td>\n",
       "      <td>0.312879</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.735517</td>\n",
       "      <td>0.331585</td>\n",
       "      <td>0.349074</td>\n",
       "      <td>0.055763</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.170094</td>\n",
       "      <td>0.243286</td>\n",
       "      <td>0.313264</td>\n",
       "      <td>0.040874</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.294776</td>\n",
       "      <td>0.133602</td>\n",
       "      <td>0.322744</td>\n",
       "      <td>0.034703</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.939967</td>\n",
       "      <td>0.173573</td>\n",
       "      <td>0.285636</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.838714</td>\n",
       "      <td>0.082162</td>\n",
       "      <td>0.273868</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>6.531932</td>\n",
       "      <td>0.432230</td>\n",
       "      <td>0.430771</td>\n",
       "      <td>0.097775</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>5.651883</td>\n",
       "      <td>0.302745</td>\n",
       "      <td>0.298237</td>\n",
       "      <td>0.038839</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>6.140741</td>\n",
       "      <td>0.414935</td>\n",
       "      <td>0.285878</td>\n",
       "      <td>0.022203</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>5.780541</td>\n",
       "      <td>0.242922</td>\n",
       "      <td>0.294227</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>5.609655</td>\n",
       "      <td>0.199330</td>\n",
       "      <td>0.303308</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>5.450360</td>\n",
       "      <td>0.369128</td>\n",
       "      <td>0.268483</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.712751</td>\n",
       "      <td>0.314677</td>\n",
       "      <td>0.302343</td>\n",
       "      <td>0.037190</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.637175</td>\n",
       "      <td>0.250045</td>\n",
       "      <td>0.275880</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.519468</td>\n",
       "      <td>0.336417</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.018491</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.096658</td>\n",
       "      <td>0.365167</td>\n",
       "      <td>0.267285</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.562713</td>\n",
       "      <td>0.316115</td>\n",
       "      <td>0.309894</td>\n",
       "      <td>0.025054</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.838950</td>\n",
       "      <td>0.550288</td>\n",
       "      <td>0.360842</td>\n",
       "      <td>0.100340</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>5.279767</td>\n",
       "      <td>0.330975</td>\n",
       "      <td>0.264855</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>6.528256</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>0.267884</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>5.874879</td>\n",
       "      <td>0.128683</td>\n",
       "      <td>0.289227</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>5.821910</td>\n",
       "      <td>0.155554</td>\n",
       "      <td>1.120640</td>\n",
       "      <td>0.061976</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.597598</td>\n",
       "      <td>0.604605</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>5.547228</td>\n",
       "      <td>0.240375</td>\n",
       "      <td>1.069610</td>\n",
       "      <td>0.061141</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.597598</td>\n",
       "      <td>0.604605</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.318644</td>\n",
       "      <td>0.288235</td>\n",
       "      <td>1.029479</td>\n",
       "      <td>0.065260</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.597598</td>\n",
       "      <td>0.604605</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.480756</td>\n",
       "      <td>0.365007</td>\n",
       "      <td>1.027878</td>\n",
       "      <td>0.067168</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.589411</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.591592</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.988523</td>\n",
       "      <td>0.253899</td>\n",
       "      <td>1.262025</td>\n",
       "      <td>0.042727</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.589411</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.591592</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.375298</td>\n",
       "      <td>0.277666</td>\n",
       "      <td>1.055180</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.589411</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.591592</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.611569</td>\n",
       "      <td>0.300030</td>\n",
       "      <td>1.266517</td>\n",
       "      <td>0.073419</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>6.520526</td>\n",
       "      <td>0.344982</td>\n",
       "      <td>1.227319</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>6.632954</td>\n",
       "      <td>0.296923</td>\n",
       "      <td>1.237304</td>\n",
       "      <td>0.072001</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>7.599114</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>1.441760</td>\n",
       "      <td>0.110392</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>6.435442</td>\n",
       "      <td>0.377886</td>\n",
       "      <td>1.265153</td>\n",
       "      <td>0.058149</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6.325690</td>\n",
       "      <td>0.312012</td>\n",
       "      <td>1.311712</td>\n",
       "      <td>0.081270</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.188918</td>\n",
       "      <td>0.347599</td>\n",
       "      <td>1.225441</td>\n",
       "      <td>0.103431</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.348500</td>\n",
       "      <td>0.392467</td>\n",
       "      <td>1.306610</td>\n",
       "      <td>0.076471</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.320431</td>\n",
       "      <td>0.110414</td>\n",
       "      <td>1.268224</td>\n",
       "      <td>0.040854</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.736780</td>\n",
       "      <td>0.178227</td>\n",
       "      <td>1.261628</td>\n",
       "      <td>0.125509</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>6.420277</td>\n",
       "      <td>0.419279</td>\n",
       "      <td>1.221001</td>\n",
       "      <td>0.085236</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>5.439438</td>\n",
       "      <td>0.202960</td>\n",
       "      <td>0.863515</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "51       2.353872      0.139754         0.384772        0.029790       5   \n",
       "31       2.568265      0.166693         0.366639        0.020163       5   \n",
       "41       2.853188      0.191577         0.471340        0.056109       5   \n",
       "3        4.380591      0.248239         0.649270        0.065163       1   \n",
       "23       4.110350      0.194912         0.670208        0.050363       1   \n",
       "13       4.134265      0.188002         0.668224        0.025363       1   \n",
       "1        2.285059      0.356587         0.320534        0.085731       1   \n",
       "21       2.551552      0.226214         0.402159        0.017890       1   \n",
       "11       2.557165      0.271641         0.416488        0.028675       1   \n",
       "33       4.357085      0.254296         0.558053        0.018864       5   \n",
       "43       4.142968      0.370902         0.606597        0.033079       5   \n",
       "53       4.855678      0.554625         0.720833        0.156519       5   \n",
       "0        2.743483      0.223548         0.297605        0.028529       1   \n",
       "18       2.797830      0.065108         0.275204        0.012306       1   \n",
       "24       3.147974      0.247351         0.305790        0.024206       1   \n",
       "26       2.929534      0.182026         0.276261        0.022103       1   \n",
       "20       2.940992      0.166402         0.291833        0.023074       1   \n",
       "22       3.194976      0.246384         0.283885        0.014481       1   \n",
       "14       3.111710      0.208718         0.287033        0.016477       1   \n",
       "12       3.258697      0.152972         0.309771        0.015764       1   \n",
       "10       3.168159      0.232131         0.291235        0.018160       1   \n",
       "8        3.211166      0.290105         0.312879        0.036750       1   \n",
       "6        3.735517      0.331585         0.349074        0.055763       1   \n",
       "4        3.170094      0.243286         0.313264        0.040874       1   \n",
       "2        3.294776      0.133602         0.322744        0.034703       1   \n",
       "16       2.939967      0.173573         0.285636        0.019682       1   \n",
       "28       2.838714      0.082162         0.273868        0.019137       1   \n",
       "52       6.531932      0.432230         0.430771        0.097775       5   \n",
       "38       5.651883      0.302745         0.298237        0.038839       5   \n",
       "42       6.140741      0.414935         0.285878        0.022203       5   \n",
       "44       5.780541      0.242922         0.294227        0.020224       5   \n",
       "46       5.609655      0.199330         0.303308        0.027154       5   \n",
       "48       5.450360      0.369128         0.268483        0.003753       5   \n",
       "40       5.712751      0.314677         0.302343        0.037190       5   \n",
       "36       5.637175      0.250045         0.275880        0.017036       5   \n",
       "50       5.519468      0.336417         0.283442        0.018491       5   \n",
       "34       6.096658      0.365167         0.267285        0.008175       5   \n",
       "32       5.562713      0.316115         0.309894        0.025054       5   \n",
       "30       6.838950      0.550288         0.360842        0.100340       5   \n",
       "58       5.279767      0.330975         0.264855        0.014432       5   \n",
       "54       6.528256      0.208900         0.267884        0.006985       5   \n",
       "56       5.874879      0.128683         0.289227        0.014048       5   \n",
       "55       5.821910      0.155554         1.120640        0.061976       5   \n",
       "45       5.547228      0.240375         1.069610        0.061141       5   \n",
       "35       5.318644      0.288235         1.029479        0.065260       5   \n",
       "15       5.480756      0.365007         1.027878        0.067168       1   \n",
       "5        5.988523      0.253899         1.262025        0.042727       1   \n",
       "25       5.375298      0.277666         1.055180        0.053225       1   \n",
       "37       6.611569      0.300030         1.266517        0.073419       5   \n",
       "57       6.520526      0.344982         1.227319        0.068824       5   \n",
       "47       6.632954      0.296923         1.237304        0.072001       5   \n",
       "29       7.599114      0.407141         1.441760        0.110392       1   \n",
       "39       6.435442      0.377886         1.265153        0.058149       5   \n",
       "27       6.325690      0.312012         1.311712        0.081270       1   \n",
       "19       6.188918      0.347599         1.225441        0.103431       1   \n",
       "17       6.348500      0.392467         1.306610        0.076471       1   \n",
       "9        6.320431      0.110414         1.268224        0.040854       1   \n",
       "7        6.736780      0.178227         1.261628        0.125509       1   \n",
       "49       6.420277      0.419279         1.221001        0.085236       5   \n",
       "59       5.439438      0.202960         0.863515        0.071212       5   \n",
       "\n",
       "   param_degree param_gamma param_kernel param_max_iter  \\\n",
       "51           10        0.04          rbf         100000   \n",
       "31            3        0.04          rbf         100000   \n",
       "41            7        0.04          rbf         100000   \n",
       "3             3     0.66874          rbf         100000   \n",
       "23           10     0.66874          rbf         100000   \n",
       "13            7     0.66874          rbf         100000   \n",
       "1             3        0.04          rbf         100000   \n",
       "21           10        0.04          rbf         100000   \n",
       "11            7        0.04          rbf         100000   \n",
       "33            3     0.66874          rbf         100000   \n",
       "43            7     0.66874          rbf         100000   \n",
       "53           10     0.66874          rbf         100000   \n",
       "0             3        0.04       linear         100000   \n",
       "18            7        3125       linear         100000   \n",
       "24           10     11.1803       linear         100000   \n",
       "26           10     186.919       linear         100000   \n",
       "20           10        0.04       linear         100000   \n",
       "22           10     0.66874       linear         100000   \n",
       "14            7     11.1803       linear         100000   \n",
       "12            7     0.66874       linear         100000   \n",
       "10            7        0.04       linear         100000   \n",
       "8             3        3125       linear         100000   \n",
       "6             3     186.919       linear         100000   \n",
       "4             3     11.1803       linear         100000   \n",
       "2             3     0.66874       linear         100000   \n",
       "16            7     186.919       linear         100000   \n",
       "28           10        3125       linear         100000   \n",
       "52           10     0.66874       linear         100000   \n",
       "38            3        3125       linear         100000   \n",
       "42            7     0.66874       linear         100000   \n",
       "44            7     11.1803       linear         100000   \n",
       "46            7     186.919       linear         100000   \n",
       "48            7        3125       linear         100000   \n",
       "40            7        0.04       linear         100000   \n",
       "36            3     186.919       linear         100000   \n",
       "50           10        0.04       linear         100000   \n",
       "34            3     11.1803       linear         100000   \n",
       "32            3     0.66874       linear         100000   \n",
       "30            3        0.04       linear         100000   \n",
       "58           10        3125       linear         100000   \n",
       "54           10     11.1803       linear         100000   \n",
       "56           10     186.919       linear         100000   \n",
       "55           10     11.1803          rbf         100000   \n",
       "45            7     11.1803          rbf         100000   \n",
       "35            3     11.1803          rbf         100000   \n",
       "15            7     11.1803          rbf         100000   \n",
       "5             3     11.1803          rbf         100000   \n",
       "25           10     11.1803          rbf         100000   \n",
       "37            3     186.919          rbf         100000   \n",
       "57           10     186.919          rbf         100000   \n",
       "47            7     186.919          rbf         100000   \n",
       "29           10        3125          rbf         100000   \n",
       "39            3        3125          rbf         100000   \n",
       "27           10     186.919          rbf         100000   \n",
       "19            7        3125          rbf         100000   \n",
       "17            7     186.919          rbf         100000   \n",
       "9             3        3125          rbf         100000   \n",
       "7             3     186.919          rbf         100000   \n",
       "49            7        3125          rbf         100000   \n",
       "59           10        3125          rbf         100000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "51  {'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.809191   \n",
       "31  {'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.809191   \n",
       "41  {'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.809191   \n",
       "3   {'C': 1, 'degree': 3, 'gamma': 0.6687403049764...           0.808192   \n",
       "23  {'C': 1, 'degree': 10, 'gamma': 0.668740304976...           0.808192   \n",
       "13  {'C': 1, 'degree': 7, 'gamma': 0.6687403049764...           0.808192   \n",
       "1   {'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.798202   \n",
       "21  {'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.798202   \n",
       "11  {'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.798202   \n",
       "33  {'C': 5, 'degree': 3, 'gamma': 0.6687403049764...           0.808192   \n",
       "43  {'C': 5, 'degree': 7, 'gamma': 0.6687403049764...           0.808192   \n",
       "53  {'C': 5, 'degree': 10, 'gamma': 0.668740304976...           0.808192   \n",
       "0   {'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.778222   \n",
       "18  {'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.778222   \n",
       "24  {'C': 1, 'degree': 10, 'gamma': 11.18033988749...           0.778222   \n",
       "26  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...           0.778222   \n",
       "20  {'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.778222   \n",
       "22  {'C': 1, 'degree': 10, 'gamma': 0.668740304976...           0.778222   \n",
       "14  {'C': 1, 'degree': 7, 'gamma': 11.180339887498...           0.778222   \n",
       "12  {'C': 1, 'degree': 7, 'gamma': 0.6687403049764...           0.778222   \n",
       "10  {'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.778222   \n",
       "8   {'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.778222   \n",
       "6   {'C': 1, 'degree': 3, 'gamma': 186.91859765265...           0.778222   \n",
       "4   {'C': 1, 'degree': 3, 'gamma': 11.180339887498...           0.778222   \n",
       "2   {'C': 1, 'degree': 3, 'gamma': 0.6687403049764...           0.778222   \n",
       "16  {'C': 1, 'degree': 7, 'gamma': 186.91859765265...           0.778222   \n",
       "28  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.778222   \n",
       "52  {'C': 5, 'degree': 10, 'gamma': 0.668740304976...           0.775225   \n",
       "38  {'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.775225   \n",
       "42  {'C': 5, 'degree': 7, 'gamma': 0.6687403049764...           0.775225   \n",
       "44  {'C': 5, 'degree': 7, 'gamma': 11.180339887498...           0.775225   \n",
       "46  {'C': 5, 'degree': 7, 'gamma': 186.91859765265...           0.775225   \n",
       "48  {'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.775225   \n",
       "40  {'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.775225   \n",
       "36  {'C': 5, 'degree': 3, 'gamma': 186.91859765265...           0.775225   \n",
       "50  {'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.775225   \n",
       "34  {'C': 5, 'degree': 3, 'gamma': 11.180339887498...           0.775225   \n",
       "32  {'C': 5, 'degree': 3, 'gamma': 0.6687403049764...           0.775225   \n",
       "30  {'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.775225   \n",
       "58  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.775225   \n",
       "54  {'C': 5, 'degree': 10, 'gamma': 11.18033988749...           0.775225   \n",
       "56  {'C': 5, 'degree': 10, 'gamma': 186.9185976526...           0.775225   \n",
       "55  {'C': 5, 'degree': 10, 'gamma': 11.18033988749...           0.594406   \n",
       "45  {'C': 5, 'degree': 7, 'gamma': 11.180339887498...           0.594406   \n",
       "35  {'C': 5, 'degree': 3, 'gamma': 11.180339887498...           0.594406   \n",
       "15  {'C': 1, 'degree': 7, 'gamma': 11.180339887498...           0.584416   \n",
       "5   {'C': 1, 'degree': 3, 'gamma': 11.180339887498...           0.584416   \n",
       "25  {'C': 1, 'degree': 10, 'gamma': 11.18033988749...           0.584416   \n",
       "37  {'C': 5, 'degree': 3, 'gamma': 186.91859765265...           0.569431   \n",
       "57  {'C': 5, 'degree': 10, 'gamma': 186.9185976526...           0.569431   \n",
       "47  {'C': 5, 'degree': 7, 'gamma': 186.91859765265...           0.569431   \n",
       "29  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.568432   \n",
       "39  {'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "27  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...           0.568432   \n",
       "19  {'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "17  {'C': 1, 'degree': 7, 'gamma': 186.91859765265...           0.568432   \n",
       "9   {'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "7   {'C': 1, 'degree': 3, 'gamma': 186.91859765265...           0.568432   \n",
       "49  {'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "59  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.568432   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "51           0.802198              0.801           0.789790   \n",
       "31           0.802198              0.801           0.789790   \n",
       "41           0.802198              0.801           0.789790   \n",
       "3            0.798202              0.786           0.784785   \n",
       "23           0.798202              0.786           0.784785   \n",
       "13           0.798202              0.786           0.784785   \n",
       "1            0.790210              0.790           0.797798   \n",
       "21           0.790210              0.790           0.797798   \n",
       "11           0.790210              0.790           0.797798   \n",
       "33           0.777223              0.787           0.781782   \n",
       "43           0.777223              0.787           0.781782   \n",
       "53           0.777223              0.787           0.781782   \n",
       "0            0.772228              0.793           0.780781   \n",
       "18           0.772228              0.793           0.780781   \n",
       "24           0.772228              0.793           0.780781   \n",
       "26           0.772228              0.793           0.780781   \n",
       "20           0.772228              0.793           0.780781   \n",
       "22           0.772228              0.793           0.780781   \n",
       "14           0.772228              0.793           0.780781   \n",
       "12           0.772228              0.793           0.780781   \n",
       "10           0.772228              0.793           0.780781   \n",
       "8            0.772228              0.793           0.780781   \n",
       "6            0.772228              0.793           0.780781   \n",
       "4            0.772228              0.793           0.780781   \n",
       "2            0.772228              0.793           0.780781   \n",
       "16           0.772228              0.793           0.780781   \n",
       "28           0.772228              0.793           0.780781   \n",
       "52           0.772228              0.792           0.779780   \n",
       "38           0.772228              0.792           0.779780   \n",
       "42           0.772228              0.792           0.779780   \n",
       "44           0.772228              0.792           0.779780   \n",
       "46           0.772228              0.792           0.779780   \n",
       "48           0.772228              0.792           0.779780   \n",
       "40           0.772228              0.792           0.779780   \n",
       "36           0.772228              0.792           0.779780   \n",
       "50           0.772228              0.792           0.779780   \n",
       "34           0.772228              0.792           0.779780   \n",
       "32           0.772228              0.792           0.779780   \n",
       "30           0.772228              0.792           0.779780   \n",
       "58           0.772228              0.792           0.779780   \n",
       "54           0.772228              0.792           0.779780   \n",
       "56           0.772228              0.792           0.779780   \n",
       "55           0.596404              0.594           0.597598   \n",
       "45           0.596404              0.594           0.597598   \n",
       "35           0.596404              0.594           0.597598   \n",
       "15           0.589411              0.588           0.591592   \n",
       "5            0.589411              0.588           0.591592   \n",
       "25           0.589411              0.588           0.591592   \n",
       "37           0.568432              0.570           0.568569   \n",
       "57           0.568432              0.570           0.568569   \n",
       "47           0.568432              0.570           0.568569   \n",
       "29           0.568432              0.569           0.568569   \n",
       "39           0.568432              0.569           0.568569   \n",
       "27           0.568432              0.569           0.568569   \n",
       "19           0.568432              0.569           0.568569   \n",
       "17           0.568432              0.569           0.568569   \n",
       "9            0.568432              0.569           0.568569   \n",
       "7            0.568432              0.569           0.568569   \n",
       "49           0.568432              0.569           0.568569   \n",
       "59           0.568432              0.569           0.568569   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "51           0.813814           0.8032        0.008173                1  \n",
       "31           0.813814           0.8032        0.008173                1  \n",
       "41           0.813814           0.8032        0.008173                1  \n",
       "3            0.801802           0.7958        0.009085                4  \n",
       "23           0.801802           0.7958        0.009085                4  \n",
       "13           0.801802           0.7958        0.009085                4  \n",
       "1            0.794795           0.7942        0.003547                7  \n",
       "21           0.794795           0.7942        0.003547                7  \n",
       "11           0.794795           0.7942        0.003547                7  \n",
       "33           0.800801           0.7910        0.011685               10  \n",
       "43           0.800801           0.7910        0.011685               10  \n",
       "53           0.800801           0.7910        0.011685               10  \n",
       "0            0.794795           0.7838        0.008714               13  \n",
       "18           0.794795           0.7838        0.008714               13  \n",
       "24           0.794795           0.7838        0.008714               13  \n",
       "26           0.794795           0.7838        0.008714               13  \n",
       "20           0.794795           0.7838        0.008714               13  \n",
       "22           0.794795           0.7838        0.008714               13  \n",
       "14           0.794795           0.7838        0.008714               13  \n",
       "12           0.794795           0.7838        0.008714               13  \n",
       "10           0.794795           0.7838        0.008714               13  \n",
       "8            0.794795           0.7838        0.008714               13  \n",
       "6            0.794795           0.7838        0.008714               13  \n",
       "4            0.794795           0.7838        0.008714               13  \n",
       "2            0.794795           0.7838        0.008714               13  \n",
       "16           0.794795           0.7838        0.008714               13  \n",
       "28           0.794795           0.7838        0.008714               13  \n",
       "52           0.794795           0.7828        0.009020               28  \n",
       "38           0.794795           0.7828        0.009020               28  \n",
       "42           0.794795           0.7828        0.009020               28  \n",
       "44           0.794795           0.7828        0.009020               28  \n",
       "46           0.794795           0.7828        0.009020               28  \n",
       "48           0.794795           0.7828        0.009020               28  \n",
       "40           0.794795           0.7828        0.009020               28  \n",
       "36           0.794795           0.7828        0.009020               28  \n",
       "50           0.794795           0.7828        0.009020               28  \n",
       "34           0.794795           0.7828        0.009020               28  \n",
       "32           0.794795           0.7828        0.009020               28  \n",
       "30           0.794795           0.7828        0.009020               28  \n",
       "58           0.794795           0.7828        0.009020               28  \n",
       "54           0.794795           0.7828        0.009020               28  \n",
       "56           0.794795           0.7828        0.009020               28  \n",
       "55           0.604605           0.5974        0.003832               43  \n",
       "45           0.604605           0.5974        0.003832               43  \n",
       "35           0.604605           0.5974        0.003832               43  \n",
       "15           0.595596           0.5898        0.003720               46  \n",
       "5            0.595596           0.5898        0.003720               46  \n",
       "25           0.595596           0.5898        0.003720               46  \n",
       "37           0.568569           0.5690        0.000613               49  \n",
       "57           0.568569           0.5690        0.000613               49  \n",
       "47           0.568569           0.5690        0.000613               49  \n",
       "29           0.568569           0.5686        0.000209               52  \n",
       "39           0.568569           0.5686        0.000209               52  \n",
       "27           0.568569           0.5686        0.000209               52  \n",
       "19           0.568569           0.5686        0.000209               52  \n",
       "17           0.568569           0.5686        0.000209               52  \n",
       "9            0.568569           0.5686        0.000209               52  \n",
       "7            0.568569           0.5686        0.000209               52  \n",
       "49           0.568569           0.5686        0.000209               52  \n",
       "59           0.568569           0.5686        0.000209               52  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kernel': 'rbf', 'max_iter': 100000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>5.798716</td>\n",
       "      <td>0.341561</td>\n",
       "      <td>1.273867</td>\n",
       "      <td>0.061789</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.870519</td>\n",
       "      <td>0.380944</td>\n",
       "      <td>1.263730</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.848913</td>\n",
       "      <td>0.398054</td>\n",
       "      <td>1.273519</td>\n",
       "      <td>0.080736</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>6.105049</td>\n",
       "      <td>0.352619</td>\n",
       "      <td>1.273745</td>\n",
       "      <td>0.073564</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.032891</td>\n",
       "      <td>0.352681</td>\n",
       "      <td>1.293661</td>\n",
       "      <td>0.073773</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>8.256499</td>\n",
       "      <td>0.474628</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.012694</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>10.086388</td>\n",
       "      <td>0.197747</td>\n",
       "      <td>0.170566</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.598050</td>\n",
       "      <td>0.267677</td>\n",
       "      <td>0.102326</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.198848</td>\n",
       "      <td>0.379884</td>\n",
       "      <td>0.102765</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 2, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.429606</td>\n",
       "      <td>0.667812</td>\n",
       "      <td>0.100597</td>\n",
       "      <td>0.011946</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 2, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "39       5.798716      0.341561         1.273867        0.061789       1   \n",
       "31       5.870519      0.380944         1.263730        0.064457       1   \n",
       "33       5.848913      0.398054         1.273519        0.080736       1   \n",
       "35       6.105049      0.352619         1.273745        0.073564       1   \n",
       "37       6.032891      0.352681         1.293661        0.073773       1   \n",
       "..            ...           ...              ...             ...     ...   \n",
       "16       8.256499      0.474628         0.109705        0.012694       1   \n",
       "18      10.086388      0.197747         0.170566        0.034574       1   \n",
       "36       7.598050      0.267677         0.102326        0.008662       1   \n",
       "8        8.198848      0.379884         0.102765        0.009486       1   \n",
       "0        7.429606      0.667812         0.100597        0.011946       1   \n",
       "\n",
       "   param_degree param_gamma param_kernel param_max_iter  \\\n",
       "39           10        3125          rbf         100000   \n",
       "31           10        0.04          rbf         100000   \n",
       "33           10     0.66874          rbf         100000   \n",
       "35           10     11.1803          rbf         100000   \n",
       "37           10     186.919          rbf         100000   \n",
       "..          ...         ...          ...            ...   \n",
       "16            3     186.919       linear         100000   \n",
       "18            3        3125       linear         100000   \n",
       "36           10     186.919       linear         100000   \n",
       "8             2        3125       linear         100000   \n",
       "0             2        0.04       linear         100000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "39  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...              0.578   \n",
       "31  {'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...              0.578   \n",
       "33  {'C': 1, 'degree': 10, 'gamma': 0.668740304976...              0.578   \n",
       "35  {'C': 1, 'degree': 10, 'gamma': 11.18033988749...              0.578   \n",
       "37  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...              0.578   \n",
       "..                                                ...                ...   \n",
       "16  {'C': 1, 'degree': 3, 'gamma': 186.91859765265...              0.528   \n",
       "18  {'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...              0.528   \n",
       "36  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...              0.528   \n",
       "8   {'C': 1, 'degree': 2, 'gamma': 3125.0, 'kernel...              0.528   \n",
       "0   {'C': 1, 'degree': 2, 'gamma': 0.04, 'kernel':...              0.528   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "39              0.578              0.578              0.578   \n",
       "31              0.578              0.578              0.578   \n",
       "33              0.578              0.578              0.578   \n",
       "35              0.578              0.578              0.578   \n",
       "37              0.578              0.578              0.578   \n",
       "..                ...                ...                ...   \n",
       "16              0.598              0.543              0.495   \n",
       "18              0.598              0.543              0.495   \n",
       "36              0.598              0.543              0.495   \n",
       "8               0.598              0.543              0.495   \n",
       "0               0.598              0.543              0.495   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "39              0.578           0.5780        0.000000                1  \n",
       "31              0.578           0.5780        0.000000                1  \n",
       "33              0.578           0.5780        0.000000                1  \n",
       "35              0.578           0.5780        0.000000                1  \n",
       "37              0.578           0.5780        0.000000                1  \n",
       "..                ...              ...             ...              ...  \n",
       "16              0.490           0.5308        0.039035               61  \n",
       "18              0.490           0.5308        0.039035               61  \n",
       "36              0.490           0.5308        0.039035               61  \n",
       "8               0.490           0.5308        0.039035               61  \n",
       "0               0.490           0.5308        0.039035               61  \n",
       "\n",
       "[80 rows x 18 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kernel': 'linear', 'max_iter': 100000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>5.987801</td>\n",
       "      <td>0.221501</td>\n",
       "      <td>0.068331</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>7.997192</td>\n",
       "      <td>0.408793</td>\n",
       "      <td>0.106914</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>73.1004</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 73.100443455321...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>8.001259</td>\n",
       "      <td>0.302747</td>\n",
       "      <td>0.099534</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>20.9063</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 20.906275773759...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>8.874059</td>\n",
       "      <td>0.521921</td>\n",
       "      <td>0.101927</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5.97907</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 5.9790658725020...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>9.529589</td>\n",
       "      <td>0.282009</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.70998</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 1.7099759466766...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>8.095172</td>\n",
       "      <td>0.437890</td>\n",
       "      <td>0.121276</td>\n",
       "      <td>0.014693</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>5.97907</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 5.97906587250...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>8.226855</td>\n",
       "      <td>0.397989</td>\n",
       "      <td>0.130860</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.70998</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 1.70997594667...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>9.454508</td>\n",
       "      <td>0.903808</td>\n",
       "      <td>0.123868</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.489043</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 0.48904256961...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.144085</td>\n",
       "      <td>0.610632</td>\n",
       "      <td>0.123669</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 0.04, 'kernel...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.795585</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.114694</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 2, 'gamma': 0.04, 'kernel...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "159       5.987801      0.221501         0.068331        0.009900       5   \n",
       "136       7.997192      0.408793         0.106914        0.023461       5   \n",
       "135       8.001259      0.302747         0.099534        0.008952       5   \n",
       "134       8.874059      0.521921         0.101927        0.006688       5   \n",
       "133       9.529589      0.282009         0.118613        0.011472       5   \n",
       "..             ...           ...              ...             ...     ...   \n",
       "24        8.095172      0.437890         0.121276        0.014693     0.1   \n",
       "23        8.226855      0.397989         0.130860        0.012974     0.1   \n",
       "22        9.454508      0.903808         0.123868        0.012350     0.1   \n",
       "20        8.144085      0.610632         0.123669        0.007490     0.1   \n",
       "0         7.795585      0.330739         0.114694        0.004886     0.1   \n",
       "\n",
       "    param_degree param_gamma param_kernel param_max_iter  \\\n",
       "159           10        3125       linear         100000   \n",
       "136            3     73.1004       linear         100000   \n",
       "135            3     20.9063       linear         100000   \n",
       "134            3     5.97907       linear         100000   \n",
       "133            3     1.70998       linear         100000   \n",
       "..           ...         ...          ...            ...   \n",
       "24             7     5.97907       linear         100000   \n",
       "23             7     1.70998       linear         100000   \n",
       "22             7    0.489043       linear         100000   \n",
       "20             7        0.04       linear         100000   \n",
       "0              2        0.04       linear         100000   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "159  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...            0.53047   \n",
       "136  {'C': 5, 'degree': 3, 'gamma': 73.100443455321...            0.53047   \n",
       "135  {'C': 5, 'degree': 3, 'gamma': 20.906275773759...            0.53047   \n",
       "134  {'C': 5, 'degree': 3, 'gamma': 5.9790658725020...            0.53047   \n",
       "133  {'C': 5, 'degree': 3, 'gamma': 1.7099759466766...            0.53047   \n",
       "..                                                 ...                ...   \n",
       "24   {'C': 0.1, 'degree': 7, 'gamma': 5.97906587250...            0.48951   \n",
       "23   {'C': 0.1, 'degree': 7, 'gamma': 1.70997594667...            0.48951   \n",
       "22   {'C': 0.1, 'degree': 7, 'gamma': 0.48904256961...            0.48951   \n",
       "20   {'C': 0.1, 'degree': 7, 'gamma': 0.04, 'kernel...            0.48951   \n",
       "0    {'C': 0.1, 'degree': 2, 'gamma': 0.04, 'kernel...            0.48951   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "159              0.464              0.614              0.564   \n",
       "136              0.464              0.614              0.564   \n",
       "135              0.464              0.614              0.564   \n",
       "134              0.464              0.614              0.564   \n",
       "133              0.464              0.614              0.564   \n",
       "..                 ...                ...                ...   \n",
       "24               0.488              0.526              0.483   \n",
       "23               0.488              0.526              0.483   \n",
       "22               0.488              0.526              0.483   \n",
       "20               0.488              0.526              0.483   \n",
       "0                0.488              0.526              0.483   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "159           0.661662           0.5668        0.067992                1  \n",
       "136           0.661662           0.5668        0.067992                1  \n",
       "135           0.661662           0.5668        0.067992                1  \n",
       "134           0.661662           0.5668        0.067992                1  \n",
       "133           0.661662           0.5668        0.067992                1  \n",
       "..                 ...              ...             ...              ...  \n",
       "24            0.530531           0.5034        0.020457              121  \n",
       "23            0.530531           0.5034        0.020457              121  \n",
       "22            0.530531           0.5034        0.020457              121  \n",
       "20            0.530531           0.5034        0.020457              121  \n",
       "0             0.530531           0.5034        0.020457              121  \n",
       "\n",
       "[160 rows x 18 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

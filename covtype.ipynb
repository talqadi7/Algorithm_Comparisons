{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, accuracy_score, f1_score, precision_score\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import ensemble, preprocessing\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from my_functions import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import LinearSVC as LSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "0       2596   51   3  258    0   510  221  232  148  6279  ...   0   0   0   \n",
       "1       2590   56   2  212   -6   390  220  235  151  6225  ...   0   0   0   \n",
       "2       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "3       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "4       2595   45   2  153   -1   391  220  234  150  6172  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "581007  2396  153  20   85   17   108  240  237  118   837  ...   0   0   0   \n",
       "581008  2391  152  19   67   12    95  240  237  119   845  ...   0   0   0   \n",
       "581009  2386  159  17   60    7    90  236  241  130   854  ...   0   0   0   \n",
       "581010  2384  170  15   60    5    90  230  245  143   864  ...   0   0   0   \n",
       "581011  2383  165  13   60    4    67  231  244  141   875  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "0        0   0   0   0   0   0   5  \n",
       "1        0   0   0   0   0   0   5  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "4        0   0   0   0   0   0   5  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "581007   0   0   0   0   0   0   3  \n",
       "581008   0   0   0   0   0   0   3  \n",
       "581009   0   0   0   0   0   0   3  \n",
       "581010   0   0   0   0   0   0   3  \n",
       "581011   0   0   0   0   0   0   3  \n",
       "\n",
       "[581012 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/covtype.data', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "3     35754\n",
       "7     20510\n",
       "6     17367\n",
       "5      9493\n",
       "4      2747\n",
       "Name: 54, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[54].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2886</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>26</td>\n",
       "      <td>5253</td>\n",
       "      <td>234</td>\n",
       "      <td>240</td>\n",
       "      <td>136</td>\n",
       "      <td>4051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2742</td>\n",
       "      <td>134</td>\n",
       "      <td>22</td>\n",
       "      <td>150</td>\n",
       "      <td>69</td>\n",
       "      <td>3215</td>\n",
       "      <td>248</td>\n",
       "      <td>224</td>\n",
       "      <td>92</td>\n",
       "      <td>6091</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576882</th>\n",
       "      <td>2617</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>390</td>\n",
       "      <td>128</td>\n",
       "      <td>2081</td>\n",
       "      <td>215</td>\n",
       "      <td>211</td>\n",
       "      <td>130</td>\n",
       "      <td>592</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576883</th>\n",
       "      <td>2614</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>125</td>\n",
       "      <td>2051</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>618</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576884</th>\n",
       "      <td>2612</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>371</td>\n",
       "      <td>123</td>\n",
       "      <td>2021</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576885</th>\n",
       "      <td>2610</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>365</td>\n",
       "      <td>110</td>\n",
       "      <td>1991</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576886</th>\n",
       "      <td>2608</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>361</td>\n",
       "      <td>108</td>\n",
       "      <td>1961</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>131</td>\n",
       "      <td>698</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495141 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "2       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "3       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "5       2579  132   6  300  -15    67  230  237  140  6031  ...   0   0   0   \n",
       "11      2886  151  11  371   26  5253  234  240  136  4051  ...   0   0   0   \n",
       "12      2742  134  22  150   69  3215  248  224   92  6091  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "576882  2617   29  13  390  128  2081  215  211  130   592  ...   0   0   0   \n",
       "576883  2614   21  13  379  125  2051  211  212  135   618  ...   0   0   0   \n",
       "576884  2612   17  13  371  123  2021  208  211  138   644  ...   0   0   0   \n",
       "576885  2610   16  14  365  110  1991  208  211  138   671  ...   0   0   0   \n",
       "576886  2608   23  14  361  108  1961  211  209  131   698  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "5        0   0   0   0   0   0   2  \n",
       "11       0   0   0   0   0   0   2  \n",
       "12       0   0   0   0   0   0   2  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "576882   0   0   0   0   0   0   2  \n",
       "576883   0   0   0   0   0   0   2  \n",
       "576884   0   0   0   0   0   0   2  \n",
       "576885   0   0   0   0   0   0   2  \n",
       "576886   0   0   0   0   0   0   2  \n",
       "\n",
       "[495141 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[54] <= 2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2886</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>26</td>\n",
       "      <td>5253</td>\n",
       "      <td>234</td>\n",
       "      <td>240</td>\n",
       "      <td>136</td>\n",
       "      <td>4051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2742</td>\n",
       "      <td>134</td>\n",
       "      <td>22</td>\n",
       "      <td>150</td>\n",
       "      <td>69</td>\n",
       "      <td>3215</td>\n",
       "      <td>248</td>\n",
       "      <td>224</td>\n",
       "      <td>92</td>\n",
       "      <td>6091</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495136</th>\n",
       "      <td>2617</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>390</td>\n",
       "      <td>128</td>\n",
       "      <td>2081</td>\n",
       "      <td>215</td>\n",
       "      <td>211</td>\n",
       "      <td>130</td>\n",
       "      <td>592</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495137</th>\n",
       "      <td>2614</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>125</td>\n",
       "      <td>2051</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>135</td>\n",
       "      <td>618</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495138</th>\n",
       "      <td>2612</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>371</td>\n",
       "      <td>123</td>\n",
       "      <td>2021</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495139</th>\n",
       "      <td>2610</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>365</td>\n",
       "      <td>110</td>\n",
       "      <td>1991</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>138</td>\n",
       "      <td>671</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495140</th>\n",
       "      <td>2608</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>361</td>\n",
       "      <td>108</td>\n",
       "      <td>1961</td>\n",
       "      <td>211</td>\n",
       "      <td>209</td>\n",
       "      <td>131</td>\n",
       "      <td>698</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495141 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  45  46  47  \\\n",
       "0       2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   \n",
       "1       2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   \n",
       "2       2579  132   6  300  -15    67  230  237  140  6031  ...   0   0   0   \n",
       "3       2886  151  11  371   26  5253  234  240  136  4051  ...   0   0   0   \n",
       "4       2742  134  22  150   69  3215  248  224   92  6091  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "495136  2617   29  13  390  128  2081  215  211  130   592  ...   0   0   0   \n",
       "495137  2614   21  13  379  125  2051  211  212  135   618  ...   0   0   0   \n",
       "495138  2612   17  13  371  123  2021  208  211  138   644  ...   0   0   0   \n",
       "495139  2610   16  14  365  110  1991  208  211  138   671  ...   0   0   0   \n",
       "495140  2608   23  14  361  108  1961  211  209  131   698  ...   0   0   0   \n",
       "\n",
       "        48  49  50  51  52  53  54  \n",
       "0        0   0   0   0   0   0   2  \n",
       "1        0   0   0   0   0   0   2  \n",
       "2        0   0   0   0   0   0   2  \n",
       "3        0   0   0   0   0   0   2  \n",
       "4        0   0   0   0   0   0   2  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "495136   0   0   0   0   0   0   2  \n",
       "495137   0   0   0   0   0   0   2  \n",
       "495138   0   0   0   0   0   0   2  \n",
       "495139   0   0   0   0   0   0   2  \n",
       "495140   0   0   0   0   0   0   2  \n",
       "\n",
       "[495141 rows x 55 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().drop('index', axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "Name: 54, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[54].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(54, axis = 1)\n",
    "Y = pd.factorize(df[54])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = shuffle(X, Y)\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_train = X_train[0:50000]\n",
    "Y_train = Y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([28844, 21156], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    clf = ensemble.RandomForestClassifier(n_jobs = -1, n_estimators=200, \n",
    "                                          random_state=42)\n",
    "    clf.fit(x_train1, y_train1)\n",
    "    \n",
    "    probs_train = clf.predict_proba(x_train1)[:,1]\n",
    "    t_f1, t_apr, t_acc, t_auc = matrix_info(0.7521, y_train1, probs_train)    \n",
    "    \n",
    "    probs = clf.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.6651,y_test1, probs)\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3), round(t_f1,3), round(t_apr,3), round(t_acc,3), round(t_auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    start = time.time()\n",
    "    mlp = MLPClassifier(solver='adam', activation='relu', alpha=0.001, \n",
    "                        hidden_layer_sizes = (32, 32), max_iter = 10000)\n",
    "    mlp.fit(x_train1, y_train1)\n",
    "    \n",
    "    probs_train = mlp.predict_proba(x_train1)[:,1]\n",
    "    t_f1, t_apr, t_acc, t_auc = matrix_info(0.7521, y_train1, probs_train)\n",
    "    \n",
    "    probs = mlp.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.77, y_test1, probs)\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3), round(t_f1,3), round(t_apr,3), round(t_acc,3), round(t_auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    start = time.time()\n",
    "    svc = SVC(kernel = 'rbf', C = 5, degree = 10, gamma = 0.04, \n",
    "              max_iter =  100000, probability = True, n_jobs = -1)\n",
    "    svc.fit(x_train1, y_train1)\n",
    "    \n",
    "    probs_train = svc.predict_proba(x_train1)[:,1]\n",
    "    t_f1, t_apr, t_acc, t_auc = matrix_info(0.7521, y_train1, probs_train)\n",
    "    \n",
    "    probs = svc.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.7521,y_test1, probs)\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3), round(t_f1,3), round(t_apr,3), round(t_acc,3), round(t_auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSVMfunc(x_train1, x_test1, y_train1, y_test1):\n",
    "    start=time.time()\n",
    "    svc = LSVC(C = 3, loss = 'hinge', max_iter=10000)\n",
    "    svc = CalibratedClassifierCV(svc)\n",
    "    svc.fit(x_train1,y_train1)\n",
    "    \n",
    "    probs_train = svc.predict_proba(x_train1)[:,1]\n",
    "    t_f1, t_apr, t_acc, t_auc = matrix_info(0.7521, y_train1, probs_train)\n",
    "    \n",
    "    probs = svc.predict_proba(x_test1)[:,1]\n",
    "    f1, apr, acc, auc = matrix_info(0.7521,y_test1, probs)\n",
    "    \n",
    "\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    return round(f1,3), round(apr,3), round(acc,3), round(auc,3), round(t_f1,3), round(t_apr,3), round(t_acc,3), round(t_auc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23044     0]\n",
      " [    0 16956]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23044\n",
      "           1       1.00      1.00      1.00     16956\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.9007915565555222\n",
      "precision_score:\n",
      "0.9006943145845983\n",
      "accuracy_score:\n",
      "0.903\n",
      "Confusion Matrix:\n",
      "[[5261  489]\n",
      " [ 481 3769]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      5750\n",
      "           1       0.89      0.89      0.89      4250\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8965681967895334\n",
      "precision_score:\n",
      "0.8972350115050589\n",
      "accuracy_score:\n",
      "0.899125\n",
      "Confusion Matrix:\n",
      "[[21127  1917]\n",
      " [ 2118 14838]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91     23044\n",
      "           1       0.89      0.88      0.88     16956\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.90      0.90      0.90     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8739328582327219\n",
      "precision_score:\n",
      "0.8731866913218745\n",
      "accuracy_score:\n",
      "0.8765\n",
      "Confusion Matrix:\n",
      "[[5096  654]\n",
      " [ 581 3669]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      5750\n",
      "           1       0.85      0.86      0.86      4250\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Elapsed Time: 38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7745064123958214\n",
      "precision_score:\n",
      "0.7731906785487533\n",
      "accuracy_score:\n",
      "0.777975\n",
      "Confusion Matrix:\n",
      "[[18040  5004]\n",
      " [ 3877 13079]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80     23044\n",
      "           1       0.72      0.77      0.75     16956\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7680026646376427\n",
      "precision_score:\n",
      "0.767123918738698\n",
      "accuracy_score:\n",
      "0.7723\n",
      "Confusion Matrix:\n",
      "[[4542 1208]\n",
      " [1069 3181]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      5750\n",
      "           1       0.72      0.75      0.74      4250\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 67 seconds\n",
      "0.2 1\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23022     0]\n",
      " [    0 16978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23022\n",
      "           1       1.00      1.00      1.00     16978\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.900847443174373\n",
      "precision_score:\n",
      "0.8987833068122502\n",
      "accuracy_score:\n",
      "0.9025\n",
      "Confusion Matrix:\n",
      "[[5158  614]\n",
      " [ 361 3867]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      5772\n",
      "           1       0.86      0.91      0.89      4228\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8946628578552197\n",
      "precision_score:\n",
      "0.8946904110330873\n",
      "accuracy_score:\n",
      "0.897075\n",
      "Confusion Matrix:\n",
      "[[20968  2054]\n",
      " [ 2063 14915]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91     23022\n",
      "           1       0.88      0.88      0.88     16978\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.89      0.89      0.89     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8662260636559702\n",
      "precision_score:\n",
      "0.8652046023389641\n",
      "accuracy_score:\n",
      "0.869\n",
      "Confusion Matrix:\n",
      "[[5065  707]\n",
      " [ 603 3625]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      5772\n",
      "           1       0.84      0.86      0.85      4228\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7708429551709264\n",
      "precision_score:\n",
      "0.769604433816248\n",
      "accuracy_score:\n",
      "0.773825\n",
      "Confusion Matrix:\n",
      "[[17758  5264]\n",
      " [ 3783 13195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80     23022\n",
      "           1       0.71      0.78      0.74     16978\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.77      0.77     40000\n",
      "\n",
      "f1_score:\n",
      "0.7778434576663362\n",
      "precision_score:\n",
      "0.7764659436987382\n",
      "accuracy_score:\n",
      "0.781\n",
      "Confusion Matrix:\n",
      "[[4501 1271]\n",
      " [ 919 3309]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80      5772\n",
      "           1       0.72      0.78      0.75      4228\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "Elapsed Time: 69 seconds\n",
      "0.2 2\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23021     0]\n",
      " [    0 16979]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23021\n",
      "           1       1.00      1.00      1.00     16979\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.9030704810076822\n",
      "precision_score:\n",
      "0.9019844352523478\n",
      "accuracy_score:\n",
      "0.9051\n",
      "Confusion Matrix:\n",
      "[[5249  524]\n",
      " [ 425 3802]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      5773\n",
      "           1       0.88      0.90      0.89      4227\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n",
      "f1_score:\n",
      "0.8987581692260642\n",
      "precision_score:\n",
      "0.8987859984325228\n",
      "accuracy_score:\n",
      "0.901075\n",
      "Confusion Matrix:\n",
      "[[21047  1974]\n",
      " [ 1983 14996]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91     23021\n",
      "           1       0.88      0.88      0.88     16979\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.90      0.90      0.90     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8774602482839948\n",
      "precision_score:\n",
      "0.877821738130089\n",
      "accuracy_score:\n",
      "0.8805\n",
      "Confusion Matrix:\n",
      "[[5190  583]\n",
      " [ 612 3615]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      5773\n",
      "           1       0.86      0.86      0.86      4227\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Elapsed Time: 43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7741496681631856\n",
      "precision_score:\n",
      "0.772929837628888\n",
      "accuracy_score:\n",
      "0.77785\n",
      "Confusion Matrix:\n",
      "[[18117  4904]\n",
      " [ 3982 12997]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     23021\n",
      "           1       0.73      0.77      0.75     16979\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7728563316297559\n",
      "precision_score:\n",
      "0.7715099430680532\n",
      "accuracy_score:\n",
      "0.7761\n",
      "Confusion Matrix:\n",
      "[[4478 1295]\n",
      " [ 944 3283]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80      5773\n",
      "           1       0.72      0.78      0.75      4227\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.77      0.78      0.77     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "Elapsed Time: 62 seconds\n",
      "0.5 0\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[14400     0]\n",
      " [    0 10600]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     14400\n",
      "           1       1.00      1.00      1.00     10600\n",
      "\n",
      "    accuracy                           1.00     25000\n",
      "   macro avg       1.00      1.00      1.00     25000\n",
      "weighted avg       1.00      1.00      1.00     25000\n",
      "\n",
      "f1_score:\n",
      "0.881012127146203\n",
      "precision_score:\n",
      "0.8795697353005985\n",
      "accuracy_score:\n",
      "0.8832\n",
      "Confusion Matrix:\n",
      "[[12735  1659]\n",
      " [ 1261  9345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90     14394\n",
      "           1       0.85      0.88      0.86     10606\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n",
      "f1_score:\n",
      "0.8909255768817244\n",
      "precision_score:\n",
      "0.8898725630270086\n",
      "accuracy_score:\n",
      "0.89312\n",
      "Confusion Matrix:\n",
      "[[12937  1463]\n",
      " [ 1209  9391]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     14400\n",
      "           1       0.87      0.89      0.88     10600\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "f1_score:\n",
      "0.8567174485686607\n",
      "precision_score:\n",
      "0.8560456534238029\n",
      "accuracy_score:\n",
      "0.85972\n",
      "Confusion Matrix:\n",
      "[[12556  1838]\n",
      " [ 1669  8937]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88     14394\n",
      "           1       0.83      0.84      0.84     10606\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n",
      "Elapsed Time: 25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7740025857493337\n",
      "precision_score:\n",
      "0.772928522416163\n",
      "accuracy_score:\n",
      "0.77808\n",
      "Confusion Matrix:\n",
      "[[11405  2995]\n",
      " [ 2553  8047]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     14400\n",
      "           1       0.73      0.76      0.74     10600\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.77      0.78      0.77     25000\n",
      "weighted avg       0.78      0.78      0.78     25000\n",
      "\n",
      "f1_score:\n",
      "0.7704779377190436\n",
      "precision_score:\n",
      "0.7692143427309853\n",
      "accuracy_score:\n",
      "0.77356\n",
      "Confusion Matrix:\n",
      "[[11118  3276]\n",
      " [ 2385  8221]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80     14394\n",
      "           1       0.72      0.78      0.74     10606\n",
      "\n",
      "    accuracy                           0.77     25000\n",
      "   macro avg       0.77      0.77      0.77     25000\n",
      "weighted avg       0.78      0.77      0.77     25000\n",
      "\n",
      "Elapsed Time: 18 seconds\n",
      "0.5 1\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[14379     0]\n",
      " [    0 10621]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     14379\n",
      "           1       1.00      1.00      1.00     10621\n",
      "\n",
      "    accuracy                           1.00     25000\n",
      "   macro avg       1.00      1.00      1.00     25000\n",
      "weighted avg       1.00      1.00      1.00     25000\n",
      "\n",
      "f1_score:\n",
      "0.885185821907158\n",
      "precision_score:\n",
      "0.8842049931042394\n",
      "accuracy_score:\n",
      "0.88756\n",
      "Confusion Matrix:\n",
      "[[12892  1523]\n",
      " [ 1288  9297]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90     14415\n",
      "           1       0.86      0.88      0.87     10585\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.88      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "f1_score:\n",
      "0.90088824438851\n",
      "precision_score:\n",
      "0.9013721969769524\n",
      "accuracy_score:\n",
      "0.90324\n",
      "Confusion Matrix:\n",
      "[[13216  1163]\n",
      " [ 1256  9365]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     14379\n",
      "           1       0.89      0.88      0.89     10621\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n",
      "f1_score:\n",
      "0.8660217111289705\n",
      "precision_score:\n",
      "0.8653825128069832\n",
      "accuracy_score:\n",
      "0.86892\n",
      "Confusion Matrix:\n",
      "[[12700  1715]\n",
      " [ 1562  9023]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89     14415\n",
      "           1       0.84      0.85      0.85     10585\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n",
      "Elapsed Time: 29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7726404291160907\n",
      "precision_score:\n",
      "0.7713669799436552\n",
      "accuracy_score:\n",
      "0.77604\n",
      "Confusion Matrix:\n",
      "[[11229  3150]\n",
      " [ 2449  8172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80     14379\n",
      "           1       0.72      0.77      0.74     10621\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.77      0.78      0.77     25000\n",
      "weighted avg       0.78      0.78      0.78     25000\n",
      "\n",
      "f1_score:\n",
      "0.770764139414511\n",
      "precision_score:\n",
      "0.7701357054090324\n",
      "accuracy_score:\n",
      "0.773\n",
      "Confusion Matrix:\n",
      "[[10897  3518]\n",
      " [ 2157  8428]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79     14415\n",
      "           1       0.71      0.80      0.75     10585\n",
      "\n",
      "    accuracy                           0.77     25000\n",
      "   macro avg       0.77      0.78      0.77     25000\n",
      "weighted avg       0.78      0.77      0.77     25000\n",
      "\n",
      "Elapsed Time: 29 seconds\n",
      "0.5 2\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[14444     0]\n",
      " [    0 10556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     14444\n",
      "           1       1.00      1.00      1.00     10556\n",
      "\n",
      "    accuracy                           1.00     25000\n",
      "   macro avg       1.00      1.00      1.00     25000\n",
      "weighted avg       1.00      1.00      1.00     25000\n",
      "\n",
      "f1_score:\n",
      "0.8798673767284841\n",
      "precision_score:\n",
      "0.8803449373119843\n",
      "accuracy_score:\n",
      "0.88264\n",
      "Confusion Matrix:\n",
      "[[12932  1418]\n",
      " [ 1516  9134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     14350\n",
      "           1       0.87      0.86      0.86     10650\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n",
      "f1_score:\n",
      "0.8920681641558699\n",
      "precision_score:\n",
      "0.8898920227819327\n",
      "accuracy_score:\n",
      "0.89376\n",
      "Confusion Matrix:\n",
      "[[12737  1707]\n",
      " [  949  9607]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91     14444\n",
      "           1       0.85      0.91      0.88     10556\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.90      0.89     25000\n",
      "weighted avg       0.90      0.89      0.89     25000\n",
      "\n",
      "f1_score:\n",
      "0.8540551101258811\n",
      "precision_score:\n",
      "0.8522898100357414\n",
      "accuracy_score:\n",
      "0.85592\n",
      "Confusion Matrix:\n",
      "[[12112  2238]\n",
      " [ 1364  9286]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     14350\n",
      "           1       0.81      0.87      0.84     10650\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.85      0.86      0.85     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n",
      "Elapsed Time: 27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7777820714510164\n",
      "precision_score:\n",
      "0.7764140976259479\n",
      "accuracy_score:\n",
      "0.78148\n",
      "Confusion Matrix:\n",
      "[[11381  3063]\n",
      " [ 2400  8156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81     14444\n",
      "           1       0.73      0.77      0.75     10556\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.78      0.78      0.78     25000\n",
      "weighted avg       0.78      0.78      0.78     25000\n",
      "\n",
      "f1_score:\n",
      "0.7689466858561287\n",
      "precision_score:\n",
      "0.7677411214476169\n",
      "accuracy_score:\n",
      "0.77196\n",
      "Confusion Matrix:\n",
      "[[11077  3273]\n",
      " [ 2428  8222]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80     14350\n",
      "           1       0.72      0.77      0.74     10650\n",
      "\n",
      "    accuracy                           0.77     25000\n",
      "   macro avg       0.77      0.77      0.77     25000\n",
      "weighted avg       0.78      0.77      0.77     25000\n",
      "\n",
      "Elapsed Time: 26 seconds\n",
      "0.8 0\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[5857    0]\n",
      " [   0 4143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5857\n",
      "           1       1.00      1.00      1.00      4143\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n",
      "f1_score:\n",
      "0.8488236949017469\n",
      "precision_score:\n",
      "0.848921956639404\n",
      "accuracy_score:\n",
      "0.852125\n",
      "Confusion Matrix:\n",
      "[[19998  2939]\n",
      " [ 2976 14087]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     22937\n",
      "           1       0.83      0.83      0.83     17063\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.85      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n",
      "f1_score:\n",
      "0.9073683557435993\n",
      "precision_score:\n",
      "0.9083833252256447\n",
      "accuracy_score:\n",
      "0.9103\n",
      "Confusion Matrix:\n",
      "[[5441  416]\n",
      " [ 481 3662]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92      5857\n",
      "           1       0.90      0.88      0.89      4143\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n",
      "f1_score:\n",
      "0.8299307879830177\n",
      "precision_score:\n",
      "0.8296478815956897\n",
      "accuracy_score:\n",
      "0.83345\n",
      "Confusion Matrix:\n",
      "[[19546  3391]\n",
      " [ 3271 13792]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85     22937\n",
      "           1       0.80      0.81      0.81     17063\n",
      "\n",
      "    accuracy                           0.83     40000\n",
      "   macro avg       0.83      0.83      0.83     40000\n",
      "weighted avg       0.83      0.83      0.83     40000\n",
      "\n",
      "Elapsed Time: 14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7760810652360273\n",
      "precision_score:\n",
      "0.7745307151645086\n",
      "accuracy_score:\n",
      "0.7809\n",
      "Confusion Matrix:\n",
      "[[4638 1219]\n",
      " [ 972 3171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81      5857\n",
      "           1       0.72      0.77      0.74      4143\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.77      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "f1_score:\n",
      "0.7729404038405092\n",
      "precision_score:\n",
      "0.771728594823081\n",
      "accuracy_score:\n",
      "0.77585\n",
      "Confusion Matrix:\n",
      "[[17781  5156]\n",
      " [ 3810 13253]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80     22937\n",
      "           1       0.72      0.78      0.75     17063\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "Elapsed Time: 3 seconds\n",
      "0.8 1\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[5756    0]\n",
      " [   0 4244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5756\n",
      "           1       1.00      1.00      1.00      4244\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n",
      "f1_score:\n",
      "0.842815098170886\n",
      "precision_score:\n",
      "0.8415976120911854\n",
      "accuracy_score:\n",
      "0.845775\n",
      "Confusion Matrix:\n",
      "[[19660  3378]\n",
      " [ 2791 14171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86     23038\n",
      "           1       0.81      0.84      0.82     16962\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.84      0.84      0.84     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n",
      "f1_score:\n",
      "0.9149160734093471\n",
      "precision_score:\n",
      "0.9151128847325316\n",
      "accuracy_score:\n",
      "0.9169\n",
      "Confusion Matrix:\n",
      "[[5348  408]\n",
      " [ 423 3821]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      5756\n",
      "           1       0.90      0.90      0.90      4244\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.91      0.91     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "f1_score:\n",
      "0.8326633854638791\n",
      "precision_score:\n",
      "0.8329264495920172\n",
      "accuracy_score:\n",
      "0.83665\n",
      "Confusion Matrix:\n",
      "[[19820  3218]\n",
      " [ 3316 13646]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     23038\n",
      "           1       0.81      0.80      0.81     16962\n",
      "\n",
      "    accuracy                           0.84     40000\n",
      "   macro avg       0.83      0.83      0.83     40000\n",
      "weighted avg       0.84      0.84      0.84     40000\n",
      "\n",
      "Elapsed Time: 13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7775461306444342\n",
      "precision_score:\n",
      "0.77675479980905\n",
      "accuracy_score:\n",
      "0.7819\n",
      "Confusion Matrix:\n",
      "[[4609 1147]\n",
      " [1034 3210]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81      5756\n",
      "           1       0.74      0.76      0.75      4244\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "f1_score:\n",
      "0.773312952472571\n",
      "precision_score:\n",
      "0.7722158040080058\n",
      "accuracy_score:\n",
      "0.77735\n",
      "Confusion Matrix:\n",
      "[[18216  4822]\n",
      " [ 4084 12878]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     23038\n",
      "           1       0.73      0.76      0.74     16962\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "Elapsed Time: 5 seconds\n",
      "0.8 2\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[5775    0]\n",
      " [   0 4225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5775\n",
      "           1       1.00      1.00      1.00      4225\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n",
      "f1_score:\n",
      "0.8478013699306683\n",
      "precision_score:\n",
      "0.8474767622174875\n",
      "accuracy_score:\n",
      "0.851125\n",
      "Confusion Matrix:\n",
      "[[19978  3041]\n",
      " [ 2914 14067]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     23019\n",
      "           1       0.82      0.83      0.83     16981\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.85      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n",
      "f1_score:\n",
      "0.9036843252843904\n",
      "precision_score:\n",
      "0.9019810005488103\n",
      "accuracy_score:\n",
      "0.9055\n",
      "Confusion Matrix:\n",
      "[[5214  561]\n",
      " [ 384 3841]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92      5775\n",
      "           1       0.87      0.91      0.89      4225\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.90      0.91      0.90     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n",
      "f1_score:\n",
      "0.8274693157467835\n",
      "precision_score:\n",
      "0.8260235233939676\n",
      "accuracy_score:\n",
      "0.830375\n",
      "Confusion Matrix:\n",
      "[[19203  3816]\n",
      " [ 2969 14012]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85     23019\n",
      "           1       0.79      0.83      0.81     16981\n",
      "\n",
      "    accuracy                           0.83     40000\n",
      "   macro avg       0.83      0.83      0.83     40000\n",
      "weighted avg       0.83      0.83      0.83     40000\n",
      "\n",
      "Elapsed Time: 12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7777865957700092\n",
      "precision_score:\n",
      "0.776472837516729\n",
      "accuracy_score:\n",
      "0.7816\n",
      "Confusion Matrix:\n",
      "[[4563 1212]\n",
      " [ 972 3253]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81      5775\n",
      "           1       0.73      0.77      0.75      4225\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "f1_score:\n",
      "0.7711997833875803\n",
      "precision_score:\n",
      "0.7699138720675713\n",
      "accuracy_score:\n",
      "0.7745\n",
      "Confusion Matrix:\n",
      "[[17892  5127]\n",
      " [ 3893 13088]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80     23019\n",
      "           1       0.72      0.77      0.74     16981\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.77      0.78     40000\n",
      "\n",
      "Elapsed Time: 2 seconds\n",
      "Elapsed Time: 547 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = np.empty([9, 7])\n",
    "mlp = np.empty([9, 7])\n",
    "svm = np.empty([9, 7])\n",
    "\n",
    "rf_train = np.empty([9, 7])\n",
    "mlp_train = np.empty([9, 7])\n",
    "svm_train = np.empty([9, 7])\n",
    "\n",
    "\n",
    "test_sizes = [0.2,0.5,0.8]\n",
    "j = 0\n",
    "start1 = time.time()\n",
    "for size in test_sizes:\n",
    "    for i in np.arange(3):\n",
    "        print(size, i)\n",
    "        x_train1, x_test1, y_train1, y_test1 = train_test_split(X_train, Y_train, test_size=size)\n",
    "        rf_time = time.time()\n",
    "        rf_f1, rf_apr, rf_acc, rf_auc, t_rf_f1, t_rf_apr, t_rf_acc, t_rf_auc = RFfunc(x_train1, x_test1, y_train1, y_test1)\n",
    "        rf_time = time.time() - rf_time\n",
    "        \n",
    "        mlp_time = time.time()\n",
    "        mlp_f1, mlp_apr, mlp_acc, mlp_auc, t_mlp_f1, t_mlp_apr, t_mlp_acc, t_mlp_auc = MLPfunc(x_train1, x_test1, y_train1, y_test1)\n",
    "        mlp_time = time.time() - mlp_time\n",
    "        \n",
    "        svm_time = time.time()\n",
    "        svm_f1, svm_apr, svm_acc, svm_auc, t_svm_f1, t_svm_apr, t_svm_acc, t_svm_auc = LSVMfunc(x_train1, x_test1, y_train1, y_test1)\n",
    "        svm_time = time.time() - svm_time\n",
    "        \n",
    "        \n",
    "        rf[j] = [rf_f1, rf_apr, rf_acc, rf_auc, i, size, rf_time]\n",
    "        mlp[j] = [mlp_f1, mlp_apr, mlp_acc, mlp_auc, i, size, mlp_time]\n",
    "        svm[j] = [svm_f1, svm_apr, svm_acc, svm_auc, i, size, svm_time]\n",
    "        \n",
    "        rf_train[j] = [t_rf_f1, t_rf_apr, t_rf_acc, t_rf_auc, i, size, rf_time]\n",
    "        mlp_train[j] = [t_mlp_f1, t_mlp_apr, t_mlp_acc, t_mlp_auc, i, size, mlp_time]\n",
    "        svm_train[j] = [t_svm_f1, t_svm_apr, t_svm_acc, t_svm_auc, i, size, svm_time]\n",
    "        \n",
    "        \n",
    "        j = j + 1\n",
    "now1 = time.time()\n",
    "print('Elapsed Time: ' + str(int(now1-start1)) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.DataFrame(rf, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "mlp_df = pd.DataFrame(mlp, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "svm_df = pd.DataFrame(svm, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "rf_df['avg'] = round(rf_df.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "mlp_df['avg'] = round(mlp_df.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "svm_df['avg'] = round(svm_df.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "mlp_mean = round(mlp_df.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "rf_mean = round(rf_df.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "svm_mean = round(svm_df.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "rf_mean['avg'] = round(rf_mean.drop('time',axis=1).mean(axis=1),3).values\n",
    "mlp_mean['avg'] = round(mlp_mean.drop('time',axis=1).mean(axis=1),3).values\n",
    "svm_mean['avg'] = round(svm_mean.drop('time',axis=1).mean(axis=1),3).values\n",
    "\n",
    "rf_mean['f1_std'] = round(rf_df.groupby('test_size').std(),3)['f1'].values\n",
    "mlp_mean['f1_std'] = round(mlp_df.groupby('test_size').std(),3)['f1'].values\n",
    "svm_mean['f1_std'] = round(svm_df.groupby('test_size').std(),3)['f1'].values\n",
    "\n",
    "rf_mean['apr_std'] = round(rf_df.groupby('test_size').std(),3)['apr'].values\n",
    "mlp_mean['apr_std'] = round(mlp_df.groupby('test_size').std(),3)['apr'].values\n",
    "svm_mean['apr_std'] = round(svm_df.groupby('test_size').std(),3)['apr'].values\n",
    "\n",
    "rf_mean['acc_std'] = round(rf_df.groupby('test_size').std(),3)['acc'].values\n",
    "mlp_mean['acc_std'] = round(mlp_df.groupby('test_size').std(),3)['acc'].values\n",
    "svm_mean['acc_std'] = round(svm_df.groupby('test_size').std(),3)['acc'].values\n",
    "\n",
    "rf_mean['auc_std'] = round(rf_df.groupby('test_size').std(),3)['auc'].values\n",
    "mlp_mean['auc_std'] = round(mlp_df.groupby('test_size').std(),3)['auc'].values\n",
    "svm_mean['auc_std'] = round(svm_df.groupby('test_size').std(),3)['auc'].values\n",
    "\n",
    "\n",
    "rf_mean['avg_std'] = round(rf_df.groupby('test_size').std(),3)['avg'].values\n",
    "mlp_mean['avg_std'] = round(mlp_df.groupby('test_size').std(),3)['avg'].values\n",
    "svm_mean['avg_std'] = round(svm_df.groupby('test_size').std(),3)['avg'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df_train = pd.DataFrame(rf_train, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "mlp_df_train = pd.DataFrame(mlp_train, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "svm_df_train = pd.DataFrame(svm_train, columns = ['f1', 'apr', 'acc', 'auc', 'trial', 'test_size','time'])\n",
    "\n",
    "\n",
    "rf_df_train['avg'] = round(rf_df_train.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "mlp_df_train['avg'] = round(mlp_df_train.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "svm_df_train['avg'] = round(svm_df_train.drop(['trial','test_size','time'],axis=1).mean(axis=1),3).values\n",
    "\n",
    "\n",
    "mlp_mean_train = round(mlp_df_train.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "rf_mean_train = round(rf_df_train.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "svm_mean_train = round(svm_df_train.groupby('test_size').mean(),3).drop('trial', axis = 1)\n",
    "\n",
    "rf_mean_train['avg'] = round(rf_mean_train.drop('time',axis=1).mean(axis=1),3).values\n",
    "mlp_mean_train['avg'] = round(mlp_mean_train.drop('time',axis=1).mean(axis=1),3).values\n",
    "svm_mean_train['avg'] = round(svm_mean_train.drop('time',axis=1).mean(axis=1),3).values\n",
    "\n",
    "rf_mean_train['f1_std'] = round(rf_df_train.groupby('test_size').std(),3)['f1'].values\n",
    "mlp_mean_train['f1_std'] = round(mlp_df_train.groupby('test_size').std(),3)['f1'].values\n",
    "svm_mean_train['f1_std'] = round(svm_df_train.groupby('test_size').std(),3)['f1'].values\n",
    "\n",
    "rf_mean_train['apr_std'] = round(rf_df_train.groupby('test_size').std(),3)['apr'].values\n",
    "mlp_mean_train['apr_std'] = round(mlp_df_train.groupby('test_size').std(),3)['apr'].values\n",
    "svm_mean_train['apr_std'] = round(svm_df_train.groupby('test_size').std(),3)['apr'].values\n",
    "\n",
    "rf_mean_train['acc_std'] = round(rf_df_train.groupby('test_size').std(),3)['acc'].values\n",
    "mlp_mean_train['acc_std'] = round(mlp_df_train.groupby('test_size').std(),3)['acc'].values\n",
    "svm_mean_train['acc_std'] = round(svm_df_train.groupby('test_size').std(),3)['acc'].values\n",
    "\n",
    "rf_mean_train['auc_std'] = round(rf_df_train.groupby('test_size').std(),3)['auc'].values\n",
    "mlp_mean_train['auc_std'] = round(mlp_df_train.groupby('test_size').std(),3)['auc'].values\n",
    "svm_mean_train['auc_std'] = round(svm_df_train.groupby('test_size').std(),3)['auc'].values\n",
    "\n",
    "\n",
    "rf_mean_train['avg_std'] = round(rf_df_train.groupby('test_size').std(),3)['avg'].values\n",
    "mlp_mean_train['avg_std'] = round(mlp_df_train.groupby('test_size').std(),3)['avg'].values\n",
    "svm_mean_train['avg_std'] = round(svm_df_train.groupby('test_size').std(),3)['avg'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.902</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.967</td>\n",
       "      <td>2.268</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.517</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.847</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc   time    avg  f1_std  apr_std  acc_std  \\\n",
       "test_size                                                                       \n",
       "0.2        0.902  0.901  0.903  0.967  2.268  0.918   0.001    0.002    0.002   \n",
       "0.5        0.882  0.881  0.885  0.954  1.517  0.901   0.003    0.002    0.003   \n",
       "0.8        0.847  0.846  0.850  0.927  0.892  0.867   0.003    0.004    0.003   \n",
       "\n",
       "           auc_std  avg_std  \n",
       "test_size                    \n",
       "0.2          0.001    0.001  \n",
       "0.5          0.002    0.002  \n",
       "0.8          0.002    0.003  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.268</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1  apr  acc  auc   time  avg  f1_std  apr_std  acc_std  auc_std  \\\n",
       "test_size                                                                      \n",
       "0.2        1.0  1.0  1.0  1.0  2.268  1.0     0.0      0.0      0.0      0.0   \n",
       "0.5        1.0  1.0  1.0  1.0  1.517  1.0     0.0      0.0      0.0      0.0   \n",
       "0.8        1.0  1.0  1.0  1.0  0.892  1.0     0.0      0.0      0.0      0.0   \n",
       "\n",
       "           avg_std  \n",
       "test_size           \n",
       "0.2            0.0  \n",
       "0.5            0.0  \n",
       "0.8            0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.872</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.945</td>\n",
       "      <td>41.021</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.859</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.938</td>\n",
       "      <td>27.735</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.830</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.911</td>\n",
       "      <td>13.463</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc    time    avg  f1_std  apr_std  \\\n",
       "test_size                                                               \n",
       "0.2        0.872  0.872  0.875  0.945  41.021  0.891   0.006    0.007   \n",
       "0.5        0.859  0.858  0.862  0.938  27.735  0.879   0.006    0.007   \n",
       "0.8        0.830  0.830  0.833  0.911  13.463  0.851   0.003    0.004   \n",
       "\n",
       "           acc_std  auc_std  avg_std  \n",
       "test_size                             \n",
       "0.2          0.006    0.002    0.005  \n",
       "0.5          0.007    0.003    0.006  \n",
       "0.8          0.004    0.002    0.003  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.897</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.964</td>\n",
       "      <td>41.021</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.895</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.965</td>\n",
       "      <td>27.735</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.909</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.971</td>\n",
       "      <td>13.463</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc    time    avg  f1_std  apr_std  \\\n",
       "test_size                                                               \n",
       "0.2        0.897  0.897  0.899  0.964  41.021  0.914   0.002    0.002   \n",
       "0.5        0.895  0.894  0.897  0.965  27.735  0.913   0.006    0.006   \n",
       "0.8        0.909  0.908  0.911  0.971  13.463  0.925   0.006    0.007   \n",
       "\n",
       "           acc_std  auc_std  avg_std  \n",
       "test_size                             \n",
       "0.2          0.002    0.001    0.002  \n",
       "0.5          0.006    0.002    0.005  \n",
       "0.8          0.006    0.002    0.005  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.847</td>\n",
       "      <td>66.683</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.848</td>\n",
       "      <td>24.880</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.847</td>\n",
       "      <td>3.855</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc    time    avg  f1_std  apr_std  \\\n",
       "test_size                                                               \n",
       "0.2        0.773  0.772  0.776  0.847  66.683  0.792   0.005    0.005   \n",
       "0.5        0.770  0.769  0.773  0.848  24.880  0.790   0.001    0.001   \n",
       "0.8        0.772  0.771  0.776  0.847   3.855  0.791   0.001    0.001   \n",
       "\n",
       "           acc_std  auc_std  avg_std  \n",
       "test_size                             \n",
       "0.2          0.005    0.003    0.004  \n",
       "0.5          0.001    0.002    0.001  \n",
       "0.8          0.002    0.001    0.001  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.849</td>\n",
       "      <td>66.683</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.849</td>\n",
       "      <td>24.880</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.850</td>\n",
       "      <td>3.855</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc    time    avg  f1_std  apr_std  \\\n",
       "test_size                                                               \n",
       "0.2        0.773  0.772  0.777  0.849  66.683  0.793   0.002    0.002   \n",
       "0.5        0.775  0.773  0.778  0.849  24.880  0.794   0.003    0.003   \n",
       "0.8        0.777  0.776  0.782  0.850   3.855  0.796   0.001    0.001   \n",
       "\n",
       "           acc_std  auc_std  avg_std  \n",
       "test_size                             \n",
       "0.2          0.002    0.001    0.002  \n",
       "0.5          0.003    0.002    0.002  \n",
       "0.8          0.001    0.003    0.002  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.291381</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.348097</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.903</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.967</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.163078</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.881</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.508186</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.885</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.522512</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.953</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.519134</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.849</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.888031</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.894019</td>\n",
       "      <td>0.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.848</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.895003</td>\n",
       "      <td>0.868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size      time    avg\n",
       "0  0.901  0.901  0.903  0.966    0.0        0.2  2.291381  0.918\n",
       "1  0.901  0.899  0.902  0.967    1.0        0.2  2.348097  0.917\n",
       "2  0.903  0.902  0.905  0.967    2.0        0.2  2.163078  0.919\n",
       "3  0.881  0.880  0.883  0.954    0.0        0.5  1.508186  0.900\n",
       "4  0.885  0.884  0.888  0.956    1.0        0.5  1.522512  0.903\n",
       "5  0.880  0.880  0.883  0.953    2.0        0.5  1.519134  0.899\n",
       "6  0.849  0.849  0.852  0.929    0.0        0.8  0.888031  0.870\n",
       "7  0.843  0.842  0.846  0.926    1.0        0.8  0.894019  0.864\n",
       "8  0.848  0.847  0.851  0.927    2.0        0.8  0.895003  0.868"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.291381</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.348097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.163078</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.508186</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.522512</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.519134</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.888031</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.894019</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.895003</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1  apr  acc  auc  trial  test_size      time  avg\n",
       "0  1.0  1.0  1.0  1.0    0.0        0.2  2.291381  1.0\n",
       "1  1.0  1.0  1.0  1.0    1.0        0.2  2.348097  1.0\n",
       "2  1.0  1.0  1.0  1.0    2.0        0.2  2.163078  1.0\n",
       "3  1.0  1.0  1.0  1.0    0.0        0.5  1.508186  1.0\n",
       "4  1.0  1.0  1.0  1.0    1.0        0.5  1.522512  1.0\n",
       "5  1.0  1.0  1.0  1.0    2.0        0.5  1.519134  1.0\n",
       "6  1.0  1.0  1.0  1.0    0.0        0.8  0.888031  1.0\n",
       "7  1.0  1.0  1.0  1.0    1.0        0.8  0.894019  1.0\n",
       "8  1.0  1.0  1.0  1.0    2.0        0.8  0.895003  1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.874</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>38.643981</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.212081</td>\n",
       "      <td>0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.877</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.947</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>43.206840</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.511510</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.792999</td>\n",
       "      <td>0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.854</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.936</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.900999</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.830</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.926970</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.833</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>13.251015</td>\n",
       "      <td>0.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.827</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.911</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>12.210998</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size       time    avg\n",
       "0  0.874  0.873  0.876  0.945    0.0        0.2  38.643981  0.892\n",
       "1  0.866  0.865  0.869  0.944    1.0        0.2  41.212081  0.886\n",
       "2  0.877  0.878  0.880  0.947    2.0        0.2  43.206840  0.896\n",
       "3  0.857  0.856  0.860  0.936    0.0        0.5  25.511510  0.877\n",
       "4  0.866  0.865  0.869  0.942    1.0        0.5  29.792999  0.886\n",
       "5  0.854  0.852  0.856  0.936    2.0        0.5  27.900999  0.874\n",
       "6  0.830  0.830  0.833  0.912    0.0        0.8  14.926970  0.851\n",
       "7  0.833  0.833  0.837  0.909    1.0        0.8  13.251015  0.853\n",
       "8  0.827  0.826  0.830  0.911    2.0        0.8  12.210998  0.848"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>38.643981</td>\n",
       "      <td>0.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.895</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>41.212081</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.965</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>43.206840</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.891</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.511510</td>\n",
       "      <td>0.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.792999</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.892</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.964</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.900999</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.907</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.926970</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.915</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>13.251015</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.904</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.969</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>12.210998</td>\n",
       "      <td>0.920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size       time    avg\n",
       "0  0.897  0.897  0.899  0.964    0.0        0.2  38.643981  0.914\n",
       "1  0.895  0.895  0.897  0.963    1.0        0.2  41.212081  0.913\n",
       "2  0.899  0.899  0.901  0.965    2.0        0.2  43.206840  0.916\n",
       "3  0.891  0.890  0.893  0.963    0.0        0.5  25.511510  0.909\n",
       "4  0.901  0.901  0.903  0.967    1.0        0.5  29.792999  0.918\n",
       "5  0.892  0.890  0.894  0.964    2.0        0.5  27.900999  0.910\n",
       "6  0.907  0.908  0.910  0.970    0.0        0.8  14.926970  0.924\n",
       "7  0.915  0.915  0.917  0.973    1.0        0.8  13.251015  0.930\n",
       "8  0.904  0.902  0.906  0.969    2.0        0.8  12.210998  0.920"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>67.270015</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>69.916050</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.850</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.863008</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.764014</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.183103</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.769</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.846</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.693999</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.011000</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.622999</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.847</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.931000</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size       time    avg\n",
       "0  0.768  0.767  0.772  0.844    0.0        0.2  67.270015  0.788\n",
       "1  0.778  0.776  0.781  0.848    1.0        0.2  69.916050  0.796\n",
       "2  0.773  0.772  0.776  0.850    2.0        0.2  62.863008  0.793\n",
       "3  0.770  0.769  0.774  0.848    0.0        0.5  18.764014  0.790\n",
       "4  0.771  0.770  0.773  0.849    1.0        0.5  29.183103  0.791\n",
       "5  0.769  0.768  0.772  0.846    2.0        0.5  26.693999  0.789\n",
       "6  0.773  0.772  0.776  0.847    0.0        0.8   3.011000  0.792\n",
       "7  0.773  0.772  0.777  0.846    1.0        0.8   5.622999  0.792\n",
       "8  0.771  0.770  0.774  0.847    2.0        0.8   2.931000  0.790"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>67.270015</td>\n",
       "      <td>0.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>69.916050</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.848</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>62.863008</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.764014</td>\n",
       "      <td>0.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.183103</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.851</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>26.693999</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.776</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.011000</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.853</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.622999</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.850</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.931000</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size       time    avg\n",
       "0  0.775  0.773  0.778  0.850    0.0        0.2  67.270015  0.794\n",
       "1  0.771  0.770  0.774  0.849    1.0        0.2  69.916050  0.791\n",
       "2  0.774  0.773  0.778  0.848    2.0        0.2  62.863008  0.793\n",
       "3  0.774  0.773  0.778  0.849    0.0        0.5  18.764014  0.794\n",
       "4  0.773  0.771  0.776  0.848    1.0        0.5  29.183103  0.792\n",
       "5  0.778  0.776  0.781  0.851    2.0        0.5  26.693999  0.796\n",
       "6  0.776  0.775  0.781  0.848    0.0        0.8   3.011000  0.795\n",
       "7  0.778  0.777  0.782  0.853    1.0        0.8   5.622999  0.798\n",
       "8  0.778  0.776  0.782  0.850    2.0        0.8   2.931000  0.797"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23075     0]\n",
      " [    0 16925]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23075\n",
      "           1       1.00      1.00      1.00     16925\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.8941796916529883\n",
      "precision_score:\n",
      "0.8922621960360357\n",
      "accuracy_score:\n",
      "0.8955\n",
      "Confusion Matrix:\n",
      "[[5036  683]\n",
      " [ 362 3919]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91      5719\n",
      "           1       0.85      0.92      0.88      4281\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.89      0.90      0.89     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8968642804464846\n",
      "precision_score:\n",
      "0.8972623887456321\n",
      "accuracy_score:\n",
      "0.8994\n",
      "Confusion Matrix:\n",
      "[[21124  1951]\n",
      " [ 2073 14852]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91     23075\n",
      "           1       0.88      0.88      0.88     16925\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.90      0.90      0.90     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8731347079738425\n",
      "precision_score:\n",
      "0.8726856968246288\n",
      "accuracy_score:\n",
      "0.8756\n",
      "Confusion Matrix:\n",
      "[[5075  644]\n",
      " [ 600 3681]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5719\n",
      "           1       0.85      0.86      0.86      4281\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Elapsed Time: 47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7743989136925039\n",
      "precision_score:\n",
      "0.7731142091559443\n",
      "accuracy_score:\n",
      "0.778175\n",
      "Confusion Matrix:\n",
      "[[18151  4924]\n",
      " [ 3949 12976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     23075\n",
      "           1       0.72      0.77      0.75     16925\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7672342328955688\n",
      "precision_score:\n",
      "0.7679478158690102\n",
      "accuracy_score:\n",
      "0.7686\n",
      "Confusion Matrix:\n",
      "[[4226 1493]\n",
      " [ 821 3460]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79      5719\n",
      "           1       0.70      0.81      0.75      4281\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 65 seconds\n",
      "Elapsed Time: 114 seconds\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23016     0]\n",
      " [    0 16984]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23016\n",
      "           1       1.00      1.00      1.00     16984\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.8996953306830883\n",
      "precision_score:\n",
      "0.8979951298701299\n",
      "accuracy_score:\n",
      "0.9016\n",
      "Confusion Matrix:\n",
      "[[5197  581]\n",
      " [ 403 3819]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91      5778\n",
      "           1       0.87      0.90      0.89      4222\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8991368889056092\n",
      "precision_score:\n",
      "0.8997525524599246\n",
      "accuracy_score:\n",
      "0.901575\n",
      "Confusion Matrix:\n",
      "[[21141  1875]\n",
      " [ 2062 14922]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91     23016\n",
      "           1       0.89      0.88      0.88     16984\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.90      0.90      0.90     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8807351387430935\n",
      "precision_score:\n",
      "0.8817160044866081\n",
      "accuracy_score:\n",
      "0.8839\n",
      "Confusion Matrix:\n",
      "[[5234  544]\n",
      " [ 617 3605]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      5778\n",
      "           1       0.87      0.85      0.86      4222\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Elapsed Time: 44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7711539242710364\n",
      "precision_score:\n",
      "0.7698797136108464\n",
      "accuracy_score:\n",
      "0.7743\n",
      "Confusion Matrix:\n",
      "[[17831  5185]\n",
      " [ 3843 13141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80     23016\n",
      "           1       0.72      0.77      0.74     16984\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.77      0.77      0.77     40000\n",
      "weighted avg       0.78      0.77      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7814681641140466\n",
      "precision_score:\n",
      "0.781522540752347\n",
      "accuracy_score:\n",
      "0.7868\n",
      "Confusion Matrix:\n",
      "[[4715 1063]\n",
      " [1069 3153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      5778\n",
      "           1       0.75      0.75      0.75      4222\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.79      0.79      0.79     10000\n",
      "\n",
      "Elapsed Time: 64 seconds\n",
      "Elapsed Time: 111 seconds\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23015     0]\n",
      " [    0 16985]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23015\n",
      "           1       1.00      1.00      1.00     16985\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.9038159352903113\n",
      "precision_score:\n",
      "0.903602003263313\n",
      "accuracy_score:\n",
      "0.9061\n",
      "Confusion Matrix:\n",
      "[[5301  478]\n",
      " [ 461 3760]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      5779\n",
      "           1       0.89      0.89      0.89      4221\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n",
      "f1_score:\n",
      "0.8901413213195293\n",
      "precision_score:\n",
      "0.8886422040491808\n",
      "accuracy_score:\n",
      "0.892125\n",
      "Confusion Matrix:\n",
      "[[20530  2485]\n",
      " [ 1830 15155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90     23015\n",
      "           1       0.86      0.89      0.88     16985\n",
      "\n",
      "    accuracy                           0.89     40000\n",
      "   macro avg       0.89      0.89      0.89     40000\n",
      "weighted avg       0.89      0.89      0.89     40000\n",
      "\n",
      "f1_score:\n",
      "0.8690685583378881\n",
      "precision_score:\n",
      "0.869224694346995\n",
      "accuracy_score:\n",
      "0.8723\n",
      "Confusion Matrix:\n",
      "[[5147  632]\n",
      " [ 645 3576]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5779\n",
      "           1       0.85      0.85      0.85      4221\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7724798808078722\n",
      "precision_score:\n",
      "0.7711905124846814\n",
      "accuracy_score:\n",
      "0.7758\n",
      "Confusion Matrix:\n",
      "[[17932  5083]\n",
      " [ 3885 13100]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80     23015\n",
      "           1       0.72      0.77      0.74     16985\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7772585592523449\n",
      "precision_score:\n",
      "0.7760124901448193\n",
      "accuracy_score:\n",
      "0.7813\n",
      "Confusion Matrix:\n",
      "[[4580 1199]\n",
      " [ 988 3233]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81      5779\n",
      "           1       0.73      0.77      0.75      4221\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.78     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n",
      "Elapsed Time: 64 seconds\n",
      "Elapsed Time: 103 seconds\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[22976     0]\n",
      " [    0 17024]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     22976\n",
      "           1       1.00      1.00      1.00     17024\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.8979933614571234\n",
      "precision_score:\n",
      "0.8968366731638446\n",
      "accuracy_score:\n",
      "0.9004\n",
      "Confusion Matrix:\n",
      "[[5270  548]\n",
      " [ 448 3734]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91      5818\n",
      "           1       0.87      0.89      0.88      4182\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8987243144232624\n",
      "precision_score:\n",
      "0.898757821687929\n",
      "accuracy_score:\n",
      "0.900975\n",
      "Confusion Matrix:\n",
      "[[21001  1975]\n",
      " [ 1986 15038]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91     22976\n",
      "           1       0.88      0.88      0.88     17024\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.90      0.90      0.90     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8714967545654826\n",
      "precision_score:\n",
      "0.870588901012924\n",
      "accuracy_score:\n",
      "0.8746\n",
      "Confusion Matrix:\n",
      "[[5150  668]\n",
      " [ 586 3596]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      5818\n",
      "           1       0.84      0.86      0.85      4182\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7752288487500183\n",
      "precision_score:\n",
      "0.7740803057466192\n",
      "accuracy_score:\n",
      "0.7789\n",
      "Confusion Matrix:\n",
      "[[18134  4842]\n",
      " [ 4002 13022]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     22976\n",
      "           1       0.73      0.76      0.75     17024\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.78     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7692820358155614\n",
      "precision_score:\n",
      "0.7679737452977656\n",
      "accuracy_score:\n",
      "0.7725\n",
      "Confusion Matrix:\n",
      "[[4453 1365]\n",
      " [ 910 3272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80      5818\n",
      "           1       0.71      0.78      0.74      4182\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 39 seconds\n",
      "Elapsed Time: 90 seconds\n",
      "f1_score:\n",
      "1.0\n",
      "precision_score:\n",
      "1.0\n",
      "accuracy_score:\n",
      "1.0\n",
      "Confusion Matrix:\n",
      "[[23094     0]\n",
      " [    0 16906]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     23094\n",
      "           1       1.00      1.00      1.00     16906\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "f1_score:\n",
      "0.8944929122005388\n",
      "precision_score:\n",
      "0.8933397792939404\n",
      "accuracy_score:\n",
      "0.8962\n",
      "Confusion Matrix:\n",
      "[[5117  583]\n",
      " [ 455 3845]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      5700\n",
      "           1       0.87      0.89      0.88      4300\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.89      0.90      0.89     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "f1_score:\n",
      "0.8998663610040707\n",
      "precision_score:\n",
      "0.898980372446309\n",
      "accuracy_score:\n",
      "0.902025\n",
      "Confusion Matrix:\n",
      "[[20977  2117]\n",
      " [ 1802 15104]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91     23094\n",
      "           1       0.88      0.89      0.89     16906\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.90      0.90      0.90     40000\n",
      "weighted avg       0.90      0.90      0.90     40000\n",
      "\n",
      "f1_score:\n",
      "0.8711533516091703\n",
      "precision_score:\n",
      "0.8709283939131021\n",
      "accuracy_score:\n",
      "0.8736\n",
      "Confusion Matrix:\n",
      "[[5057  643]\n",
      " [ 621 3679]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5700\n",
      "           1       0.85      0.86      0.85      4300\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "Elapsed Time: 38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7746762259730647\n",
      "precision_score:\n",
      "0.7734348077478777\n",
      "accuracy_score:\n",
      "0.77865\n",
      "Confusion Matrix:\n",
      "[[18229  4865]\n",
      " [ 3989 12917]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80     23094\n",
      "           1       0.73      0.76      0.74     16906\n",
      "\n",
      "    accuracy                           0.78     40000\n",
      "   macro avg       0.77      0.78      0.77     40000\n",
      "weighted avg       0.78      0.78      0.78     40000\n",
      "\n",
      "f1_score:\n",
      "0.7714749206983564\n",
      "precision_score:\n",
      "0.7703738692095286\n",
      "accuracy_score:\n",
      "0.7742\n",
      "Confusion Matrix:\n",
      "[[4417 1283]\n",
      " [ 975 3325]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80      5700\n",
      "           1       0.72      0.77      0.75      4300\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.78      0.77      0.77     10000\n",
      "\n",
      "Elapsed Time: 62 seconds\n",
      "Elapsed Time: 103 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "rf = np.empty([5, 4])\n",
    "mlp = np.empty([5, 4])\n",
    "svm = np.empty([5, 4])\n",
    "\n",
    "rf_train = np.empty([5, 4])\n",
    "mlp_train = np.empty([5, 4])\n",
    "svm_train = np.empty([5, 4])\n",
    "\n",
    "j = 0\n",
    "for train_index, test_index in kf.split(X_train, Y_train):\n",
    "    x_train = X_train[train_index]\n",
    "    y_train = Y_train[train_index]\n",
    "    \n",
    "    x_test = X_train[test_index]\n",
    "    y_test = Y_train[test_index]\n",
    "        \n",
    "    start = time.time()\n",
    "    rf_f1, rf_apr, rf_acc, rf_auc, t_rf_f1, t_rf_apr, t_rf_acc, t_rf_auc = RFfunc(x_train, x_test, y_train, y_test)\n",
    "    mlp_f1, mlp_apr, mlp_acc, mlp_auc, t_mlp_f1, t_mlp_apr, t_mlp_acc, t_mlp_auc = MLPfunc(x_train, x_test, y_train, y_test)\n",
    "    svm_f1, svm_apr, svm_acc, svm_auc, t_svm_f1, t_svm_apr, t_svm_acc, t_svm_auc = LSVMfunc(x_train, x_test, y_train, y_test)\n",
    " \n",
    "\n",
    "    rf[j] = [rf_f1, rf_apr, rf_acc, rf_auc]\n",
    "    mlp[j] = [mlp_f1, mlp_apr, mlp_acc, mlp_auc]\n",
    "    svm[j] = [svm_f1, svm_apr, svm_acc, svm_auc]\n",
    "    \n",
    "    rf_train[j] = [t_rf_f1, t_rf_apr, t_rf_acc, t_rf_auc]\n",
    "    mlp_train[j] = [t_mlp_f1, t_mlp_apr, t_mlp_acc, t_mlp_auc]\n",
    "    svm_train[j] = [t_svm_f1, t_svm_apr, t_svm_acc, t_svm_auc]\n",
    "\n",
    "    now = time.time()\n",
    "    print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_stats = pd.DataFrame(rf, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "mlp_stats = pd.DataFrame(mlp, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "svm_stats = pd.DataFrame(svm, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "rf_stats['avg'] = rf_stats.mean(axis=1).values\n",
    "mlp_stats['avg'] = mlp_stats.mean(axis=1).values\n",
    "svm_stats['avg'] = svm_stats.mean(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_stats_train = pd.DataFrame(rf_train, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "mlp_stats_train = pd.DataFrame(mlp_train, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "svm_stats_train = pd.DataFrame(svm_train, columns = ['f1', 'apr', 'acc', 'auc'])\n",
    "rf_stats_train['avg'] = rf_stats_train.mean(axis=1).values\n",
    "mlp_stats_train['avg'] = mlp_stats_train.mean(axis=1).values\n",
    "svm_stats_train['avg'] = svm_stats_train.mean(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.91175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.91675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.904</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.92050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.898</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.91475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.894</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.91175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.894  0.892  0.896  0.965  0.91175\n",
       "1  0.900  0.898  0.902  0.967  0.91675\n",
       "2  0.904  0.904  0.906  0.968  0.92050\n",
       "3  0.898  0.897  0.900  0.964  0.91475\n",
       "4  0.894  0.893  0.896  0.964  0.91175"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.898\n",
       "apr    0.897\n",
       "acc    0.900\n",
       "auc    0.966\n",
       "avg    0.915\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rf_stats.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.004\n",
       "apr    0.005\n",
       "acc    0.004\n",
       "auc    0.002\n",
       "avg    0.004\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rf_stats.std(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1  apr  acc  auc  avg\n",
       "0  1.0  1.0  1.0  1.0  1.0\n",
       "1  1.0  1.0  1.0  1.0  1.0\n",
       "2  1.0  1.0  1.0  1.0  1.0\n",
       "3  1.0  1.0  1.0  1.0  1.0\n",
       "4  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_stats_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     1.0\n",
       "apr    1.0\n",
       "acc    1.0\n",
       "auc    1.0\n",
       "avg    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rf_stats_train.mean(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.0\n",
       "apr    0.0\n",
       "acc    0.0\n",
       "auc    0.0\n",
       "avg    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rf_stats_train.std(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.89250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.881</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.89950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.869</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.88825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.89050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.89025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.873  0.873  0.876  0.948  0.89250\n",
       "1  0.881  0.882  0.884  0.951  0.89950\n",
       "2  0.869  0.869  0.872  0.943  0.88825\n",
       "3  0.871  0.871  0.875  0.945  0.89050\n",
       "4  0.871  0.871  0.874  0.945  0.89025"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.873\n",
       "apr    0.873\n",
       "acc    0.876\n",
       "auc    0.946\n",
       "avg    0.892\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mlp_stats.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.005\n",
       "apr    0.005\n",
       "acc    0.005\n",
       "auc    0.003\n",
       "avg    0.004\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mlp_stats.std(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.91450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.91650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.890</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.90825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.91625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.91700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.897  0.897  0.899  0.965  0.91450\n",
       "1  0.899  0.900  0.902  0.965  0.91650\n",
       "2  0.890  0.889  0.892  0.962  0.90825\n",
       "3  0.899  0.899  0.901  0.966  0.91625\n",
       "4  0.900  0.899  0.902  0.967  0.91700"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_stats_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.897\n",
       "apr    0.897\n",
       "acc    0.899\n",
       "auc    0.965\n",
       "avg    0.914\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mlp_stats_train.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.004\n",
       "apr    0.004\n",
       "acc    0.004\n",
       "auc    0.002\n",
       "avg    0.004\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mlp_stats_train.std(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.767</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.78775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.80100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.79650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.769</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.78800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.79000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.767  0.768  0.769  0.847  0.78775\n",
       "1  0.781  0.782  0.787  0.854  0.80100\n",
       "2  0.777  0.776  0.781  0.852  0.79650\n",
       "3  0.769  0.768  0.772  0.843  0.78800\n",
       "4  0.771  0.770  0.774  0.845  0.79000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.773\n",
       "apr    0.773\n",
       "acc    0.777\n",
       "auc    0.848\n",
       "avg    0.793\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(svm_stats.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.006\n",
       "apr    0.006\n",
       "acc    0.007\n",
       "auc    0.005\n",
       "avg    0.006\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(svm_stats.std(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.79350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.79050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.79175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.79450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.79400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc      avg\n",
       "0  0.774  0.773  0.778  0.849  0.79350\n",
       "1  0.771  0.770  0.774  0.847  0.79050\n",
       "2  0.772  0.771  0.776  0.848  0.79175\n",
       "3  0.775  0.774  0.779  0.850  0.79450\n",
       "4  0.775  0.773  0.779  0.849  0.79400"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_stats_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.773\n",
       "apr    0.772\n",
       "acc    0.777\n",
       "auc    0.849\n",
       "avg    0.793\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(svm_stats_train.mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1     0.002\n",
       "apr    0.002\n",
       "acc    0.002\n",
       "auc    0.001\n",
       "avg    0.002\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(svm_stats_train.std(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.966</td>\n",
       "      <td>2.297</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.881</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1.466</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.847</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc   time    avg  f1_std  apr_std  acc_std  \\\n",
       "test_size                                                                       \n",
       "0.2        0.901  0.901  0.904  0.966  2.297  0.918   0.003    0.003    0.003   \n",
       "0.5        0.881  0.881  0.884  0.953  1.466  0.900   0.002    0.003    0.003   \n",
       "0.8        0.847  0.848  0.851  0.927  0.840  0.868   0.003    0.005    0.004   \n",
       "\n",
       "           auc_std  avg_std  \n",
       "test_size                    \n",
       "0.2          0.001    0.003  \n",
       "0.5          0.002    0.003  \n",
       "0.8          0.001    0.003  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.876</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.947</td>\n",
       "      <td>56.281</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.856</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.935</td>\n",
       "      <td>31.583</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.907</td>\n",
       "      <td>13.572</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc    time    avg  f1_std  apr_std  \\\n",
       "test_size                                                               \n",
       "0.2        0.876  0.875  0.879  0.947  56.281  0.894   0.004    0.005   \n",
       "0.5        0.856  0.855  0.859  0.935  31.583  0.876   0.002    0.003   \n",
       "0.8        0.828  0.827  0.831  0.907  13.572  0.848   0.003    0.003   \n",
       "\n",
       "           acc_std  auc_std  avg_std  \n",
       "test_size                             \n",
       "0.2          0.004    0.003    0.004  \n",
       "0.5          0.003    0.001    0.002  \n",
       "0.8          0.003    0.002    0.003  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>apr_std</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>auc_std</th>\n",
       "      <th>avg_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.849</td>\n",
       "      <td>73.531</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.846</td>\n",
       "      <td>31.828</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.845</td>\n",
       "      <td>5.017</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1    apr    acc    auc    time    avg  f1_std  apr_std  \\\n",
       "test_size                                                               \n",
       "0.2        0.777  0.776  0.780  0.849  73.531  0.795   0.003    0.003   \n",
       "0.5        0.771  0.770  0.773  0.846  31.828  0.790   0.002    0.002   \n",
       "0.8        0.770  0.769  0.773  0.845   5.017  0.789   0.003    0.002   \n",
       "\n",
       "           acc_std  auc_std  avg_std  \n",
       "test_size                             \n",
       "0.2          0.004    0.004    0.003  \n",
       "0.5          0.002    0.001    0.002  \n",
       "0.8          0.004    0.000    0.002  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.344039</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.212571</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.904</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.966</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.334002</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.412442</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.884</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.475002</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.880</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.951</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.510001</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.776029</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.844</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.848</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.871509</td>\n",
       "      <td>0.869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size      time    avg\n",
       "0  0.899  0.899  0.901  0.965    0.0        0.2  2.344039  0.916\n",
       "1  0.901  0.900  0.903  0.966    1.0        0.2  2.212571  0.918\n",
       "2  0.904  0.905  0.907  0.966    2.0        0.2  2.334002  0.921\n",
       "3  0.880  0.880  0.883  0.953    0.0        0.5  1.412442  0.899\n",
       "4  0.884  0.885  0.887  0.955    1.0        0.5  1.475002  0.903\n",
       "5  0.880  0.879  0.882  0.951    2.0        0.5  1.510001  0.898\n",
       "6  0.850  0.852  0.854  0.928    0.0        0.8  0.776029  0.871\n",
       "7  0.844  0.843  0.847  0.926    1.0        0.8  0.872000  0.865\n",
       "8  0.848  0.848  0.852  0.927    2.0        0.8  0.871509  0.869"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.878</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>63.859345</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.878</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>55.142839</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.943</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>49.840869</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.856</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.621953</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.854</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.340651</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.858</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.936</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>33.786509</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.829</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>11.861124</td>\n",
       "      <td>0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.830</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10.062000</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.905</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>18.791530</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size       time    avg\n",
       "0  0.878  0.878  0.881  0.949    0.0        0.2  63.859345  0.896\n",
       "1  0.878  0.877  0.881  0.949    1.0        0.2  55.142839  0.896\n",
       "2  0.871  0.869  0.874  0.943    2.0        0.2  49.840869  0.889\n",
       "3  0.856  0.856  0.860  0.934    0.0        0.5  30.621953  0.876\n",
       "4  0.854  0.852  0.856  0.935    1.0        0.5  30.340651  0.874\n",
       "5  0.858  0.857  0.861  0.936    2.0        0.5  33.786509  0.878\n",
       "6  0.829  0.829  0.833  0.907    0.0        0.8  11.861124  0.849\n",
       "7  0.830  0.829  0.833  0.909    1.0        0.8  10.062000  0.850\n",
       "8  0.825  0.824  0.828  0.905    2.0        0.8  18.791530  0.845"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>apr</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>trial</th>\n",
       "      <th>test_size</th>\n",
       "      <th>time</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.774</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>78.909204</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>78.613059</td>\n",
       "      <td>0.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>63.070758</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.769</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>35.570660</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>29.320400</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.847</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>30.592607</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.313310</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.659288</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.767</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.845</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.078000</td>\n",
       "      <td>0.787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      f1    apr    acc    auc  trial  test_size       time    avg\n",
       "0  0.774  0.773  0.776  0.845    0.0        0.2  78.909204  0.792\n",
       "1  0.778  0.776  0.781  0.849    1.0        0.2  78.613059  0.796\n",
       "2  0.779  0.778  0.783  0.852    2.0        0.2  63.070758  0.798\n",
       "3  0.769  0.768  0.772  0.845    0.0        0.5  35.570660  0.788\n",
       "4  0.771  0.770  0.773  0.847    1.0        0.5  29.320400  0.790\n",
       "5  0.772  0.771  0.775  0.847    2.0        0.5  30.592607  0.791\n",
       "6  0.771  0.770  0.775  0.845    0.0        0.8   5.313310  0.790\n",
       "7  0.772  0.771  0.776  0.845    1.0        0.8   4.659288  0.791\n",
       "8  0.767  0.767  0.769  0.845    2.0        0.8   5.078000  0.787"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([180.136,  93.52 ,  13.636])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_mean.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "now = time.time()\n",
    "print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "\n",
    "probs_svm = svc.predict_proba(x_test1)[:,1]\n",
    "#probs_svm = svc.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = shuffle(X, Y)\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_train = X_train[0:50000]\n",
    "Y_train = Y_train[0:50000]\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X_train, Y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 4 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf = ensemble.RandomForestClassifier(n_jobs = -1, n_estimators=200, random_state=42)\n",
    "clf.fit(x_train1, y_train1)\n",
    "\n",
    "now = time.time()\n",
    "print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "\n",
    "probs = clf.predict_proba(x_test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(tpr - fpr) w/ th =  0.45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_test1, probs, 'RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.849855245405214\n",
      "precision_score:\n",
      "0.8487540683517956\n",
      "accuracy_score:\n",
      "0.852425\n",
      "Confusion Matrix:\n",
      "[[19665  3223]\n",
      " [ 2680 14432]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87     22888\n",
      "           1       0.82      0.84      0.83     17112\n",
      "\n",
      "    accuracy                           0.85     40000\n",
      "   macro avg       0.85      0.85      0.85     40000\n",
      "weighted avg       0.85      0.85      0.85     40000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.849855245405214, 0.8487540683517956, 0.852425, 0.9304391912856051)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_info(0.455,y_test1, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 17 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "mlp = MLPClassifier(solver='adam', activation='relu', alpha=0.001, hidden_layer_sizes = (32, 32), max_iter = 10000)\n",
    "mlp.fit(x_train1, y_train1)\n",
    "\n",
    "now = time.time()\n",
    "print('Elapsed Time: ' + str(int(now-start)) + ' seconds')\n",
    "probs_nn = mlp.predict_proba(x_test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(tpr - fpr) w/ th =  0.4461228714959328\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:\n",
      "0.7724523399472379\n",
      "precision_score:\n",
      "0.7752285593730208\n",
      "accuracy_score:\n",
      "0.7787\n",
      "Confusion Matrix:\n",
      "[[4722  984]\n",
      " [1229 3065]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81      5706\n",
      "           1       0.76      0.71      0.73      4294\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.77      0.77     10000\n",
      "weighted avg       0.78      0.78      0.78     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7724523399472379, 0.7752285593730208, 0.7787, 0.8533112008686465)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_roc(y_test1, probs_nn, 'MLPClassifier')\n",
    "\n",
    "matrix_info(0.53, y_test1, probs_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svc = LSVC(C = 3, loss = 'hinge', max_iter=10000)\n",
    "svc = CalibratedClassifierCV(svc)\n",
    "svc.fit(x_train1,y_train1)\n",
    "probs = svc.predict_proba(x_test1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(tpr - fpr) w/ th =  0.4392937121505492\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc(y_test1, probs, 'LinearSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1336s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (3.5927s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   42.9s\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:   45.2s\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   50.5s\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:   52.2s\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:   52.4s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:   58.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:  1.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 280 out of 280 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=200,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_sta...\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['relu'],\n",
       "                         'alpha': array([0.1   , 0.01  , 0.001 , 0.0001]),\n",
       "                         'hidden_layer_sizes': [(10, 1), (16, 16), (10, 10),\n",
       "                                                (64, 1), (64, 64), (32, 32),\n",
       "                                                (32, 16)],\n",
       "                         'max_iter': [10000], 'solver': ['adam', 'lbfgs']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'solver': ['adam','lbfgs'], \n",
    "              'activation': ['relu'],\n",
    "              'max_iter': [10000], \n",
    "              'alpha': 10.0 ** -np.arange(1, 5), \n",
    "              'hidden_layer_sizes':[(10,1),(16,16),(10,10),(64,1),(64,64),(32,32),(32,16)]}\n",
    "cv = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1, cv = 5, verbose = 20)\n",
    "cv.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (32, 32), 'max_iter': 10000, 'solver': 'adam'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>7.990436</td>\n",
       "      <td>1.569940</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.769479</td>\n",
       "      <td>0.772617</td>\n",
       "      <td>0.752323</td>\n",
       "      <td>0.750707</td>\n",
       "      <td>0.763232</td>\n",
       "      <td>0.761674</td>\n",
       "      <td>0.008841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>7.058115</td>\n",
       "      <td>2.703894</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.751716</td>\n",
       "      <td>0.760097</td>\n",
       "      <td>0.754747</td>\n",
       "      <td>0.747879</td>\n",
       "      <td>0.785051</td>\n",
       "      <td>0.759897</td>\n",
       "      <td>0.013194</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.674869</td>\n",
       "      <td>1.525631</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.714574</td>\n",
       "      <td>0.758481</td>\n",
       "      <td>0.753939</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.763232</td>\n",
       "      <td>0.751899</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.320126</td>\n",
       "      <td>1.681171</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.731530</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>0.728889</td>\n",
       "      <td>0.785859</td>\n",
       "      <td>0.748101</td>\n",
       "      <td>0.020433</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.913055</td>\n",
       "      <td>2.750792</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.735567</td>\n",
       "      <td>0.765751</td>\n",
       "      <td>0.705455</td>\n",
       "      <td>0.754747</td>\n",
       "      <td>0.766061</td>\n",
       "      <td>0.745516</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.506609</td>\n",
       "      <td>1.948132</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.713363</td>\n",
       "      <td>0.760501</td>\n",
       "      <td>0.709899</td>\n",
       "      <td>0.766061</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.741477</td>\n",
       "      <td>0.024548</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9.704839</td>\n",
       "      <td>2.271285</td>\n",
       "      <td>0.016954</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.746871</td>\n",
       "      <td>0.682553</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.720404</td>\n",
       "      <td>0.734206</td>\n",
       "      <td>0.030471</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.900554</td>\n",
       "      <td>3.510965</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.766653</td>\n",
       "      <td>0.598950</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.728712</td>\n",
       "      <td>0.065155</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.403415</td>\n",
       "      <td>1.289711</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.768268</td>\n",
       "      <td>0.723748</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.771717</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.058188</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.644494</td>\n",
       "      <td>2.149511</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.716996</td>\n",
       "      <td>0.721325</td>\n",
       "      <td>0.715960</td>\n",
       "      <td>0.756768</td>\n",
       "      <td>0.701010</td>\n",
       "      <td>0.722411</td>\n",
       "      <td>0.018494</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.418736</td>\n",
       "      <td>1.055409</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.747679</td>\n",
       "      <td>0.711632</td>\n",
       "      <td>0.671515</td>\n",
       "      <td>0.730101</td>\n",
       "      <td>0.750303</td>\n",
       "      <td>0.722249</td>\n",
       "      <td>0.028919</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>4.339085</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.752523</td>\n",
       "      <td>0.766155</td>\n",
       "      <td>0.754747</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.756768</td>\n",
       "      <td>0.719502</td>\n",
       "      <td>0.076245</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.898115</td>\n",
       "      <td>1.589946</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.714978</td>\n",
       "      <td>0.711228</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.723636</td>\n",
       "      <td>0.748687</td>\n",
       "      <td>0.719098</td>\n",
       "      <td>0.017114</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>4.873254</td>\n",
       "      <td>0.801719</td>\n",
       "      <td>0.009573</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.754542</td>\n",
       "      <td>0.752423</td>\n",
       "      <td>0.725253</td>\n",
       "      <td>0.736566</td>\n",
       "      <td>0.612525</td>\n",
       "      <td>0.716271</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>9.038930</td>\n",
       "      <td>3.347687</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.765442</td>\n",
       "      <td>0.651050</td>\n",
       "      <td>0.703434</td>\n",
       "      <td>0.753131</td>\n",
       "      <td>0.697374</td>\n",
       "      <td>0.714090</td>\n",
       "      <td>0.041304</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>5.658972</td>\n",
       "      <td>1.124508</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.664110</td>\n",
       "      <td>0.657512</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>0.763232</td>\n",
       "      <td>0.712231</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.210390</td>\n",
       "      <td>2.714899</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.763827</td>\n",
       "      <td>0.699515</td>\n",
       "      <td>0.571717</td>\n",
       "      <td>0.690505</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.698982</td>\n",
       "      <td>0.071294</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.970049</td>\n",
       "      <td>1.898774</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.005990</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.707711</td>\n",
       "      <td>0.683764</td>\n",
       "      <td>0.758384</td>\n",
       "      <td>0.583838</td>\n",
       "      <td>0.753131</td>\n",
       "      <td>0.697366</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>8.413029</td>\n",
       "      <td>0.984643</td>\n",
       "      <td>0.016784</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.662495</td>\n",
       "      <td>0.551696</td>\n",
       "      <td>0.749495</td>\n",
       "      <td>0.768485</td>\n",
       "      <td>0.747071</td>\n",
       "      <td>0.695831</td>\n",
       "      <td>0.080835</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.558672</td>\n",
       "      <td>0.900883</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.704078</td>\n",
       "      <td>0.723748</td>\n",
       "      <td>0.620606</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.674343</td>\n",
       "      <td>0.687995</td>\n",
       "      <td>0.037722</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.408656</td>\n",
       "      <td>5.986479</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.570505</td>\n",
       "      <td>0.778182</td>\n",
       "      <td>0.610115</td>\n",
       "      <td>0.084030</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>4.749013</td>\n",
       "      <td>2.109914</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>0.608337</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.457776</td>\n",
       "      <td>0.415416</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.617124</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.608081</td>\n",
       "      <td>0.585393</td>\n",
       "      <td>0.022399</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.763739</td>\n",
       "      <td>0.954291</td>\n",
       "      <td>0.014362</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.599596</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.573760</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.017235</td>\n",
       "      <td>0.486691</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.570044</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.568485</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.731167</td>\n",
       "      <td>1.624097</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.195055</td>\n",
       "      <td>2.215836</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.436230</td>\n",
       "      <td>1.343536</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.199000</td>\n",
       "      <td>1.925228</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.485845</td>\n",
       "      <td>0.261638</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.217780</td>\n",
       "      <td>0.874409</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.316649</td>\n",
       "      <td>0.493024</td>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.566007</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567054</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.129839</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.565253</td>\n",
       "      <td>0.566893</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.737171</td>\n",
       "      <td>1.441851</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.562778</td>\n",
       "      <td>0.567851</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.566061</td>\n",
       "      <td>0.564848</td>\n",
       "      <td>0.565762</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.882086</td>\n",
       "      <td>0.014160</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.554343</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.564712</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.171541</td>\n",
       "      <td>0.114868</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.548648</td>\n",
       "      <td>0.562601</td>\n",
       "      <td>0.572929</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.539394</td>\n",
       "      <td>0.558168</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.361633</td>\n",
       "      <td>0.091521</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.492128</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.571717</td>\n",
       "      <td>0.553159</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.969273</td>\n",
       "      <td>0.534459</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.500606</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.548283</td>\n",
       "      <td>0.550170</td>\n",
       "      <td>0.025865</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.227036</td>\n",
       "      <td>1.006160</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.541195</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.538182</td>\n",
       "      <td>0.517980</td>\n",
       "      <td>0.546373</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.396893</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.580541</td>\n",
       "      <td>0.530291</td>\n",
       "      <td>0.566869</td>\n",
       "      <td>0.520404</td>\n",
       "      <td>0.533737</td>\n",
       "      <td>0.546373</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.702365</td>\n",
       "      <td>0.553420</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.544272</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.122207</td>\n",
       "      <td>0.653832</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 32)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.509487</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.534141</td>\n",
       "      <td>0.532929</td>\n",
       "      <td>0.542252</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.279504</td>\n",
       "      <td>0.812623</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 64)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.568026</td>\n",
       "      <td>0.567044</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.433131</td>\n",
       "      <td>0.540556</td>\n",
       "      <td>0.053705</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.419162</td>\n",
       "      <td>0.233678</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.540394</td>\n",
       "      <td>0.053825</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.112695</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.540394</td>\n",
       "      <td>0.053825</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.184682</td>\n",
       "      <td>0.009574</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.432553</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.540313</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.334714</td>\n",
       "      <td>0.266565</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.564392</td>\n",
       "      <td>0.566640</td>\n",
       "      <td>0.465859</td>\n",
       "      <td>0.519192</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.536678</td>\n",
       "      <td>0.039805</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.138262</td>\n",
       "      <td>0.935874</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.515751</td>\n",
       "      <td>0.482020</td>\n",
       "      <td>0.544646</td>\n",
       "      <td>0.572525</td>\n",
       "      <td>0.536436</td>\n",
       "      <td>0.033781</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.103918</td>\n",
       "      <td>0.033503</td>\n",
       "      <td>0.005386</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.530480</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.533042</td>\n",
       "      <td>0.052142</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.450079</td>\n",
       "      <td>0.279044</td>\n",
       "      <td>0.006583</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(16, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.491922</td>\n",
       "      <td>0.525657</td>\n",
       "      <td>0.508283</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.532073</td>\n",
       "      <td>0.030640</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.267883</td>\n",
       "      <td>0.214943</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.475172</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.482020</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.531831</td>\n",
       "      <td>0.043527</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.521421</td>\n",
       "      <td>0.155729</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.001, 'hidden...</td>\n",
       "      <td>0.497780</td>\n",
       "      <td>0.557351</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.452929</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.528518</td>\n",
       "      <td>0.045809</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.181530</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>(64, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.01, 'hidden_...</td>\n",
       "      <td>0.432782</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092550</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.1, 'hidden_l...</td>\n",
       "      <td>0.432782</td>\n",
       "      <td>0.567447</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.130053</td>\n",
       "      <td>0.055953</td>\n",
       "      <td>0.007580</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>0.432553</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.513411</td>\n",
       "      <td>0.065947</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.273413</td>\n",
       "      <td>0.175963</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>10000</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.0001, 'hidde...</td>\n",
       "      <td>0.448123</td>\n",
       "      <td>0.439822</td>\n",
       "      <td>0.432727</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.567273</td>\n",
       "      <td>0.491032</td>\n",
       "      <td>0.062428</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "38       7.990436      1.569940         0.015160        0.010392   \n",
       "46       7.058115      2.703894         0.005784        0.000399   \n",
       "4        4.674869      1.525631         0.008380        0.003867   \n",
       "40       7.320126      1.681171         0.007979        0.001410   \n",
       "50       8.913055      2.750792         0.013364        0.001492   \n",
       "24       6.506609      1.948132         0.011171        0.003051   \n",
       "8        9.704839      2.271285         0.016954        0.003154   \n",
       "18       6.900554      3.510965         0.006383        0.001018   \n",
       "2        4.403415      1.289711         0.005984        0.000630   \n",
       "30       5.644494      2.149511         0.006384        0.001492   \n",
       "16       4.418736      1.055409         0.007181        0.000751   \n",
       "54       4.339085      0.795141         0.004946        0.001681   \n",
       "10       4.898115      1.589946         0.007982        0.000636   \n",
       "52       4.873254      0.801719         0.009573        0.001016   \n",
       "36       9.038930      3.347687         0.014960        0.002602   \n",
       "44       5.658972      1.124508         0.006189        0.000744   \n",
       "12       7.210390      2.714899         0.011569        0.007792   \n",
       "32       3.970049      1.898774         0.007981        0.005990   \n",
       "22       8.413029      0.984643         0.016784        0.003225   \n",
       "26       6.558672      0.900883         0.008378        0.001492   \n",
       "14       5.408656      5.986479         0.004788        0.000399   \n",
       "28       4.749013      2.109914         0.005386        0.001353   \n",
       "45       0.457776      0.415416         0.008379        0.006296   \n",
       "9        1.763739      0.954291         0.014362        0.001353   \n",
       "51       1.017235      0.486691         0.015758        0.003645   \n",
       "42       2.731167      1.624097         0.005385        0.000491   \n",
       "20       5.195055      2.215836         0.009970        0.002094   \n",
       "48       4.436230      1.343536         0.010173        0.003420   \n",
       "6        5.199000      1.925228         0.009375        0.001353   \n",
       "49       0.485845      0.261638         0.010171        0.001935   \n",
       "34       3.217780      0.874409         0.008577        0.000489   \n",
       "0        2.316649      0.493024         0.004987        0.001093   \n",
       "33       0.207445      0.129839         0.008377        0.005802   \n",
       "23       1.737171      1.441851         0.022140        0.007553   \n",
       "39       0.885720      0.882086         0.014160        0.007608   \n",
       "5        0.171541      0.114868         0.005386        0.000489   \n",
       "11       0.361633      0.091521         0.008380        0.001020   \n",
       "25       0.969273      0.534459         0.012269        0.003648   \n",
       "27       1.227036      1.006160         0.011368        0.004661   \n",
       "17       0.396893      0.252252         0.006982        0.002524   \n",
       "13       0.702365      0.553420         0.007779        0.001715   \n",
       "53       1.122207      0.653832         0.009576        0.002648   \n",
       "37       1.279504      0.812623         0.017951        0.006370   \n",
       "35       0.419162      0.233678         0.008378        0.000489   \n",
       "15       0.112695      0.023263         0.006184        0.001164   \n",
       "7        0.282843      0.184682         0.009574        0.002054   \n",
       "31       0.334714      0.266565         0.007381        0.004306   \n",
       "55       1.138262      0.935874         0.007181        0.000977   \n",
       "29       0.103918      0.033503         0.005386        0.001850   \n",
       "3        0.450079      0.279044         0.006583        0.001850   \n",
       "19       0.267883      0.214943         0.007979        0.004040   \n",
       "41       0.521421      0.155729         0.007980        0.001093   \n",
       "21       0.181530      0.037242         0.010172        0.000747   \n",
       "1        0.092550      0.031893         0.005985        0.001093   \n",
       "43       0.130053      0.055953         0.007580        0.002411   \n",
       "47       0.273413      0.175963         0.008374        0.005339   \n",
       "\n",
       "   param_activation param_alpha param_hidden_layer_sizes param_max_iter  \\\n",
       "38             relu       0.001                 (32, 32)          10000   \n",
       "46             relu      0.0001                 (10, 10)          10000   \n",
       "4              relu         0.1                 (10, 10)          10000   \n",
       "40             relu       0.001                 (32, 16)          10000   \n",
       "50             relu      0.0001                 (64, 64)          10000   \n",
       "24             relu        0.01                 (32, 32)          10000   \n",
       "8              relu         0.1                 (64, 64)          10000   \n",
       "18             relu        0.01                 (10, 10)          10000   \n",
       "2              relu         0.1                 (16, 16)          10000   \n",
       "30             relu       0.001                 (16, 16)          10000   \n",
       "16             relu        0.01                 (16, 16)          10000   \n",
       "54             relu      0.0001                 (32, 16)          10000   \n",
       "10             relu         0.1                 (32, 32)          10000   \n",
       "52             relu      0.0001                 (32, 32)          10000   \n",
       "36             relu       0.001                 (64, 64)          10000   \n",
       "44             relu      0.0001                 (16, 16)          10000   \n",
       "12             relu         0.1                 (32, 16)          10000   \n",
       "32             relu       0.001                 (10, 10)          10000   \n",
       "22             relu        0.01                 (64, 64)          10000   \n",
       "26             relu        0.01                 (32, 16)          10000   \n",
       "14             relu        0.01                  (10, 1)          10000   \n",
       "28             relu       0.001                  (10, 1)          10000   \n",
       "45             relu      0.0001                 (16, 16)          10000   \n",
       "9              relu         0.1                 (64, 64)          10000   \n",
       "51             relu      0.0001                 (64, 64)          10000   \n",
       "42             relu      0.0001                  (10, 1)          10000   \n",
       "20             relu        0.01                  (64, 1)          10000   \n",
       "48             relu      0.0001                  (64, 1)          10000   \n",
       "6              relu         0.1                  (64, 1)          10000   \n",
       "49             relu      0.0001                  (64, 1)          10000   \n",
       "34             relu       0.001                  (64, 1)          10000   \n",
       "0              relu         0.1                  (10, 1)          10000   \n",
       "33             relu       0.001                 (10, 10)          10000   \n",
       "23             relu        0.01                 (64, 64)          10000   \n",
       "39             relu       0.001                 (32, 32)          10000   \n",
       "5              relu         0.1                 (10, 10)          10000   \n",
       "11             relu         0.1                 (32, 32)          10000   \n",
       "25             relu        0.01                 (32, 32)          10000   \n",
       "27             relu        0.01                 (32, 16)          10000   \n",
       "17             relu        0.01                 (16, 16)          10000   \n",
       "13             relu         0.1                 (32, 16)          10000   \n",
       "53             relu      0.0001                 (32, 32)          10000   \n",
       "37             relu       0.001                 (64, 64)          10000   \n",
       "35             relu       0.001                  (64, 1)          10000   \n",
       "15             relu        0.01                  (10, 1)          10000   \n",
       "7              relu         0.1                  (64, 1)          10000   \n",
       "31             relu       0.001                 (16, 16)          10000   \n",
       "55             relu      0.0001                 (32, 16)          10000   \n",
       "29             relu       0.001                  (10, 1)          10000   \n",
       "3              relu         0.1                 (16, 16)          10000   \n",
       "19             relu        0.01                 (10, 10)          10000   \n",
       "41             relu       0.001                 (32, 16)          10000   \n",
       "21             relu        0.01                  (64, 1)          10000   \n",
       "1              relu         0.1                  (10, 1)          10000   \n",
       "43             relu      0.0001                  (10, 1)          10000   \n",
       "47             relu      0.0001                 (10, 10)          10000   \n",
       "\n",
       "   param_solver                                             params  \\\n",
       "38         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "46         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "4          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "40         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "50         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "24         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "8          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "18         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "2          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "30         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "16         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "54         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "10         adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "52         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "36         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "44         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "12         adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "32         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "22         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "26         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "14         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "28         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "45        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "9         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "51        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "42         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "20         adam  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "48         adam  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "6          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "49        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "34         adam  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "0          adam  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "33        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "23        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "39        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "5         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "11        lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "25        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "27        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "17        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "13        lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "53        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "37        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "35        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "15        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "7         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "31        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "55        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "29        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "3         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "19        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "41        lbfgs  {'activation': 'relu', 'alpha': 0.001, 'hidden...   \n",
       "21        lbfgs  {'activation': 'relu', 'alpha': 0.01, 'hidden_...   \n",
       "1         lbfgs  {'activation': 'relu', 'alpha': 0.1, 'hidden_l...   \n",
       "43        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "47        lbfgs  {'activation': 'relu', 'alpha': 0.0001, 'hidde...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "38           0.769479           0.772617           0.752323   \n",
       "46           0.751716           0.760097           0.754747   \n",
       "4            0.714574           0.758481           0.753939   \n",
       "40           0.731530           0.750000           0.744242   \n",
       "50           0.735567           0.765751           0.705455   \n",
       "24           0.713363           0.760501           0.709899   \n",
       "8            0.746871           0.682553           0.749899   \n",
       "18           0.766653           0.598950           0.749899   \n",
       "2            0.768268           0.723748           0.616162   \n",
       "30           0.716996           0.721325           0.715960   \n",
       "16           0.747679           0.711632           0.671515   \n",
       "54           0.752523           0.766155           0.754747   \n",
       "10           0.714978           0.711228           0.696970   \n",
       "52           0.754542           0.752423           0.725253   \n",
       "36           0.765442           0.651050           0.703434   \n",
       "44           0.664110           0.657512           0.769293   \n",
       "12           0.763827           0.699515           0.571717   \n",
       "32           0.707711           0.683764           0.758384   \n",
       "22           0.662495           0.551696           0.749495   \n",
       "26           0.704078           0.723748           0.620606   \n",
       "14           0.567218           0.567447           0.567273   \n",
       "28           0.567218           0.567447           0.567273   \n",
       "45           0.567218           0.617124           0.567273   \n",
       "9            0.567218           0.567447           0.599596   \n",
       "51           0.570044           0.567447           0.567273   \n",
       "42           0.567218           0.567447           0.567273   \n",
       "20           0.567218           0.567447           0.567273   \n",
       "48           0.567218           0.567447           0.567273   \n",
       "6            0.567218           0.567447           0.567273   \n",
       "49           0.567218           0.567447           0.567273   \n",
       "34           0.567218           0.567447           0.567273   \n",
       "0            0.566007           0.567447           0.567273   \n",
       "33           0.567218           0.567447           0.567273   \n",
       "23           0.562778           0.567851           0.567273   \n",
       "39           0.567218           0.567447           0.567273   \n",
       "5            0.548648           0.562601           0.572929   \n",
       "11           0.492128           0.567447           0.567273   \n",
       "25           0.500606           0.567447           0.567273   \n",
       "27           0.567218           0.541195           0.567273   \n",
       "17           0.580541           0.530291           0.566869   \n",
       "13           0.553492           0.544426           0.567273   \n",
       "53           0.509487           0.567447           0.567273   \n",
       "37           0.568026           0.567044           0.567273   \n",
       "35           0.567218           0.567447           0.432727   \n",
       "15           0.567218           0.567447           0.567273   \n",
       "7            0.567218           0.432553           0.567273   \n",
       "31           0.564392           0.566640           0.465859   \n",
       "55           0.567218           0.515751           0.482020   \n",
       "29           0.530480           0.567447           0.432727   \n",
       "3            0.567218           0.491922           0.525657   \n",
       "19           0.475172           0.567447           0.482020   \n",
       "41           0.497780           0.557351           0.567273   \n",
       "21           0.432782           0.567447           0.567273   \n",
       "1            0.432782           0.567447           0.567273   \n",
       "43           0.567218           0.432553           0.432727   \n",
       "47           0.448123           0.439822           0.432727   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "38           0.750707           0.763232         0.761674        0.008841   \n",
       "46           0.747879           0.785051         0.759897        0.013194   \n",
       "4            0.769293           0.763232         0.751899        0.019351   \n",
       "40           0.728889           0.785859         0.748101        0.020433   \n",
       "50           0.754747           0.766061         0.745516        0.022892   \n",
       "24           0.766061           0.757576         0.741477        0.024548   \n",
       "8            0.771313           0.720404         0.734206        0.030471   \n",
       "18           0.763636           0.764444         0.728712        0.065155   \n",
       "2            0.771717           0.757576         0.727500        0.058188   \n",
       "30           0.756768           0.701010         0.722411        0.018494   \n",
       "16           0.730101           0.750303         0.722249        0.028919   \n",
       "54           0.567273           0.756768         0.719502        0.076245   \n",
       "10           0.723636           0.748687         0.719098        0.017114   \n",
       "52           0.736566           0.612525         0.716271        0.052964   \n",
       "36           0.753131           0.697374         0.714090        0.041304   \n",
       "44           0.707071           0.763232         0.712231        0.047316   \n",
       "12           0.690505           0.769293         0.698982        0.071294   \n",
       "32           0.583838           0.753131         0.697366        0.063259   \n",
       "22           0.768485           0.747071         0.695831        0.080835   \n",
       "26           0.717172           0.674343         0.687995        0.037722   \n",
       "14           0.570505           0.778182         0.610115        0.084030   \n",
       "28           0.567273           0.772525         0.608337        0.082082   \n",
       "45           0.567273           0.608081         0.585393        0.022399   \n",
       "9            0.567273           0.567273         0.573760        0.012916   \n",
       "51           0.568485           0.567273         0.568105        0.001070   \n",
       "42           0.567273           0.567273         0.567297        0.000078   \n",
       "20           0.567273           0.567273         0.567297        0.000078   \n",
       "48           0.567273           0.567273         0.567297        0.000078   \n",
       "6            0.567273           0.567273         0.567297        0.000078   \n",
       "49           0.567273           0.567273         0.567297        0.000078   \n",
       "34           0.567273           0.567273         0.567297        0.000078   \n",
       "0            0.567273           0.567273         0.567054        0.000528   \n",
       "33           0.567273           0.565253         0.566893        0.000824   \n",
       "23           0.566061           0.564848         0.565762        0.001816   \n",
       "39           0.554343           0.567273         0.564712        0.005184   \n",
       "5            0.567273           0.539394         0.558168        0.012357   \n",
       "11           0.567273           0.571717         0.553159        0.030574   \n",
       "25           0.567273           0.548283         0.550170        0.025865   \n",
       "27           0.538182           0.517980         0.546373        0.018822   \n",
       "17           0.520404           0.533737         0.546373        0.023154   \n",
       "13           0.488889           0.567273         0.544272        0.029015   \n",
       "53           0.534141           0.532929         0.542252        0.022305   \n",
       "37           0.567273           0.433131         0.540556        0.053705   \n",
       "35           0.567273           0.567273         0.540394        0.053825   \n",
       "15           0.567273           0.432727         0.540394        0.053825   \n",
       "7            0.567273           0.567273         0.540313        0.053886   \n",
       "31           0.519192           0.567273         0.536678        0.039805   \n",
       "55           0.544646           0.572525         0.536436        0.033781   \n",
       "29           0.567273           0.567273         0.533042        0.052142   \n",
       "3            0.508283           0.567273         0.532073        0.030640   \n",
       "19           0.567273           0.567273         0.531831        0.043527   \n",
       "41           0.452929           0.567273         0.528518        0.045809   \n",
       "21           0.432727           0.567273         0.513492        0.065931   \n",
       "1            0.432727           0.567273         0.513492        0.065931   \n",
       "43           0.567273           0.567273         0.513411        0.065947   \n",
       "47           0.567273           0.567273         0.491032        0.062428   \n",
       "\n",
       "    rank_test_score  \n",
       "38                1  \n",
       "46                2  \n",
       "4                 3  \n",
       "40                4  \n",
       "50                5  \n",
       "24                6  \n",
       "8                 7  \n",
       "18                8  \n",
       "2                 9  \n",
       "30               10  \n",
       "16               11  \n",
       "54               12  \n",
       "10               13  \n",
       "52               14  \n",
       "36               15  \n",
       "44               16  \n",
       "12               17  \n",
       "32               18  \n",
       "22               19  \n",
       "26               20  \n",
       "14               21  \n",
       "28               22  \n",
       "45               23  \n",
       "9                24  \n",
       "51               25  \n",
       "42               26  \n",
       "20               26  \n",
       "48               26  \n",
       "6                26  \n",
       "49               26  \n",
       "34               26  \n",
       "0                32  \n",
       "33               33  \n",
       "23               34  \n",
       "39               35  \n",
       "5                36  \n",
       "11               37  \n",
       "25               38  \n",
       "27               39  \n",
       "17               39  \n",
       "13               41  \n",
       "53               42  \n",
       "37               43  \n",
       "35               44  \n",
       "15               44  \n",
       "7                46  \n",
       "31               47  \n",
       "55               48  \n",
       "29               49  \n",
       "3                50  \n",
       "19               51  \n",
       "41               52  \n",
       "21               53  \n",
       "1                53  \n",
       "43               55  \n",
       "47               56  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  30 | elapsed:   30.5s remaining:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed:   33.3s remaining:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  30 | elapsed:   34.5s remaining:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   39.4s remaining:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  30 | elapsed:   44.2s remaining:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:   45.2s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   46.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=-1,\n",
       "                                              oob_score=False, random_state=42,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [1, 10, 25, 50, 75, 100],\n",
       "                         'n_estimators': [200]},\n",
       "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "             return_train_score=False, scoring='accuracy', verbose=20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth' : [1,10,25,50,75,100]\n",
    "}\n",
    "\n",
    "clf = ensemble.RandomForestClassifier(n_jobs = -1, random_state=42)\n",
    "cv = GridSearchCV(clf, param_grid=params, scoring='accuracy', n_jobs=-1, verbose=20, refit='accuracy', cv=5)\n",
    "cv.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'max_depth': 75, 'n_estimators': 200}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.444399</td>\n",
       "      <td>0.431850</td>\n",
       "      <td>3.123003</td>\n",
       "      <td>0.597215</td>\n",
       "      <td>75</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 75, 'n_estimators': 200}</td>\n",
       "      <td>0.904410</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9056</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.900490</td>\n",
       "      <td>0.90434</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.823797</td>\n",
       "      <td>1.265460</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>1.035584</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 100, 'n_estimators': 200}</td>\n",
       "      <td>0.904410</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9056</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.900490</td>\n",
       "      <td>0.90434</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.103399</td>\n",
       "      <td>0.261796</td>\n",
       "      <td>3.297001</td>\n",
       "      <td>0.307195</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 200}</td>\n",
       "      <td>0.903810</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>0.900490</td>\n",
       "      <td>0.90420</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.634397</td>\n",
       "      <td>0.359733</td>\n",
       "      <td>2.959002</td>\n",
       "      <td>0.239607</td>\n",
       "      <td>25</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 200}</td>\n",
       "      <td>0.897310</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>0.8992</td>\n",
       "      <td>0.8983</td>\n",
       "      <td>0.891689</td>\n",
       "      <td>0.89694</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.322509</td>\n",
       "      <td>0.806921</td>\n",
       "      <td>3.067602</td>\n",
       "      <td>0.296348</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.806619</td>\n",
       "      <td>0.8058</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.802380</td>\n",
       "      <td>0.80434</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.808720</td>\n",
       "      <td>0.625019</td>\n",
       "      <td>2.465400</td>\n",
       "      <td>0.233617</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 200}</td>\n",
       "      <td>0.602440</td>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.599860</td>\n",
       "      <td>0.59716</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4      10.444399      0.431850         3.123003        0.597215   \n",
       "5       9.823797      1.265460         0.999000        1.035584   \n",
       "3      11.103399      0.261796         3.297001        0.307195   \n",
       "2      10.634397      0.359733         2.959002        0.239607   \n",
       "1       7.322509      0.806921         3.067602        0.296348   \n",
       "0       1.808720      0.625019         2.465400        0.233617   \n",
       "\n",
       "  param_max_depth param_n_estimators                                   params  \\\n",
       "4              75                200   {'max_depth': 75, 'n_estimators': 200}   \n",
       "5             100                200  {'max_depth': 100, 'n_estimators': 200}   \n",
       "3              50                200   {'max_depth': 50, 'n_estimators': 200}   \n",
       "2              25                200   {'max_depth': 25, 'n_estimators': 200}   \n",
       "1              10                200   {'max_depth': 10, 'n_estimators': 200}   \n",
       "0               1                200    {'max_depth': 1, 'n_estimators': 200}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "4           0.904410             0.9064             0.9056             0.9048   \n",
       "5           0.904410             0.9064             0.9056             0.9048   \n",
       "3           0.903810             0.9059             0.9057             0.9051   \n",
       "2           0.897310             0.8982             0.8992             0.8983   \n",
       "1           0.806619             0.8058             0.8055             0.8014   \n",
       "0           0.602440             0.5867             0.5932             0.6036   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "4           0.900490          0.90434        0.002043                1  \n",
       "5           0.900490          0.90434        0.002043                1  \n",
       "3           0.900490          0.90420        0.001993                3  \n",
       "2           0.891689          0.89694        0.002693                4  \n",
       "1           0.802380          0.80434        0.002057                5  \n",
       "0           0.599860          0.59716        0.006352                6  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_rf.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_rf.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.117403</td>\n",
       "      <td>1.013367</td>\n",
       "      <td>1.698999</td>\n",
       "      <td>0.255120</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 200}</td>\n",
       "      <td>0.904410</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9056</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.90049</td>\n",
       "      <td>0.90434</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.474804</td>\n",
       "      <td>1.736694</td>\n",
       "      <td>2.841802</td>\n",
       "      <td>0.437798</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 1, 'n_estimators': 200}</td>\n",
       "      <td>0.602440</td>\n",
       "      <td>0.5867</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.59986</td>\n",
       "      <td>0.59716</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.628413</td>\n",
       "      <td>0.846129</td>\n",
       "      <td>2.988997</td>\n",
       "      <td>0.237431</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.806619</td>\n",
       "      <td>0.8058</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.80238</td>\n",
       "      <td>0.80434</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.069333</td>\n",
       "      <td>0.438946</td>\n",
       "      <td>3.249159</td>\n",
       "      <td>0.182484</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 50, 'n_estimators': 200}</td>\n",
       "      <td>0.903810</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>0.90049</td>\n",
       "      <td>0.90420</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.801171</td>\n",
       "      <td>1.141707</td>\n",
       "      <td>1.208400</td>\n",
       "      <td>0.992366</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 100, 'n_estimators': 200}</td>\n",
       "      <td>0.904410</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9056</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.90049</td>\n",
       "      <td>0.90434</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       9.117403      1.013367         1.698999        0.255120   \n",
       "1       4.474804      1.736694         2.841802        0.437798   \n",
       "2       6.628413      0.846129         2.988997        0.237431   \n",
       "3      10.069333      0.438946         3.249159        0.182484   \n",
       "4       9.801171      1.141707         1.208400        0.992366   \n",
       "\n",
       "  param_max_depth param_n_estimators  \\\n",
       "0            None                200   \n",
       "1               1                200   \n",
       "2              10                200   \n",
       "3              50                200   \n",
       "4             100                200   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "0  {'max_depth': None, 'n_estimators': 200}           0.904410   \n",
       "1     {'max_depth': 1, 'n_estimators': 200}           0.602440   \n",
       "2    {'max_depth': 10, 'n_estimators': 200}           0.806619   \n",
       "3    {'max_depth': 50, 'n_estimators': 200}           0.903810   \n",
       "4   {'max_depth': 100, 'n_estimators': 200}           0.904410   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0             0.9064             0.9056             0.9048            0.90049   \n",
       "1             0.5867             0.5932             0.6036            0.59986   \n",
       "2             0.8058             0.8055             0.8014            0.80238   \n",
       "3             0.9059             0.9057             0.9051            0.90049   \n",
       "4             0.9064             0.9056             0.9048            0.90049   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0          0.90434        0.002043                1  \n",
       "1          0.59716        0.006352                5  \n",
       "2          0.80434        0.002057                4  \n",
       "3          0.90420        0.001993                3  \n",
       "4          0.90434        0.002043                1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90434, 0.59716, 0.80434, 0.9042 , 0.90434])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_list = [1,10,25,50,75,100]\n",
    "\n",
    "# Draw heatmaps for result of grid search.\n",
    "def draw_heatmap(errors, D_list, title):\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(errors, annot=True, fmt='.3f', yticklabels=D_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label('error')\n",
    "    ax.set(ylabel='max depth D')\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "cross_val_errors = 1 - cv.cv_results_['mean_test_score'].reshape((-1, 1))\n",
    "print(cross_val_errors.shape)\n",
    "draw_heatmap(cross_val_errors, D_list, title='(Cov_type RF) 5 Fold CV error w.r.t D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  40 | elapsed:   11.8s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  40 | elapsed:   17.5s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  40 | elapsed:   19.9s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  40 | elapsed:   21.9s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.1s finished\n",
      "C:\\Users\\taq19\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=10000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.0001, verbose=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [1, 3, 5, 10], 'loss': ['hinge'],\n",
       "                         'max_iter': [10000, 20000]},\n",
       "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "             return_train_score=False, scoring='accuracy', verbose=20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [1,3,5,10],\n",
    "    'loss':['hinge'],\n",
    "    'max_iter': [10000,20000]\n",
    "}\n",
    "\n",
    "svc = LSVC(max_iter=10000)\n",
    "cv = GridSearchCV(svc, param_grid=params, scoring='accuracy', n_jobs=-1, verbose=20, refit='accuracy', cv=5)\n",
    "cv.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 3, 'loss': 'hinge', 'max_iter': 10000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.475669</td>\n",
       "      <td>0.261214</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>6.299989e-04</td>\n",
       "      <td>3</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 3, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.969010</td>\n",
       "      <td>0.078606</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>3.234067e-07</td>\n",
       "      <td>1</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 1, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.788789</td>\n",
       "      <td>0.7816</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.875406</td>\n",
       "      <td>0.591390</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>1.340243e-06</td>\n",
       "      <td>3</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 3, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.787788</td>\n",
       "      <td>0.7814</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.007708</td>\n",
       "      <td>0.088960</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>3.994022e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 1, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.028292</td>\n",
       "      <td>0.420864</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>1.163857e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 5, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.504991</td>\n",
       "      <td>1.917206</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.265192e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>hinge</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'C': 10, 'loss': 'hinge', 'max_iter': 20000}</td>\n",
       "      <td>0.793207</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.332012</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>3.991615e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 5, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.787788</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>9.771397</td>\n",
       "      <td>0.950261</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.997331e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>hinge</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'C': 10, 'loss': 'hinge', 'max_iter': 10000}</td>\n",
       "      <td>0.792208</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.787788</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "2       2.475669      0.261214         0.000997    6.299989e-04       3   \n",
       "0       0.969010      0.078606         0.000997    3.234067e-07       1   \n",
       "3       2.875406      0.591390         0.000998    1.340243e-06       3   \n",
       "1       1.007708      0.088960         0.000799    3.994022e-04       1   \n",
       "5       5.028292      0.420864         0.001795    1.163857e-03       5   \n",
       "7       7.504991      1.917206         0.001000    1.265192e-06      10   \n",
       "4       5.332012      0.568569         0.001196    3.991615e-04       5   \n",
       "6       9.771397      0.950261         0.001796    3.997331e-04      10   \n",
       "\n",
       "  param_loss param_max_iter                                         params  \\\n",
       "2      hinge          10000   {'C': 3, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "0      hinge          10000   {'C': 1, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "3      hinge          20000   {'C': 3, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "1      hinge          20000   {'C': 1, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "5      hinge          20000   {'C': 5, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "7      hinge          20000  {'C': 10, 'loss': 'hinge', 'max_iter': 20000}   \n",
       "4      hinge          10000   {'C': 5, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "6      hinge          10000  {'C': 10, 'loss': 'hinge', 'max_iter': 10000}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "2           0.793207              0.783              0.773              0.770   \n",
       "0           0.793207              0.783              0.773              0.770   \n",
       "3           0.793207              0.783              0.773              0.770   \n",
       "1           0.793207              0.783              0.773              0.770   \n",
       "5           0.793207              0.783              0.773              0.770   \n",
       "7           0.793207              0.785              0.772              0.769   \n",
       "4           0.791209              0.783              0.773              0.769   \n",
       "6           0.792208              0.780              0.773              0.771   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "2           0.789790           0.7818        0.009079                1  \n",
       "0           0.788789           0.7816        0.008910                2  \n",
       "3           0.787788           0.7814        0.008757                3  \n",
       "1           0.786787           0.7812        0.008619                4  \n",
       "5           0.786787           0.7812        0.008619                4  \n",
       "7           0.786787           0.7812        0.009202                4  \n",
       "4           0.787788           0.7808        0.008511                7  \n",
       "6           0.787788           0.7808        0.008204                7  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   55.1s\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 267 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 270 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 271 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 273 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 275 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 279 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 282 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 283 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [1, 5], 'degree': [3, 7, 10],\n",
       "                         'gamma': array([4.00000000e-02, 6.68740305e-01, 1.11803399e+01, 1.86918598e+02,\n",
       "       3.12500000e+03]),\n",
       "                         'kernel': ['linear', 'rbf'], 'max_iter': [100000]},\n",
       "             pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "             return_train_score=False, scoring='accuracy', verbose=20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': [1, 5], \n",
    "    'kernel': ['linear', 'rbf'], \n",
    "    'degree': [3, 7, 10],\n",
    "    'gamma': np.power(5, np.linspace(-2,5, 5)),\n",
    "    'max_iter': [100000]\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "cv = GridSearchCV(svc, param_grid=params, scoring='accuracy', n_jobs=-1, verbose=20, refit='accuracy', cv=5)\n",
    "cv.fit(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>304001</td>\n",
       "      <td>2867</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>1924</td>\n",
       "      <td>206</td>\n",
       "      <td>212</td>\n",
       "      <td>142</td>\n",
       "      <td>1812</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333606</td>\n",
       "      <td>3097</td>\n",
       "      <td>295</td>\n",
       "      <td>15</td>\n",
       "      <td>175</td>\n",
       "      <td>49</td>\n",
       "      <td>1505</td>\n",
       "      <td>178</td>\n",
       "      <td>235</td>\n",
       "      <td>197</td>\n",
       "      <td>1191</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260019</td>\n",
       "      <td>3039</td>\n",
       "      <td>162</td>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>16</td>\n",
       "      <td>3059</td>\n",
       "      <td>223</td>\n",
       "      <td>240</td>\n",
       "      <td>152</td>\n",
       "      <td>1292</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72966</td>\n",
       "      <td>3041</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>376</td>\n",
       "      <td>66</td>\n",
       "      <td>5389</td>\n",
       "      <td>216</td>\n",
       "      <td>213</td>\n",
       "      <td>131</td>\n",
       "      <td>3750</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351036</td>\n",
       "      <td>3210</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>224</td>\n",
       "      <td>228</td>\n",
       "      <td>139</td>\n",
       "      <td>1812</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238314</td>\n",
       "      <td>3068</td>\n",
       "      <td>208</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>-12</td>\n",
       "      <td>2190</td>\n",
       "      <td>217</td>\n",
       "      <td>244</td>\n",
       "      <td>164</td>\n",
       "      <td>3705</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336087</td>\n",
       "      <td>2932</td>\n",
       "      <td>222</td>\n",
       "      <td>12</td>\n",
       "      <td>663</td>\n",
       "      <td>96</td>\n",
       "      <td>2627</td>\n",
       "      <td>205</td>\n",
       "      <td>251</td>\n",
       "      <td>181</td>\n",
       "      <td>1716</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461295</td>\n",
       "      <td>3250</td>\n",
       "      <td>308</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>11</td>\n",
       "      <td>693</td>\n",
       "      <td>212</td>\n",
       "      <td>237</td>\n",
       "      <td>164</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269104</td>\n",
       "      <td>3175</td>\n",
       "      <td>108</td>\n",
       "      <td>19</td>\n",
       "      <td>277</td>\n",
       "      <td>68</td>\n",
       "      <td>752</td>\n",
       "      <td>249</td>\n",
       "      <td>214</td>\n",
       "      <td>86</td>\n",
       "      <td>666</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283134</td>\n",
       "      <td>3245</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>365</td>\n",
       "      <td>132</td>\n",
       "      <td>1136</td>\n",
       "      <td>238</td>\n",
       "      <td>200</td>\n",
       "      <td>85</td>\n",
       "      <td>2012</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1   2    3    4     5    6    7    8     9   ...  44  45  46  \\\n",
       "304001  2867   12  13  212    2  1924  206  212  142  1812  ...   1   0   0   \n",
       "333606  3097  295  15  175   49  1505  178  235  197  1191  ...   0   0   0   \n",
       "260019  3039  162   4  134   16  3059  223  240  152  1292  ...   0   1   0   \n",
       "72966   3041   29  12  376   66  5389  216  213  131  3750  ...   0   0   0   \n",
       "351036  3210   59   6    0    0  1902  224  228  139  1812  ...   0   0   0   \n",
       "...      ...  ...  ..  ...  ...   ...  ...  ...  ...   ...  ...  ..  ..  ..   \n",
       "238314  3068  208   5   90  -12  2190  217  244  164  3705  ...   0   0   0   \n",
       "336087  2932  222  12  663   96  2627  205  251  181  1716  ...   0   0   0   \n",
       "461295  3250  308   3   67   11   693  212  237  164  1262  ...   0   0   0   \n",
       "269104  3175  108  19  277   68   752  249  214   86   666  ...   0   0   1   \n",
       "283134  3245   72  19  365  132  1136  238  200   85  2012  ...   0   0   0   \n",
       "\n",
       "        47  48  49  50  51  52  53  \n",
       "304001   0   0   0   0   0   0   0  \n",
       "333606   0   0   0   0   0   0   0  \n",
       "260019   0   0   0   0   0   0   0  \n",
       "72966    0   0   0   0   0   0   0  \n",
       "351036   0   0   0   0   0   0   0  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  \n",
       "238314   0   0   0   0   0   0   0  \n",
       "336087   0   0   0   0   0   0   0  \n",
       "461295   0   0   0   0   0   1   0  \n",
       "269104   0   0   0   0   0   0   0  \n",
       "283134   0   0   0   0   0   0   0  \n",
       "\n",
       "[5000 rows x 54 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel': 'rbf', 'max_iter': 100000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.353872</td>\n",
       "      <td>0.139754</td>\n",
       "      <td>0.384772</td>\n",
       "      <td>0.029790</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.568265</td>\n",
       "      <td>0.166693</td>\n",
       "      <td>0.366639</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.853188</td>\n",
       "      <td>0.191577</td>\n",
       "      <td>0.471340</td>\n",
       "      <td>0.056109</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.809191</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.380591</td>\n",
       "      <td>0.248239</td>\n",
       "      <td>0.649270</td>\n",
       "      <td>0.065163</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.784785</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.110350</td>\n",
       "      <td>0.194912</td>\n",
       "      <td>0.670208</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.784785</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.134265</td>\n",
       "      <td>0.188002</td>\n",
       "      <td>0.668224</td>\n",
       "      <td>0.025363</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.784785</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.7958</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.285059</td>\n",
       "      <td>0.356587</td>\n",
       "      <td>0.320534</td>\n",
       "      <td>0.085731</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.797798</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.551552</td>\n",
       "      <td>0.226214</td>\n",
       "      <td>0.402159</td>\n",
       "      <td>0.017890</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.797798</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.557165</td>\n",
       "      <td>0.271641</td>\n",
       "      <td>0.416488</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.798202</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.797798</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.357085</td>\n",
       "      <td>0.254296</td>\n",
       "      <td>0.558053</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.800801</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.142968</td>\n",
       "      <td>0.370902</td>\n",
       "      <td>0.606597</td>\n",
       "      <td>0.033079</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.800801</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>4.855678</td>\n",
       "      <td>0.554625</td>\n",
       "      <td>0.720833</td>\n",
       "      <td>0.156519</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.777223</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.781782</td>\n",
       "      <td>0.800801</td>\n",
       "      <td>0.7910</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.743483</td>\n",
       "      <td>0.223548</td>\n",
       "      <td>0.297605</td>\n",
       "      <td>0.028529</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.797830</td>\n",
       "      <td>0.065108</td>\n",
       "      <td>0.275204</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.147974</td>\n",
       "      <td>0.247351</td>\n",
       "      <td>0.305790</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.929534</td>\n",
       "      <td>0.182026</td>\n",
       "      <td>0.276261</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.940992</td>\n",
       "      <td>0.166402</td>\n",
       "      <td>0.291833</td>\n",
       "      <td>0.023074</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.194976</td>\n",
       "      <td>0.246384</td>\n",
       "      <td>0.283885</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.111710</td>\n",
       "      <td>0.208718</td>\n",
       "      <td>0.287033</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.258697</td>\n",
       "      <td>0.152972</td>\n",
       "      <td>0.309771</td>\n",
       "      <td>0.015764</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.168159</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>0.291235</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.211166</td>\n",
       "      <td>0.290105</td>\n",
       "      <td>0.312879</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.735517</td>\n",
       "      <td>0.331585</td>\n",
       "      <td>0.349074</td>\n",
       "      <td>0.055763</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.170094</td>\n",
       "      <td>0.243286</td>\n",
       "      <td>0.313264</td>\n",
       "      <td>0.040874</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.294776</td>\n",
       "      <td>0.133602</td>\n",
       "      <td>0.322744</td>\n",
       "      <td>0.034703</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.939967</td>\n",
       "      <td>0.173573</td>\n",
       "      <td>0.285636</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.838714</td>\n",
       "      <td>0.082162</td>\n",
       "      <td>0.273868</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.778222</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>6.531932</td>\n",
       "      <td>0.432230</td>\n",
       "      <td>0.430771</td>\n",
       "      <td>0.097775</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>5.651883</td>\n",
       "      <td>0.302745</td>\n",
       "      <td>0.298237</td>\n",
       "      <td>0.038839</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>6.140741</td>\n",
       "      <td>0.414935</td>\n",
       "      <td>0.285878</td>\n",
       "      <td>0.022203</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>5.780541</td>\n",
       "      <td>0.242922</td>\n",
       "      <td>0.294227</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>5.609655</td>\n",
       "      <td>0.199330</td>\n",
       "      <td>0.303308</td>\n",
       "      <td>0.027154</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>5.450360</td>\n",
       "      <td>0.369128</td>\n",
       "      <td>0.268483</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.712751</td>\n",
       "      <td>0.314677</td>\n",
       "      <td>0.302343</td>\n",
       "      <td>0.037190</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>5.637175</td>\n",
       "      <td>0.250045</td>\n",
       "      <td>0.275880</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.519468</td>\n",
       "      <td>0.336417</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.018491</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.096658</td>\n",
       "      <td>0.365167</td>\n",
       "      <td>0.267285</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.562713</td>\n",
       "      <td>0.316115</td>\n",
       "      <td>0.309894</td>\n",
       "      <td>0.025054</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.6687403049764...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.838950</td>\n",
       "      <td>0.550288</td>\n",
       "      <td>0.360842</td>\n",
       "      <td>0.100340</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>5.279767</td>\n",
       "      <td>0.330975</td>\n",
       "      <td>0.264855</td>\n",
       "      <td>0.014432</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>6.528256</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>0.267884</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>5.874879</td>\n",
       "      <td>0.128683</td>\n",
       "      <td>0.289227</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.775225</td>\n",
       "      <td>0.772228</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.779780</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>5.821910</td>\n",
       "      <td>0.155554</td>\n",
       "      <td>1.120640</td>\n",
       "      <td>0.061976</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.597598</td>\n",
       "      <td>0.604605</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>5.547228</td>\n",
       "      <td>0.240375</td>\n",
       "      <td>1.069610</td>\n",
       "      <td>0.061141</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.597598</td>\n",
       "      <td>0.604605</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.318644</td>\n",
       "      <td>0.288235</td>\n",
       "      <td>1.029479</td>\n",
       "      <td>0.065260</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.597598</td>\n",
       "      <td>0.604605</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.480756</td>\n",
       "      <td>0.365007</td>\n",
       "      <td>1.027878</td>\n",
       "      <td>0.067168</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.589411</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.591592</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.988523</td>\n",
       "      <td>0.253899</td>\n",
       "      <td>1.262025</td>\n",
       "      <td>0.042727</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 11.180339887498...</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.589411</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.591592</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.375298</td>\n",
       "      <td>0.277666</td>\n",
       "      <td>1.055180</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.589411</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.591592</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.611569</td>\n",
       "      <td>0.300030</td>\n",
       "      <td>1.266517</td>\n",
       "      <td>0.073419</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>6.520526</td>\n",
       "      <td>0.344982</td>\n",
       "      <td>1.227319</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>6.632954</td>\n",
       "      <td>0.296923</td>\n",
       "      <td>1.237304</td>\n",
       "      <td>0.072001</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>7.599114</td>\n",
       "      <td>0.407141</td>\n",
       "      <td>1.441760</td>\n",
       "      <td>0.110392</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>6.435442</td>\n",
       "      <td>0.377886</td>\n",
       "      <td>1.265153</td>\n",
       "      <td>0.058149</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6.325690</td>\n",
       "      <td>0.312012</td>\n",
       "      <td>1.311712</td>\n",
       "      <td>0.081270</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.188918</td>\n",
       "      <td>0.347599</td>\n",
       "      <td>1.225441</td>\n",
       "      <td>0.103431</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.348500</td>\n",
       "      <td>0.392467</td>\n",
       "      <td>1.306610</td>\n",
       "      <td>0.076471</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 7, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.320431</td>\n",
       "      <td>0.110414</td>\n",
       "      <td>1.268224</td>\n",
       "      <td>0.040854</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.736780</td>\n",
       "      <td>0.178227</td>\n",
       "      <td>1.261628</td>\n",
       "      <td>0.125509</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>6.420277</td>\n",
       "      <td>0.419279</td>\n",
       "      <td>1.221001</td>\n",
       "      <td>0.085236</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>5.439438</td>\n",
       "      <td>0.202960</td>\n",
       "      <td>0.863515</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.568432</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.568569</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "51       2.353872      0.139754         0.384772        0.029790       5   \n",
       "31       2.568265      0.166693         0.366639        0.020163       5   \n",
       "41       2.853188      0.191577         0.471340        0.056109       5   \n",
       "3        4.380591      0.248239         0.649270        0.065163       1   \n",
       "23       4.110350      0.194912         0.670208        0.050363       1   \n",
       "13       4.134265      0.188002         0.668224        0.025363       1   \n",
       "1        2.285059      0.356587         0.320534        0.085731       1   \n",
       "21       2.551552      0.226214         0.402159        0.017890       1   \n",
       "11       2.557165      0.271641         0.416488        0.028675       1   \n",
       "33       4.357085      0.254296         0.558053        0.018864       5   \n",
       "43       4.142968      0.370902         0.606597        0.033079       5   \n",
       "53       4.855678      0.554625         0.720833        0.156519       5   \n",
       "0        2.743483      0.223548         0.297605        0.028529       1   \n",
       "18       2.797830      0.065108         0.275204        0.012306       1   \n",
       "24       3.147974      0.247351         0.305790        0.024206       1   \n",
       "26       2.929534      0.182026         0.276261        0.022103       1   \n",
       "20       2.940992      0.166402         0.291833        0.023074       1   \n",
       "22       3.194976      0.246384         0.283885        0.014481       1   \n",
       "14       3.111710      0.208718         0.287033        0.016477       1   \n",
       "12       3.258697      0.152972         0.309771        0.015764       1   \n",
       "10       3.168159      0.232131         0.291235        0.018160       1   \n",
       "8        3.211166      0.290105         0.312879        0.036750       1   \n",
       "6        3.735517      0.331585         0.349074        0.055763       1   \n",
       "4        3.170094      0.243286         0.313264        0.040874       1   \n",
       "2        3.294776      0.133602         0.322744        0.034703       1   \n",
       "16       2.939967      0.173573         0.285636        0.019682       1   \n",
       "28       2.838714      0.082162         0.273868        0.019137       1   \n",
       "52       6.531932      0.432230         0.430771        0.097775       5   \n",
       "38       5.651883      0.302745         0.298237        0.038839       5   \n",
       "42       6.140741      0.414935         0.285878        0.022203       5   \n",
       "44       5.780541      0.242922         0.294227        0.020224       5   \n",
       "46       5.609655      0.199330         0.303308        0.027154       5   \n",
       "48       5.450360      0.369128         0.268483        0.003753       5   \n",
       "40       5.712751      0.314677         0.302343        0.037190       5   \n",
       "36       5.637175      0.250045         0.275880        0.017036       5   \n",
       "50       5.519468      0.336417         0.283442        0.018491       5   \n",
       "34       6.096658      0.365167         0.267285        0.008175       5   \n",
       "32       5.562713      0.316115         0.309894        0.025054       5   \n",
       "30       6.838950      0.550288         0.360842        0.100340       5   \n",
       "58       5.279767      0.330975         0.264855        0.014432       5   \n",
       "54       6.528256      0.208900         0.267884        0.006985       5   \n",
       "56       5.874879      0.128683         0.289227        0.014048       5   \n",
       "55       5.821910      0.155554         1.120640        0.061976       5   \n",
       "45       5.547228      0.240375         1.069610        0.061141       5   \n",
       "35       5.318644      0.288235         1.029479        0.065260       5   \n",
       "15       5.480756      0.365007         1.027878        0.067168       1   \n",
       "5        5.988523      0.253899         1.262025        0.042727       1   \n",
       "25       5.375298      0.277666         1.055180        0.053225       1   \n",
       "37       6.611569      0.300030         1.266517        0.073419       5   \n",
       "57       6.520526      0.344982         1.227319        0.068824       5   \n",
       "47       6.632954      0.296923         1.237304        0.072001       5   \n",
       "29       7.599114      0.407141         1.441760        0.110392       1   \n",
       "39       6.435442      0.377886         1.265153        0.058149       5   \n",
       "27       6.325690      0.312012         1.311712        0.081270       1   \n",
       "19       6.188918      0.347599         1.225441        0.103431       1   \n",
       "17       6.348500      0.392467         1.306610        0.076471       1   \n",
       "9        6.320431      0.110414         1.268224        0.040854       1   \n",
       "7        6.736780      0.178227         1.261628        0.125509       1   \n",
       "49       6.420277      0.419279         1.221001        0.085236       5   \n",
       "59       5.439438      0.202960         0.863515        0.071212       5   \n",
       "\n",
       "   param_degree param_gamma param_kernel param_max_iter  \\\n",
       "51           10        0.04          rbf         100000   \n",
       "31            3        0.04          rbf         100000   \n",
       "41            7        0.04          rbf         100000   \n",
       "3             3     0.66874          rbf         100000   \n",
       "23           10     0.66874          rbf         100000   \n",
       "13            7     0.66874          rbf         100000   \n",
       "1             3        0.04          rbf         100000   \n",
       "21           10        0.04          rbf         100000   \n",
       "11            7        0.04          rbf         100000   \n",
       "33            3     0.66874          rbf         100000   \n",
       "43            7     0.66874          rbf         100000   \n",
       "53           10     0.66874          rbf         100000   \n",
       "0             3        0.04       linear         100000   \n",
       "18            7        3125       linear         100000   \n",
       "24           10     11.1803       linear         100000   \n",
       "26           10     186.919       linear         100000   \n",
       "20           10        0.04       linear         100000   \n",
       "22           10     0.66874       linear         100000   \n",
       "14            7     11.1803       linear         100000   \n",
       "12            7     0.66874       linear         100000   \n",
       "10            7        0.04       linear         100000   \n",
       "8             3        3125       linear         100000   \n",
       "6             3     186.919       linear         100000   \n",
       "4             3     11.1803       linear         100000   \n",
       "2             3     0.66874       linear         100000   \n",
       "16            7     186.919       linear         100000   \n",
       "28           10        3125       linear         100000   \n",
       "52           10     0.66874       linear         100000   \n",
       "38            3        3125       linear         100000   \n",
       "42            7     0.66874       linear         100000   \n",
       "44            7     11.1803       linear         100000   \n",
       "46            7     186.919       linear         100000   \n",
       "48            7        3125       linear         100000   \n",
       "40            7        0.04       linear         100000   \n",
       "36            3     186.919       linear         100000   \n",
       "50           10        0.04       linear         100000   \n",
       "34            3     11.1803       linear         100000   \n",
       "32            3     0.66874       linear         100000   \n",
       "30            3        0.04       linear         100000   \n",
       "58           10        3125       linear         100000   \n",
       "54           10     11.1803       linear         100000   \n",
       "56           10     186.919       linear         100000   \n",
       "55           10     11.1803          rbf         100000   \n",
       "45            7     11.1803          rbf         100000   \n",
       "35            3     11.1803          rbf         100000   \n",
       "15            7     11.1803          rbf         100000   \n",
       "5             3     11.1803          rbf         100000   \n",
       "25           10     11.1803          rbf         100000   \n",
       "37            3     186.919          rbf         100000   \n",
       "57           10     186.919          rbf         100000   \n",
       "47            7     186.919          rbf         100000   \n",
       "29           10        3125          rbf         100000   \n",
       "39            3        3125          rbf         100000   \n",
       "27           10     186.919          rbf         100000   \n",
       "19            7        3125          rbf         100000   \n",
       "17            7     186.919          rbf         100000   \n",
       "9             3        3125          rbf         100000   \n",
       "7             3     186.919          rbf         100000   \n",
       "49            7        3125          rbf         100000   \n",
       "59           10        3125          rbf         100000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "51  {'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.809191   \n",
       "31  {'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.809191   \n",
       "41  {'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.809191   \n",
       "3   {'C': 1, 'degree': 3, 'gamma': 0.6687403049764...           0.808192   \n",
       "23  {'C': 1, 'degree': 10, 'gamma': 0.668740304976...           0.808192   \n",
       "13  {'C': 1, 'degree': 7, 'gamma': 0.6687403049764...           0.808192   \n",
       "1   {'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.798202   \n",
       "21  {'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.798202   \n",
       "11  {'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.798202   \n",
       "33  {'C': 5, 'degree': 3, 'gamma': 0.6687403049764...           0.808192   \n",
       "43  {'C': 5, 'degree': 7, 'gamma': 0.6687403049764...           0.808192   \n",
       "53  {'C': 5, 'degree': 10, 'gamma': 0.668740304976...           0.808192   \n",
       "0   {'C': 1, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.778222   \n",
       "18  {'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.778222   \n",
       "24  {'C': 1, 'degree': 10, 'gamma': 11.18033988749...           0.778222   \n",
       "26  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...           0.778222   \n",
       "20  {'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.778222   \n",
       "22  {'C': 1, 'degree': 10, 'gamma': 0.668740304976...           0.778222   \n",
       "14  {'C': 1, 'degree': 7, 'gamma': 11.180339887498...           0.778222   \n",
       "12  {'C': 1, 'degree': 7, 'gamma': 0.6687403049764...           0.778222   \n",
       "10  {'C': 1, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.778222   \n",
       "8   {'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.778222   \n",
       "6   {'C': 1, 'degree': 3, 'gamma': 186.91859765265...           0.778222   \n",
       "4   {'C': 1, 'degree': 3, 'gamma': 11.180339887498...           0.778222   \n",
       "2   {'C': 1, 'degree': 3, 'gamma': 0.6687403049764...           0.778222   \n",
       "16  {'C': 1, 'degree': 7, 'gamma': 186.91859765265...           0.778222   \n",
       "28  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.778222   \n",
       "52  {'C': 5, 'degree': 10, 'gamma': 0.668740304976...           0.775225   \n",
       "38  {'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.775225   \n",
       "42  {'C': 5, 'degree': 7, 'gamma': 0.6687403049764...           0.775225   \n",
       "44  {'C': 5, 'degree': 7, 'gamma': 11.180339887498...           0.775225   \n",
       "46  {'C': 5, 'degree': 7, 'gamma': 186.91859765265...           0.775225   \n",
       "48  {'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.775225   \n",
       "40  {'C': 5, 'degree': 7, 'gamma': 0.04, 'kernel':...           0.775225   \n",
       "36  {'C': 5, 'degree': 3, 'gamma': 186.91859765265...           0.775225   \n",
       "50  {'C': 5, 'degree': 10, 'gamma': 0.04, 'kernel'...           0.775225   \n",
       "34  {'C': 5, 'degree': 3, 'gamma': 11.180339887498...           0.775225   \n",
       "32  {'C': 5, 'degree': 3, 'gamma': 0.6687403049764...           0.775225   \n",
       "30  {'C': 5, 'degree': 3, 'gamma': 0.04, 'kernel':...           0.775225   \n",
       "58  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.775225   \n",
       "54  {'C': 5, 'degree': 10, 'gamma': 11.18033988749...           0.775225   \n",
       "56  {'C': 5, 'degree': 10, 'gamma': 186.9185976526...           0.775225   \n",
       "55  {'C': 5, 'degree': 10, 'gamma': 11.18033988749...           0.594406   \n",
       "45  {'C': 5, 'degree': 7, 'gamma': 11.180339887498...           0.594406   \n",
       "35  {'C': 5, 'degree': 3, 'gamma': 11.180339887498...           0.594406   \n",
       "15  {'C': 1, 'degree': 7, 'gamma': 11.180339887498...           0.584416   \n",
       "5   {'C': 1, 'degree': 3, 'gamma': 11.180339887498...           0.584416   \n",
       "25  {'C': 1, 'degree': 10, 'gamma': 11.18033988749...           0.584416   \n",
       "37  {'C': 5, 'degree': 3, 'gamma': 186.91859765265...           0.569431   \n",
       "57  {'C': 5, 'degree': 10, 'gamma': 186.9185976526...           0.569431   \n",
       "47  {'C': 5, 'degree': 7, 'gamma': 186.91859765265...           0.569431   \n",
       "29  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.568432   \n",
       "39  {'C': 5, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "27  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...           0.568432   \n",
       "19  {'C': 1, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "17  {'C': 1, 'degree': 7, 'gamma': 186.91859765265...           0.568432   \n",
       "9   {'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "7   {'C': 1, 'degree': 3, 'gamma': 186.91859765265...           0.568432   \n",
       "49  {'C': 5, 'degree': 7, 'gamma': 3125.0, 'kernel...           0.568432   \n",
       "59  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...           0.568432   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "51           0.802198              0.801           0.789790   \n",
       "31           0.802198              0.801           0.789790   \n",
       "41           0.802198              0.801           0.789790   \n",
       "3            0.798202              0.786           0.784785   \n",
       "23           0.798202              0.786           0.784785   \n",
       "13           0.798202              0.786           0.784785   \n",
       "1            0.790210              0.790           0.797798   \n",
       "21           0.790210              0.790           0.797798   \n",
       "11           0.790210              0.790           0.797798   \n",
       "33           0.777223              0.787           0.781782   \n",
       "43           0.777223              0.787           0.781782   \n",
       "53           0.777223              0.787           0.781782   \n",
       "0            0.772228              0.793           0.780781   \n",
       "18           0.772228              0.793           0.780781   \n",
       "24           0.772228              0.793           0.780781   \n",
       "26           0.772228              0.793           0.780781   \n",
       "20           0.772228              0.793           0.780781   \n",
       "22           0.772228              0.793           0.780781   \n",
       "14           0.772228              0.793           0.780781   \n",
       "12           0.772228              0.793           0.780781   \n",
       "10           0.772228              0.793           0.780781   \n",
       "8            0.772228              0.793           0.780781   \n",
       "6            0.772228              0.793           0.780781   \n",
       "4            0.772228              0.793           0.780781   \n",
       "2            0.772228              0.793           0.780781   \n",
       "16           0.772228              0.793           0.780781   \n",
       "28           0.772228              0.793           0.780781   \n",
       "52           0.772228              0.792           0.779780   \n",
       "38           0.772228              0.792           0.779780   \n",
       "42           0.772228              0.792           0.779780   \n",
       "44           0.772228              0.792           0.779780   \n",
       "46           0.772228              0.792           0.779780   \n",
       "48           0.772228              0.792           0.779780   \n",
       "40           0.772228              0.792           0.779780   \n",
       "36           0.772228              0.792           0.779780   \n",
       "50           0.772228              0.792           0.779780   \n",
       "34           0.772228              0.792           0.779780   \n",
       "32           0.772228              0.792           0.779780   \n",
       "30           0.772228              0.792           0.779780   \n",
       "58           0.772228              0.792           0.779780   \n",
       "54           0.772228              0.792           0.779780   \n",
       "56           0.772228              0.792           0.779780   \n",
       "55           0.596404              0.594           0.597598   \n",
       "45           0.596404              0.594           0.597598   \n",
       "35           0.596404              0.594           0.597598   \n",
       "15           0.589411              0.588           0.591592   \n",
       "5            0.589411              0.588           0.591592   \n",
       "25           0.589411              0.588           0.591592   \n",
       "37           0.568432              0.570           0.568569   \n",
       "57           0.568432              0.570           0.568569   \n",
       "47           0.568432              0.570           0.568569   \n",
       "29           0.568432              0.569           0.568569   \n",
       "39           0.568432              0.569           0.568569   \n",
       "27           0.568432              0.569           0.568569   \n",
       "19           0.568432              0.569           0.568569   \n",
       "17           0.568432              0.569           0.568569   \n",
       "9            0.568432              0.569           0.568569   \n",
       "7            0.568432              0.569           0.568569   \n",
       "49           0.568432              0.569           0.568569   \n",
       "59           0.568432              0.569           0.568569   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "51           0.813814           0.8032        0.008173                1  \n",
       "31           0.813814           0.8032        0.008173                1  \n",
       "41           0.813814           0.8032        0.008173                1  \n",
       "3            0.801802           0.7958        0.009085                4  \n",
       "23           0.801802           0.7958        0.009085                4  \n",
       "13           0.801802           0.7958        0.009085                4  \n",
       "1            0.794795           0.7942        0.003547                7  \n",
       "21           0.794795           0.7942        0.003547                7  \n",
       "11           0.794795           0.7942        0.003547                7  \n",
       "33           0.800801           0.7910        0.011685               10  \n",
       "43           0.800801           0.7910        0.011685               10  \n",
       "53           0.800801           0.7910        0.011685               10  \n",
       "0            0.794795           0.7838        0.008714               13  \n",
       "18           0.794795           0.7838        0.008714               13  \n",
       "24           0.794795           0.7838        0.008714               13  \n",
       "26           0.794795           0.7838        0.008714               13  \n",
       "20           0.794795           0.7838        0.008714               13  \n",
       "22           0.794795           0.7838        0.008714               13  \n",
       "14           0.794795           0.7838        0.008714               13  \n",
       "12           0.794795           0.7838        0.008714               13  \n",
       "10           0.794795           0.7838        0.008714               13  \n",
       "8            0.794795           0.7838        0.008714               13  \n",
       "6            0.794795           0.7838        0.008714               13  \n",
       "4            0.794795           0.7838        0.008714               13  \n",
       "2            0.794795           0.7838        0.008714               13  \n",
       "16           0.794795           0.7838        0.008714               13  \n",
       "28           0.794795           0.7838        0.008714               13  \n",
       "52           0.794795           0.7828        0.009020               28  \n",
       "38           0.794795           0.7828        0.009020               28  \n",
       "42           0.794795           0.7828        0.009020               28  \n",
       "44           0.794795           0.7828        0.009020               28  \n",
       "46           0.794795           0.7828        0.009020               28  \n",
       "48           0.794795           0.7828        0.009020               28  \n",
       "40           0.794795           0.7828        0.009020               28  \n",
       "36           0.794795           0.7828        0.009020               28  \n",
       "50           0.794795           0.7828        0.009020               28  \n",
       "34           0.794795           0.7828        0.009020               28  \n",
       "32           0.794795           0.7828        0.009020               28  \n",
       "30           0.794795           0.7828        0.009020               28  \n",
       "58           0.794795           0.7828        0.009020               28  \n",
       "54           0.794795           0.7828        0.009020               28  \n",
       "56           0.794795           0.7828        0.009020               28  \n",
       "55           0.604605           0.5974        0.003832               43  \n",
       "45           0.604605           0.5974        0.003832               43  \n",
       "35           0.604605           0.5974        0.003832               43  \n",
       "15           0.595596           0.5898        0.003720               46  \n",
       "5            0.595596           0.5898        0.003720               46  \n",
       "25           0.595596           0.5898        0.003720               46  \n",
       "37           0.568569           0.5690        0.000613               49  \n",
       "57           0.568569           0.5690        0.000613               49  \n",
       "47           0.568569           0.5690        0.000613               49  \n",
       "29           0.568569           0.5686        0.000209               52  \n",
       "39           0.568569           0.5686        0.000209               52  \n",
       "27           0.568569           0.5686        0.000209               52  \n",
       "19           0.568569           0.5686        0.000209               52  \n",
       "17           0.568569           0.5686        0.000209               52  \n",
       "9            0.568569           0.5686        0.000209               52  \n",
       "7            0.568569           0.5686        0.000209               52  \n",
       "49           0.568569           0.5686        0.000209               52  \n",
       "59           0.568569           0.5686        0.000209               52  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kernel': 'rbf', 'max_iter': 100000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>5.798716</td>\n",
       "      <td>0.341561</td>\n",
       "      <td>1.273867</td>\n",
       "      <td>0.061789</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.870519</td>\n",
       "      <td>0.380944</td>\n",
       "      <td>1.263730</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.848913</td>\n",
       "      <td>0.398054</td>\n",
       "      <td>1.273519</td>\n",
       "      <td>0.080736</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66874</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 0.668740304976...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>6.105049</td>\n",
       "      <td>0.352619</td>\n",
       "      <td>1.273745</td>\n",
       "      <td>0.073564</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.1803</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 11.18033988749...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.032891</td>\n",
       "      <td>0.352681</td>\n",
       "      <td>1.293661</td>\n",
       "      <td>0.073773</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>rbf</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>8.256499</td>\n",
       "      <td>0.474628</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.012694</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 186.91859765265...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>10.086388</td>\n",
       "      <td>0.197747</td>\n",
       "      <td>0.170566</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.598050</td>\n",
       "      <td>0.267677</td>\n",
       "      <td>0.102326</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>186.919</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 10, 'gamma': 186.9185976526...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.198848</td>\n",
       "      <td>0.379884</td>\n",
       "      <td>0.102765</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 2, 'gamma': 3125.0, 'kernel...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.429606</td>\n",
       "      <td>0.667812</td>\n",
       "      <td>0.100597</td>\n",
       "      <td>0.011946</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 1, 'degree': 2, 'gamma': 0.04, 'kernel':...</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.5308</td>\n",
       "      <td>0.039035</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "39       5.798716      0.341561         1.273867        0.061789       1   \n",
       "31       5.870519      0.380944         1.263730        0.064457       1   \n",
       "33       5.848913      0.398054         1.273519        0.080736       1   \n",
       "35       6.105049      0.352619         1.273745        0.073564       1   \n",
       "37       6.032891      0.352681         1.293661        0.073773       1   \n",
       "..            ...           ...              ...             ...     ...   \n",
       "16       8.256499      0.474628         0.109705        0.012694       1   \n",
       "18      10.086388      0.197747         0.170566        0.034574       1   \n",
       "36       7.598050      0.267677         0.102326        0.008662       1   \n",
       "8        8.198848      0.379884         0.102765        0.009486       1   \n",
       "0        7.429606      0.667812         0.100597        0.011946       1   \n",
       "\n",
       "   param_degree param_gamma param_kernel param_max_iter  \\\n",
       "39           10        3125          rbf         100000   \n",
       "31           10        0.04          rbf         100000   \n",
       "33           10     0.66874          rbf         100000   \n",
       "35           10     11.1803          rbf         100000   \n",
       "37           10     186.919          rbf         100000   \n",
       "..          ...         ...          ...            ...   \n",
       "16            3     186.919       linear         100000   \n",
       "18            3        3125       linear         100000   \n",
       "36           10     186.919       linear         100000   \n",
       "8             2        3125       linear         100000   \n",
       "0             2        0.04       linear         100000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "39  {'C': 1, 'degree': 10, 'gamma': 3125.0, 'kerne...              0.578   \n",
       "31  {'C': 1, 'degree': 10, 'gamma': 0.04, 'kernel'...              0.578   \n",
       "33  {'C': 1, 'degree': 10, 'gamma': 0.668740304976...              0.578   \n",
       "35  {'C': 1, 'degree': 10, 'gamma': 11.18033988749...              0.578   \n",
       "37  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...              0.578   \n",
       "..                                                ...                ...   \n",
       "16  {'C': 1, 'degree': 3, 'gamma': 186.91859765265...              0.528   \n",
       "18  {'C': 1, 'degree': 3, 'gamma': 3125.0, 'kernel...              0.528   \n",
       "36  {'C': 1, 'degree': 10, 'gamma': 186.9185976526...              0.528   \n",
       "8   {'C': 1, 'degree': 2, 'gamma': 3125.0, 'kernel...              0.528   \n",
       "0   {'C': 1, 'degree': 2, 'gamma': 0.04, 'kernel':...              0.528   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "39              0.578              0.578              0.578   \n",
       "31              0.578              0.578              0.578   \n",
       "33              0.578              0.578              0.578   \n",
       "35              0.578              0.578              0.578   \n",
       "37              0.578              0.578              0.578   \n",
       "..                ...                ...                ...   \n",
       "16              0.598              0.543              0.495   \n",
       "18              0.598              0.543              0.495   \n",
       "36              0.598              0.543              0.495   \n",
       "8               0.598              0.543              0.495   \n",
       "0               0.598              0.543              0.495   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "39              0.578           0.5780        0.000000                1  \n",
       "31              0.578           0.5780        0.000000                1  \n",
       "33              0.578           0.5780        0.000000                1  \n",
       "35              0.578           0.5780        0.000000                1  \n",
       "37              0.578           0.5780        0.000000                1  \n",
       "..                ...              ...             ...              ...  \n",
       "16              0.490           0.5308        0.039035               61  \n",
       "18              0.490           0.5308        0.039035               61  \n",
       "36              0.490           0.5308        0.039035               61  \n",
       "8               0.490           0.5308        0.039035               61  \n",
       "0               0.490           0.5308        0.039035               61  \n",
       "\n",
       "[80 rows x 18 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kernel': 'linear', 'max_iter': 100000}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_degree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>5.987801</td>\n",
       "      <td>0.221501</td>\n",
       "      <td>0.068331</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3125</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>7.997192</td>\n",
       "      <td>0.408793</td>\n",
       "      <td>0.106914</td>\n",
       "      <td>0.023461</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>73.1004</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 73.100443455321...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>8.001259</td>\n",
       "      <td>0.302747</td>\n",
       "      <td>0.099534</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>20.9063</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 20.906275773759...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>8.874059</td>\n",
       "      <td>0.521921</td>\n",
       "      <td>0.101927</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5.97907</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 5.9790658725020...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>9.529589</td>\n",
       "      <td>0.282009</td>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.70998</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 5, 'degree': 3, 'gamma': 1.7099759466766...</td>\n",
       "      <td>0.53047</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.661662</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>8.095172</td>\n",
       "      <td>0.437890</td>\n",
       "      <td>0.121276</td>\n",
       "      <td>0.014693</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>5.97907</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 5.97906587250...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>8.226855</td>\n",
       "      <td>0.397989</td>\n",
       "      <td>0.130860</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.70998</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 1.70997594667...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>9.454508</td>\n",
       "      <td>0.903808</td>\n",
       "      <td>0.123868</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.489043</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 0.48904256961...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.144085</td>\n",
       "      <td>0.610632</td>\n",
       "      <td>0.123669</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 7, 'gamma': 0.04, 'kernel...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.795585</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.114694</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'C': 0.1, 'degree': 2, 'gamma': 0.04, 'kernel...</td>\n",
       "      <td>0.48951</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.5034</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "159       5.987801      0.221501         0.068331        0.009900       5   \n",
       "136       7.997192      0.408793         0.106914        0.023461       5   \n",
       "135       8.001259      0.302747         0.099534        0.008952       5   \n",
       "134       8.874059      0.521921         0.101927        0.006688       5   \n",
       "133       9.529589      0.282009         0.118613        0.011472       5   \n",
       "..             ...           ...              ...             ...     ...   \n",
       "24        8.095172      0.437890         0.121276        0.014693     0.1   \n",
       "23        8.226855      0.397989         0.130860        0.012974     0.1   \n",
       "22        9.454508      0.903808         0.123868        0.012350     0.1   \n",
       "20        8.144085      0.610632         0.123669        0.007490     0.1   \n",
       "0         7.795585      0.330739         0.114694        0.004886     0.1   \n",
       "\n",
       "    param_degree param_gamma param_kernel param_max_iter  \\\n",
       "159           10        3125       linear         100000   \n",
       "136            3     73.1004       linear         100000   \n",
       "135            3     20.9063       linear         100000   \n",
       "134            3     5.97907       linear         100000   \n",
       "133            3     1.70998       linear         100000   \n",
       "..           ...         ...          ...            ...   \n",
       "24             7     5.97907       linear         100000   \n",
       "23             7     1.70998       linear         100000   \n",
       "22             7    0.489043       linear         100000   \n",
       "20             7        0.04       linear         100000   \n",
       "0              2        0.04       linear         100000   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "159  {'C': 5, 'degree': 10, 'gamma': 3125.0, 'kerne...            0.53047   \n",
       "136  {'C': 5, 'degree': 3, 'gamma': 73.100443455321...            0.53047   \n",
       "135  {'C': 5, 'degree': 3, 'gamma': 20.906275773759...            0.53047   \n",
       "134  {'C': 5, 'degree': 3, 'gamma': 5.9790658725020...            0.53047   \n",
       "133  {'C': 5, 'degree': 3, 'gamma': 1.7099759466766...            0.53047   \n",
       "..                                                 ...                ...   \n",
       "24   {'C': 0.1, 'degree': 7, 'gamma': 5.97906587250...            0.48951   \n",
       "23   {'C': 0.1, 'degree': 7, 'gamma': 1.70997594667...            0.48951   \n",
       "22   {'C': 0.1, 'degree': 7, 'gamma': 0.48904256961...            0.48951   \n",
       "20   {'C': 0.1, 'degree': 7, 'gamma': 0.04, 'kernel...            0.48951   \n",
       "0    {'C': 0.1, 'degree': 2, 'gamma': 0.04, 'kernel...            0.48951   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "159              0.464              0.614              0.564   \n",
       "136              0.464              0.614              0.564   \n",
       "135              0.464              0.614              0.564   \n",
       "134              0.464              0.614              0.564   \n",
       "133              0.464              0.614              0.564   \n",
       "..                 ...                ...                ...   \n",
       "24               0.488              0.526              0.483   \n",
       "23               0.488              0.526              0.483   \n",
       "22               0.488              0.526              0.483   \n",
       "20               0.488              0.526              0.483   \n",
       "0                0.488              0.526              0.483   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "159           0.661662           0.5668        0.067992                1  \n",
       "136           0.661662           0.5668        0.067992                1  \n",
       "135           0.661662           0.5668        0.067992                1  \n",
       "134           0.661662           0.5668        0.067992                1  \n",
       "133           0.661662           0.5668        0.067992                1  \n",
       "..                 ...              ...             ...              ...  \n",
       "24            0.530531           0.5034        0.020457              121  \n",
       "23            0.530531           0.5034        0.020457              121  \n",
       "22            0.530531           0.5034        0.020457              121  \n",
       "20            0.530531           0.5034        0.020457              121  \n",
       "0             0.530531           0.5034        0.020457              121  \n",
       "\n",
       "[160 rows x 18 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(cv.cv_results_)\n",
    "print(\"best params: \", cv_results.sort_values('rank_test_score').reset_index()['params'][0])\n",
    "cv_results.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_cov = np.array([(0.2, 0.903), (0.5, 0.883), (0.8, 0.852)])\n",
    "nn_cov = np.array([(0.2, 0.876), (0.5, 0.860), (0.8, 0.830)])\n",
    "svm_cov = np.array([(0.2, 0.770), (0.5, 0.772), (0.8, 0.772)])\n",
    "\n",
    "x1 = rf_cov[:,0]\n",
    "y1 = rf_cov[:,1]\n",
    "\n",
    "x2 = nn_cov[:,0]\n",
    "y2 = nn_cov[:,1]\n",
    "\n",
    "x3 = svm_cov[:,0]\n",
    "y3 = svm_cov[:,1]\n",
    "\n",
    "plt.plot(x1, y1, marker='o', linestyle='dashed', linewidth=2, markersize=8, label='Random Forest')\n",
    "plt.plot(x2, y2, marker='o', linestyle='dashed', linewidth=2, markersize=8, label = 'Neural Network')\n",
    "plt.plot(x3, y3, marker='o', linestyle='dashed', linewidth=2, markersize=8, label = 'SVM')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Area under Curve')\n",
    "plt.title('AUC vs Training Size in Cov_Type Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

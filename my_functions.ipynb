{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, accuracy_score, f1_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(y, preds, model):\n",
    "    fpr, tpr, thresholds = roc_curve(y, preds)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    print(\"max(tpr - fpr) w/ th = \", optimal_threshold)\n",
    "    l1, = plt.plot([0, 1], [0, 1], '--')\n",
    "    l2, = plt.plot(fpr, tpr, label = 'Random Forest')\n",
    "    auc = roc_auc_score(y, preds)\n",
    "    l = plt.legend([l2], [model+str(' AUC: %.2f' % auc)])\n",
    "    for x, y, txt in zip(fpr[::5], tpr[::5], thresholds[::5]):\n",
    "        plt.annotate(np.round(txt,5), (x, y-0.04))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "    \n",
    "def matrix_info(th, y, probs):\n",
    "    preds = (probs >= th).astype(np.int)\n",
    "    print('f1_score:')\n",
    "    print(f1_score(y, preds, average = 'micro'))\n",
    "    print('precision_score:')\n",
    "    print(precision_score(y, preds, average = 'micro'))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y,preds))\n",
    "    print(classification_report(y,preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
